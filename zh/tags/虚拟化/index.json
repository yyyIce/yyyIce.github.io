[{"content":"\r Yaml文档：https://yaml.org/ 可以先参看他人博客，大概了解以后看官方文档进行使用，不建议直接看文档进行学习。  \r \r1 Yaml简介  YAML is a human friendly data serialization language for all programming languages.\n 　YAML （发音 /ˈjæməl/ ）是一种适用于所有编程语言的人类友好的数据序列化语言。它专门用来写配置文件。可以在本章快速了解Yaml的组成和语法。\n1.1 基本语法规则 　它具有以下基本句法规则：\n 大小写敏感； 使用缩进表示层级关系，缩进不允许使用TAB键，只允许使用空格，缩进时不要求空格数，相同层级元素左侧对齐即可； #表示注释这一整行的字符； 冒号：冒号后面必须有空格，以冒号结尾除外； 短横线：使用一个短横线加一个空格表示列表项，多个项使用同样的缩进机别作为同一列表。  　支持数据结构包括：\n（1）对象：键值对的集合，又可以称为映射（mapping）/哈希（hashes）/字典（dictionary），键值对即\u0026lt;key\u0026gt;: \u0026lt;value\u0026gt;，如name: Talent注意冒号后有空格。\n（2）数组：一组按次序排列的值，又称为序列（sequence）/列表（list），以短横线开头并且左对齐一列或直接写为数组，注意一个短横线为起始表示数组的一个元素，元素可以是对象（包含在{}内）、数组（包含在[]内）和纯量（字符串、整数等）例如：\n1 2 3 4 5 6  food:- chocolate- sweets- ice cream# 或行内表示food:[chocolate, sweets, ice cream]  （3）字面量/纯量：单个的、不可再分的值。包括字符串（默认不使用引号表示，也可以使用'string'）、布尔值（全大写或全小写均可，TRUE/true）、浮点数、整数、Null、日期（yyyy-MM-dd，如2021-10-12）、时间（\u0026lt;日期\u0026gt;T\u0026lt;hh:mm:ss\u0026gt;+\u0026lt;时区\u0026gt;，如2021-10-12T15:15:16+08:00）。\n　YAML文件可以由一个或多个文档组成，文档间使用---在每文档开始作为分隔符，用...作为结束符。如果知识单个文档，分隔符---可以省略；每个文档也不需要使用结束符...来表示结束，但对于网络传输或者流来说，明确的结束符号有利于软件处理。\n1.2 简单Yaml示例 　一个简单的YAML文件01_hello_yaml.yml如下，\n1 2 3 4  ---text:Hello, Yaml!date:2021-10-13...  　各种语言均能够对YAML语言进行解析，例如使用python解析YAML文件，可以检查YAML文件格式是否正确。首先安装pyyaml模块。\n1  $ pip install pyyaml   　然后用pyyaml模块解析我们前面的01_hello_yaml.yml，将python文件保存为01_hello_yaml.py：\n1 2 3 4 5 6 7 8 9 10 11  #!/usr/bin/env python # -*- coding: utf-8 -*- import yaml file_path = \u0026#34;./01_hello_yaml.yml\u0026#34; file = open(file_path, \u0026#39;r\u0026#39;) # 读取包含一个yml文档对象的yml文件 yml_content = yaml.load(file.read(), Loader=yaml.Loader) print(yml_content) print(type(yml_content))   　查看结果，可以看到用python解析yaml文件，会将yaml的内容转换为一个dict对象\n1 2 3  $ python3 01_hello_yaml.py {\u0026#39;text\u0026#39;: \u0026#39;Hello, Yaml!\u0026#39;, \u0026#39;date\u0026#39;: datetime.date(2021, 10, 13)} \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt;   2 Yaml语法规则  Yaml语法规则：https://github.com/yaml/yaml-grammar/blob/master/yaml-spec-1.2.yaml\n 　基本语法规则在1.1节中可以找到。\n2.1 多个对象文档文件 　一个包含两个的YAML文件02_multi_yaml.yml如下，它包含两个文档：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ---text:Hello, Yaml!date:2021-10-13...---name:Talentsex:femaleage:16hobby:- swim- draw- delicious foodfavorite food:[chocolate, ice cream]family:father:LiMather:Liu...  　各种语言均能够对YAML语言进行解析，例如使用python解析YAML文件，可以检查YAML文件格式是否正确。首先安装pyyaml模块。\n1  $ pip install pyyaml   　然后用pyyaml模块解析我们前面的02_multi_yaml.yml，将python文件保存为02_multi_yaml.py：\n1 2 3 4 5 6 7 8 9 10 11 12  #!/usr/bin/env python # -*- coding: utf-8 -*- import yaml file_path = \u0026#34;./02_multi_yaml.yml\u0026#34; file = open(file_path, \u0026#39;r\u0026#39;) # 解析包含多个YAML文件的yml文件 yml_content = yaml.load_all(file.read(), Loader=yaml.Loader) for yml_i in yml_content: print(yml_i) print(type(yml_i))   　查看结果：\n1 2 3 4 5  $ python3 02_multi_yaml.py {\u0026#39;text\u0026#39;: \u0026#39;Hello, Yaml!\u0026#39;, \u0026#39;date\u0026#39;: datetime.date(2021, 10, 13)} \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; {\u0026#39;name\u0026#39;: \u0026#39;Talent\u0026#39;, \u0026#39;sex\u0026#39;: \u0026#39;female\u0026#39;, \u0026#39;age\u0026#39;: 16, \u0026#39;hobby\u0026#39;: [\u0026#39;swim\u0026#39;, \u0026#39;draw\u0026#39;, \u0026#39;delicious food\u0026#39;], \u0026#39;favorite food\u0026#39;: [\u0026#39;chocolate\u0026#39;, \u0026#39;ice cream\u0026#39;], \u0026#39;family\u0026#39;: {\u0026#39;father\u0026#39;: \u0026#39;Li\u0026#39;, \u0026#39;Mather\u0026#39;: \u0026#39;Liu\u0026#39;}} \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt;   2.2 复合结构  程序来源：https://cloud.tencent.com/developer/article/1718377\n 　下面是一个包含复合结构的yaml文件03_compose_yaml.yml，在只有一个yaml文档时可以省略---和...。注意短横线的运用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  shop:GoodShoppingaddress:BJgoods:Food:- sell_time:\u0026#34;AM 08:30\u0026#34;food01:potatofood02:porkFruits:- sell_time:\u0026#34;AM 09:00\u0026#34;- fruit01:orangeprice:3.50- fruit02:bananaprice:3.00Clothes:- sell_time:\u0026#34;AM 09:30\u0026#34;- clothe01- clothe02  　我们用python程序输出解析出来的yaml文件，观察输出格式，体会短横线书写格式的用法区别，将python程序另存为03_compose_yaml.py。\n1 2 3 4 5 6 7 8 9 10 11 12  #!/usr/bin/env python # -*- coding: utf-8 -*- import yaml import json file_path = \u0026#34;./03_compose_yaml.yml\u0026#34; file = open(file_path, \u0026#39;r\u0026#39;) # 读取包含一个yml文档对象的yml文件 yml_content = yaml.load(file.read(), Loader=yaml.Loader) print(yml_content) # print(json.dumps(yml_content, indent=2))   　运行python程序，输出结果如下：\n1  {\u0026#39;shop\u0026#39;: \u0026#39;GoodShopping\u0026#39;, \u0026#39;goods\u0026#39;: {\u0026#39;Food\u0026#39;: [{\u0026#39;food02\u0026#39;: \u0026#39;pork\u0026#39;, \u0026#39;sell_time\u0026#39;: \u0026#39;AM 08:30\u0026#39;, \u0026#39;food01\u0026#39;: \u0026#39;rice\u0026#39;}], \u0026#39;Fruits\u0026#39;: [{\u0026#39;sell_time\u0026#39;: \u0026#39;AM 09:00\u0026#39;}, {\u0026#39;fruit01\u0026#39;: \u0026#39;orange\u0026#39;, \u0026#39;price\u0026#39;: 3.5}, {\u0026#39;price\u0026#39;: 3.0, \u0026#39;fruit02\u0026#39;: \u0026#39;banana\u0026#39;}], \u0026#39;clothes\u0026#39;: [{\u0026#39;sell_time\u0026#39;: \u0026#39;AM 09:30\u0026#39;}, \u0026#39;clothe01\u0026#39;, \u0026#39;clothe02\u0026#39;]}, \u0026#39;address\u0026#39;: \u0026#39;BJ\u0026#39;}   　这样看非常不方便，我们把它转换为json格式，并增加缩进：\n1  print(json.dumps(yml_content, indent=2))   　运行python程序，输出结果如下，我们可以看到：\n 冒号:后是键值中的值，值可以是对象（用{}包含）、数组或纯量； 一列短横线-表示一个数组，用[]包含，一个-后面是数组的一个元素； 一个短横线-后的值表示数组的一个元素，数组元素可以是对象（用{}包含）、数组、纯量。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  { \u0026#34;shop\u0026#34;: \u0026#34;GoodShopping\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;BJ\u0026#34;, \u0026#34;goods\u0026#34;: { \u0026#34;Food\u0026#34;: [ { \u0026#34;sell_time\u0026#34;: \u0026#34;AM 08:30\u0026#34;, \u0026#34;food01\u0026#34;: \u0026#34;potato\u0026#34;, \u0026#34;food02\u0026#34;: \u0026#34;pork\u0026#34; } ], \u0026#34;Fruits\u0026#34;: [ { \u0026#34;sell_time\u0026#34;: \u0026#34;AM 09:00\u0026#34; }, { \u0026#34;fruit01\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;price\u0026#34;: 3.5 }, { \u0026#34;fruit02\u0026#34;: \u0026#34;banana\u0026#34;, \u0026#34;price\u0026#34;: 3.0 } ], \u0026#34;Clothes\u0026#34;: [ { \u0026#34;sell_time\u0026#34;: \u0026#34;AM 09:30\u0026#34; }, \u0026#34;clothe01\u0026#34;, \u0026#34;clothe02\u0026#34; ] } }   2.3 引用  参考：https://www.runoob.com/w3cnote/yaml-intro.html\n 　YAML语言可以通过建立锚点来快速添加重复内容，对应的符号与语法规则如下：\n \u0026amp;：建立锚点（defaults），\u0026amp;后跟锚点名称； \u0026lt;\u0026lt;：合并到当前数据，后跟冒号:； *：引用锚点，*后跟锚点名称。  　我们建立一个文件04_ref_yaml.yml来体会引用的用法，下面的例子中，首先为想要引用的内容建立锚点，锚点名称为defaults_content，然后在development和test中用*引用锚点，最后用\u0026lt;\u0026lt;将锚点内容合并到文件。\n1 2 3 4 5 6 7 8 9 10 11  defaults:\u0026amp;defaults_contentadapter:postgreshost:localhostdevelopment:database:myapp_development\u0026lt;\u0026lt;:*defaults_contenttest:database:myapp_test\u0026lt;\u0026lt;:*defaults_content  　我们建立python文件04_ref_yaml.py解析yaml文件：\n1 2 3 4 5 6 7 8 9 10 11 12  #!/usr/bin/env python # -*- coding: utf-8 -*- import yaml import json file_path = \u0026#34;./04_ref_yaml.yml\u0026#34; file = open(file_path, \u0026#39;r\u0026#39;) # 读取包含一个yml文档对象的yml文件 yml_content = yaml.load(file.read(), Loader=yaml.Loader) print(yml_content) print(json.dumps(yml_content, indent=2))   　输出Json格式的结果如下，我们编写输出结果这样内容的yaml文件和04_ref_yaml.yml的效果是一样的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  { \u0026#34;defaults\u0026#34;: { \u0026#34;adapter\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34; }, \u0026#34;development\u0026#34;: { \u0026#34;adapter\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;myapp_development\u0026#34; }, \u0026#34;test\u0026#34;: { \u0026#34;adapter\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;myapp_test\u0026#34; } }   2.4 字符串 　yaml字符串默认不使用引号表示；\n1  str:hello  　如果字符串中包含空格或特殊字符，需要放在引号（单引号双引号皆可）中；\n1  quotation_str:\u0026#34;Hello, world!\u0026#34;  　双引号不会对特殊字符转义，如果单引号内还有单引号，必须连续使用两个单引号进行转义；\n1 2 3  quotation_str:\u0026#39;hi,\\nyaml\u0026#39;double_quotation_str:\u0026#34;hi,\\nyaml\u0026#34;escape_character_str:\u0026#39;user\u0026#39;\u0026#39;s QoE\u0026#39;  　字符串可以写成多行，从第二行开始，必须有空格缩进，换行符会被转换为空格；\n1 2 3  multi_line: str:firstlinesecondlinethirdline  　多行字符串可以使用|保留换行符，也可以使用\u0026gt;去掉换行符，+表示保留字符串末尾的换行符，-表示删除字符串末尾的换行符；\n1 2 3 4 5 6 7 8 9 10 11 12  remain_line_break:|Foo barfold_line_break:\u0026gt;Foo barremain_end_line_break:|+Foo barfold_end_line_break:|-Foo bar  　字符串中可以插入HTML标记。\n1 2 3 4  html_content:|\u0026lt;p\u0026gt; This is a paragraph. \u0026lt;/p\u0026gt;  　我们使用编写有一个示例05_str_yaml.yml来体会yaml字符串的用法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  str:helloquotation_str:\u0026#34;Hello, world!\u0026#34;quotation_str:\u0026#39;hi,\\nyaml\u0026#39;double_quotation_str:\u0026#34;hi,\\nyaml\u0026#34;escape_character_str:\u0026#39;user\u0026#39;\u0026#39;s QoE\u0026#39;multi_line_str:firstlinesecondlinethirdlineremain_line_break:|Foo bar testfold_line_break:\u0026gt;Foo bar testremain_end_line_break:|+Foo bar testfold_end_line_break:|-Foo bar testhtml_content:| \u0026lt;p\u0026gt; This is a paragraph. \u0026lt;/p\u0026gt;  　用05_str_yaml.py对yaml文件进行解析，查看输出，体会yaml的方便之处。\n1 2 3 4 5 6 7 8 9 10 11 12  #!/usr/bin/env python # -*- coding: utf-8 -*- import yaml import json file_path = \u0026#34;./05_str_yaml.yml\u0026#34; file = open(file_path, \u0026#39;r\u0026#39;) # 读取包含一个yml文档对象的yml文件 yml_content = yaml.load(file.read(), Loader=yaml.Loader) print(yml_content) print(json.dumps(yml_content, indent=2))   　查看Json格式的输出结果，可以看到双引号不转义特殊字符，换行符被转换成空格等上面描述过的结果。\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;str\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;quotation_str\u0026#34;: \u0026#34;hi,\\\\nyaml\u0026#34;, \u0026#34;double_quotation_str\u0026#34;: \u0026#34;hi,\\nyaml\u0026#34;, \u0026#34;escape_character_str\u0026#34;: \u0026#34;user\u0026#39;s QoE\u0026#34;, \u0026#34;multi_line_str\u0026#34;: \u0026#34;firstline secondline thirdline\u0026#34;, \u0026#34;remain_line_break\u0026#34;: \u0026#34;Foo\\nbar\\ntest\\n\u0026#34;, \u0026#34;fold_line_break\u0026#34;: \u0026#34;Foo bar test\\n\u0026#34;, \u0026#34;remain_end_line_break\u0026#34;: \u0026#34;Foo\\nbar\\ntest\\n\u0026#34;, \u0026#34;fold_end_line_break\u0026#34;: \u0026#34;Foo\\nbar\\ntest\u0026#34;, \u0026#34;html_content\u0026#34;: \u0026#34;\u0026lt;p\u0026gt; \\u00a0 \\n This is a paragraph. \\n\u0026lt;/p\u0026gt;\\n\u0026#34; }   2.5 强制类型转换 　yaml可用双叹号!!纯量进行强制类型转换，格式为!!\u0026lt;强制转换类型\u0026gt; 原值，例如06_type_cast_yaml.yml：\n1 2  float_to_str:!!str6.66str_to_int:!!int\u0026#34;888\u0026#34;  　用06_type_cast_yaml.py解析。\n1 2 3 4 5 6 7 8 9 10 11 12  #!/usr/bin/env python # -*- coding: utf-8 -*- import yaml import json file_path = \u0026#34;./06_type_cast_yaml.yml\u0026#34; file = open(file_path, \u0026#39;r\u0026#39;) # 读取包含一个yml文档对象的yml文件 yml_content = yaml.load(file.read(), Loader=yaml.Loader) print(yml_content) print(json.dumps(yml_content, indent=2))   　查看json格式的输出，可以看到原来的浮点数变成了字符串，原来的字符串变成了整数：\n1 2 3 4  { \u0026#34;float_to_str\u0026#34;: \u0026#34;6.66\u0026#34;, \u0026#34;str_to_int\u0026#34;: 888 }   3 示例代码地址 　通过下列代码获取本文全部示例：\n1  $ git clone https://github.com/yyyIce/yaml_exercise.git   参考资料 YAML 语言教程与使用案例：https://cloud.tencent.com/developer/article/1718377\nYAML 入门教程：https://www.runoob.com/w3cnote/yaml-intro.html\n","description":"yaml的基本语法、使用方法和使用示例","id":3,"section":"posts","tags":["yaml"],"title":"YAML：yaml基本使用","uri":"https://yyyIce.github.io/zh/posts/yamlyaml%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"content":"　注：本节适合对常用数据集操作一知半解但混淆不清的人群阅读，不适合一条常用命令都不清楚的人群阅读。参数、返回值、使用示例还是官方文档写得清楚，在有所了解、思维清晰后推荐使用官方文档。\n pandas文档：https://pandas.pydata.org/docs/reference/api/\nnumpy文档：https://numpy.org/doc/stable/reference/index.html\n 1 读入数据 　我们以一组瞎编的数据为例，这组数据保存在train.csv中，我们通过train.csv进行操作，体会他们之间的差异和共通之处，普通的数据集csv文件可能有以下几种形式：\n　形式A——标准表格型：一个单元格一条数据。\nid\tfeature1\tfeature2 ... feature10 label\r1001\t1 5 ... 3\t1\r1002\t2 8 ... 7\t1\r1003\t5 6 ... 3\t1\r1004\t9 5 ... 4\t2\r1005\t2 2 ... 4 2\r1006\t6 4 ... 2\t0\r1007\t2 5 ... 7 0\r1008\t2 5 ... 1\t0\r1009\t3 1 ... 2 1\r1010\t7 9 ... 8\t2\r1011\t5 8 ... 8\t1\r1012\t4 1 ... 5 0\r　形式B——特征一列型：特征用,连接，特征用一列来表示。\nid\tfeature\tlabel\r1001\t1,5,2,3,8,9,2,1,1,3\t1\r1002\t2,8,9,4,4,4,6,1,0,7\t1\r1003\t5,6,9,7,4,1,2,5,1,3\t1\r1004\t9,5,4,1,5,6,3,2,1,4\t2\r1005\t2,2,2,2,2,1,4,4,4,4\t2\r1006\t6,4,5,8,1,1,4,3,7,2\t0\r1007\t2,5,8,9,1,6,1,5,6,7\t0\r1008\t2,5,6,7,8,9,2,1,1,1\t0\r1009\t3,1,2,4,1,5,6,7,1,2\t1\r1010\t7,9,5,1,2,3,4,4,1,8\t2\r1011\t5,8,7,7,5,6,1,2,2,8\t1\r1012\t4,1,6,8,3,2,7,1,6,5\t0\r　形式C——一列型：一共就一列，用分隔符隔开了不同含义的数据列。\nid|feature1|feature2|...|feature10|label\r1001|1|5|2|3|8|9|2|1|1|3|1\r1002|2|8|9|4|4|4|6|1|0|7|1\r1003|5|6|9|7|4|1|2|5|1|3|1\r1004|9|5|4|1|5|6|3|2|1|4|2\r1005|2|2|2|2|2|1|4|4|4|4|2\r1006|6|4|5|8|1|1|4|3|7|2|0\r1007|2|5|8|9|1|6|1|5|6|7|0\r1008|2|5|6|7|8|9|2|1|1|1|0\r1009|3|1|2|4|1|5|6|7|1|2|1\r1010|7|9|5|1|2|3|4|4|1|8|2\r1011|5|8|7|7|5|6|1|2|2|8|1\r1012|4|1|6|8|3|2|7|1|6|5|0\r　注意不要把数据处理成下面的形式，这样的数据如果用pd.read_csv('xxx.csv', sep='|')读入，由于,和|同时存在，导致无法把1001|1,5,2,3,8,9,2,1,1,3|1分成12列，而是分成三列，第一列的值是1001|1,5,2,3,8,9,2,1,1,3|1，第二三列都是nan。\nid|feature|label\r1001|1,5,2,3,8,9,2,1,1,3|1\r1002|2,8,9,4,4,4,6,1,0,7|1\r1003|5,6,9,7,4,1,2,5,1,3|1\r1004|9,5,4,1,5,6,3,2,1,4|2\r1005|2,2,2,2,2,1,4,4,4,4|2\r1006|6,4,5,8,1,1,4,3,7,2|0\r1007|2,5,8,9,1,6,1,5,6,7|0\r1008|2,5,6,7,8,9,2,1,1,1|0\r1009|3,1,2,4,1,5,6,7,1,2|1\r1010|7,9,5,1,2,3,4,4,1,8|2\r1011|5,8,7,7,5,6,1,2,2,8|1\r1012|4,1,6,8,3,2,7,1,6,5|0\r　对于形式B和C，我们通常都要处理成形式A的格式，方便处理。以上格式都是处理数据集过程中遇到的形式，如果遇到新的再更新。\n1.1 Pandas读取并处理CSV文件 　在数据集较小的情况下，我们通常使用pandas读入数据，它提供的数据层次化结构非常方便，可以让我们快速操作数据集。我们存储了三种形式的csv文件，他们的数据内容完全相同，只有形式不同，分别命名为trainA.csv、trainB.csv和trainC.csv。\n　当数据集文件是A的形式时，我们可以直接用pd.read_csv(\u0026lt;文件路径\u0026gt;)来读取数据集，我们先直接读取三个文件，查看他们的形状（shape，即几行几列）：\n1 2 3 4 5 6 7 8 9 10 11 12  import pandas as pd train_dataA = pd.read_csv(\u0026#39;trainA.csv\u0026#39;) train_dataB = pd.read_csv(\u0026#39;trainB.csv\u0026#39;) train_dataC = pd.read_csv(\u0026#39;trainC.csv\u0026#39;) print(train_dataA.shape) print(train_dataB.shape) print(train_dataC.shape) # 输出： (12, 12) # A是12行，12列 (12, 3) # B是12行，3列 (12, 1) # C是12行，1列   　为了便于处理，我们希望将形式B和形式C都转换成形式A的形式，形式C转换成形式A的格式非常简单，只需要在read_csv中增加一个sep参数，他表示分隔符，将C读成12行12列的格式的代码如下：\n1 2 3  import pandas as pd train_dataC = pd.read_csv(\u0026#39;trainC.csv\u0026#39;,sep=\u0026#34;|\u0026#34;) print(train_dataC.shape)   　将形式B转换为形式C的操作稍微复杂，需要先提取出feature列，然后用feature列str属性的split方法，将feature列转换成10列，然后再将id和label列加回去即可。需要注意的是这里str是pandas.core.strings.accessor.StringMethods类，不是字符串，这种用法和python中的s1 = a.__str__()原理类似，s1是一个类，而s2 = str(\u0026lt;string\u0026gt;)是调用了str()方法，s2是一个字符串，是类的一个实例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import pandas as pd # 提出feature列 train_dataB = pd.read_csv(\u0026#39;trainB.csv\u0026#39;) train_B_feature = train_dataB[\u0026#39;feature\u0026#39;] # \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; print(type(train_B_feature)) # 将feature列的每一行都转换为str对象，再用\u0026#39;,\u0026#39;，作为分隔符分开，设置expand参数为True，将分隔结果展开。 train_B_feature_expand = train_B_feature.str.split(\u0026#39;,\u0026#39;,expand = True) print(train_B_feature_expand) # \u0026lt;class \u0026#39;pandas.core.strings.accessor.StringMethods\u0026#39;\u0026gt; print(type(train_B_feature_expand.str)) # \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; print(type(train_B_feature_expand)) # 加入id和label列，train_B_feature_expand就是展开的train_dataB train_B_feature_expand[\u0026#39;id\u0026#39;] = train_dataB[\u0026#39;id\u0026#39;] train_B_feature_expand[\u0026#39;label\u0026#39;] = train_dataB[\u0026#39;label\u0026#39;] print(train_B_feature_expand) \u0026gt;\u0026gt;\u0026gt; 0 1 2 3 4 5 6 7 8 9 id label 0 1 5 2 3 8 9 2 1 1 3 1001 1 1 2 8 9 4 4 4 6 1 0 7 1002 1 2 5 6 9 7 4 1 2 5 1 3 1003 1 3 9 5 4 1 5 6 3 2 1 4 1004 2 4 2 2 2 2 2 1 4 4 4 4 1005 2 5 6 4 5 8 1 1 4 3 7 2 1006 0 6 2 5 8 9 1 6 1 5 6 7 1007 0 7 2 5 6 7 8 9 2 1 1 1 1008 0 8 3 1 2 4 1 5 6 7 1 2 1009 1 9 7 9 5 1 2 3 4 4 1 8 1010 2 10 5 8 7 7 5 6 1 2 2 8 1011 1 11 4 1 6 8 3 2 7 1 6 5 1012 0   　注意expand参数必须设置为True才会展开，即一个分隔符为一列，否则将会得到下面这样的一列结果，它的类型不是DataFrame而是Series\n0 [1, 5, 2, 3, 8, 9, 2, 1, 1, 3]\r1 [2, 8, 9, 4, 4, 4, 6, 1, 0, 7]\r2 [5, 6, 9, 7, 4, 1, 2, 5, 1, 3]\r3 [9, 5, 4, 1, 5, 6, 3, 2, 1, 4]\r4 [2, 2, 2, 2, 2, 1, 4, 4, 4, 4]\r5 [6, 4, 5, 8, 1, 1, 4, 3, 7, 2]\r6 [2, 5, 8, 9, 1, 6, 1, 5, 6, 7]\r7 [2, 5, 6, 7, 8, 9, 2, 1, 1, 1]\r8 [3, 1, 2, 4, 1, 5, 6, 7, 1, 2]\r9 [7, 9, 5, 1, 2, 3, 4, 4, 1, 8]\r10 [5, 8, 7, 7, 5, 6, 1, 2, 2, 8]\r11 [4, 1, 6, 8, 3, 2, 7, 1, 6, 5]\rName: feature, dtype: object\r\u0026lt;class 'pandas.core.series.Series'\u0026gt;\r还有时我们会读取到没有列说明的csv文件，比如下面这样train_no_col_index.csv，\n1 2 3 4 5 6 7 8 9 10 11 12  1001\t1,5,2,3,8,9,2,1,1,3\t1 1002\t2,8,9,4,4,4,6,1,0,7\t1 1003\t5,6,9,7,4,1,2,5,1,3\t1 1004\t9,5,4,1,5,6,3,2,1,4\t2 1005\t2,2,2,2,2,1,4,4,4,4\t2 1006\t6,4,5,8,1,1,4,3,7,2\t0 1007\t2,5,8,9,1,6,1,5,6,7\t0 1008\t2,5,6,7,8,9,2,1,1,1\t0 1009\t3,1,2,4,1,5,6,7,1,2\t1 1010\t7,9,5,1,2,3,4,4,1,8\t2 1011\t5,8,7,7,5,6,1,2,2,8\t1 1012\t4,1,6,8,3,2,7,1,6,5\t0   这时我们只需要设置header=None，就可以正常读入，如果不设置的话，read_csv会默认将第一行设置为列索引。\n1    1.2 按行读入CSV/txt文件 　有时还会采用逐行读取的方式，适用于不同csv文件之间存在关联的情况，比如A.csv的key是B.csv的一个属性。文件逐行读取时将每行作为一个字符串依次读入，我们可以对这个读入的字符串进行任意分割、拆分、重组，重新写入到新的csv文件中，伪代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12  with open(source_file, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as fp, open(target_file, \u0026#34;w\u0026#34;) as gp: # 逐行读取 for line in fp: # 读取每一列的值，strip()是移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。 # line.strip().split(\u0026#34;|\u0026#34;)是一个list，形如[1,4,5,...], # line.strip().split(\u0026#34;|\u0026#34;)[0]就是取这个list索引为0的元素，就是1。 col_0 = line.strip().split(\u0026#34;|\u0026#34;)[0] col_1 = line.strip().split(\u0026#34;|\u0026#34;)[1] ... col_n = line.strip().split(\u0026#34;|\u0026#34;)[n] # 将想要提取的列写入目标文件 gp.write(\u0026#34;{}|{}|...|{}\\n\u0026#34;.format(col_0, col_1, ..., col_n))    tqdm详细使用：https://github.com/tqdm/tqdm\n 　有时文件过大，不知道文件读取到什么位置，也不想输出到屏幕或日志文件，可以用tqdm包为文件读取增加进度条，将可迭代对象作为参数传递给tqdm()即可。简单用法如下（高级用法可查看github页面，这里仅需要简单用法就足够）：\n1 2 3 4  from tqdm import tqdm with open(source_file, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as fp, open(target_file, \u0026#34;w\u0026#34;) as gp: for line in tqdm(fp, \u0026#34;[INFO] loading from {} line-by-line...\u0026#34;.format(source_file_name)): ...   　输出在屏幕上的进度样式如下:\n1 2 3 4 5  # 读文件 [INFO] loading from source_file line-by-line...: 2000000it [00:11, 177291.19it/s] # 写文件 [INFO] write id and index to file...: 100%|███████████████████████████████████████████████████████████████| 351/351 [00:00\u0026lt;00:00, 846092.36it/s]   　后续遇到数据集非常非常大需要采样读取会更新采样读取，目前还没接触过，暂时不提。\n1.3 读取npy文件 　读取npy文件可以使用numpy读入。\n1  np_file = np.load(\u0026#39;data.npy\u0026#39;)   2 数据模型  参考和引用：\npython pandas stack和unstack函数：https://www.cnblogs.com/bambipai/p/7658311.html\n 　在pandas处理数据时，会用到stack()和unstack()方法，借用第一个参考链接的说法，stack()函数将数据从表格结构转换为花括号结构，即从图1转换到图2；而unstack()函数将花括号结构转换成表格结构，即从图2转换到图1。\n图1 表格结构:\r\r 图2 花括号结构:\r\r  参考和引用：\n三种数据模型\u0026mdash;层次模型、网状模型以及关系模型：https://www.cnblogs.com/daniumeng/p/8717438.html\n数据模型包括：层次模型（树状图）、网状模型（有向图）、关系模型（表格），这里我们不详细说明，想要具体了解可以进入参考连接阅读。\n 　这两种结构实际上是数据模型中的层次模型（花括号，图2）和关系模型（表格，图1）。层次模型是一个树状图，它只有列方向上的索引，结构上偏向堆叠，将原来m行n列的元素按照[0][0]、[0][1]、[0][2]、\u0026hellip;、[0][n]、[1][0]、[1][1]、[1][2]、\u0026hellip;、[1][n]、\u0026hellip;、[m][n]的顺序堆成一列，所以将关系模型（图1）转换成层次模型时，调用的是stack()函数；将层次模型转换成关系模型时，使用unstack()函数，用stack()和unstack()函数进行图1和图2的转化代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import numpy as np import pandas as pd # np.arange(1,7) = [1,2,3,4,5,6] # reshape方法将数组转换为2行3列的形状 # np.arange(1,7).reshape((2,3)) = [ [1, 2, 3], [4, 5, 6] ] data = pd.DataFrame(np.arange(1,7).reshape((2,3)), index=pd.Index([\u0026#39;street1\u0026#39;,\u0026#39;street2\u0026#39;]), columns=pd.Index([\u0026#39;one\u0026#39;,\u0026#39;two\u0026#39;,\u0026#39;three\u0026#39;])) print(data) print(\u0026#39;-----------------------------------------\\n\u0026#39;) data_tree = data.stack() print(data_tree) print(type(data_tree)) print(data_tree.shape) print(\u0026#39;-----------------------------------------\\n\u0026#39;) data_table = data_tree.unstack() print(data_table) print(type(data_table)) print(data_table.shape)   　输出如下，其中(6,)表示有6个元素的一维数组，(2,3)表示2行3列。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  one two three street1 1 2 3 street2 4 5 6 ----------------------------------------- street1 one 1 two 2 three 3 street2 one 4 two 5 three 6 dtype: int32 \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; (6,) ----------------------------------------- one two three street1 1 2 3 street2 4 5 6 \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; (2, 3)   　常用stack()将数据转列处理，将各维特征用作时间序列中的时间信号。\n3 筛选数据 　很多网上的实践示例代码对数据进行了一些准备工作，但基本没有注释，很难看懂，比如取行元素、列元素，重置索引，将某列列为索引等等，想不出要这样做的原因。各种方法交织在一起，五（luan）花（qi）八（ba）门（zao），水平各不相同的示例让人眼花缭乱，然后头晕程度加倍😱，到最后也没看懂。本节对取指定行列元素和索引修改方法进行归纳总结。\n　同样地，我们先导入pandas和numpy。\n1 2  import numpy as np import pandas as pd   3.1 DataFrame定制索引  参考：https://zhuanlan.zhihu.com/p/110819220?from_voters_page=true\n 　对索引的操作也是数据格式处理时的常用操作。我们先读入数据集trainB.csv，然后对特征进行转列处理。可以看到输出变成了第2节提到的层次模型，这里前两列是它的索引信息，并不是数据，所以可以看到它的形状是一个有120个元素的一维数组。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  train_dataB = pd.read_csv(\u0026#39;trainB.csv\u0026#39;) train_dataB_feature = train_dataB[\u0026#39;feature\u0026#39;].str.split(\u0026#34;,\u0026#34;, expand=True).stack() print(train_dataB_feature) print(train_dataB_feature.shape) # 输出 0 0 1 1 5 2 2 3 3 4 8 .. 11 5 2 6 7 7 1 8 6 9 5 Length: 120, dtype: object (120,)   　（1）重置行索引：DataFrame.reset_index()\n　reset_index()重置DataFrame数据的行索引，如果没有参数，则添加一列从0开始的连续自然数作为行索引，并将原来的索引列添加为数据列。\n1  DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill=\u0026#39;\u0026#39;)    level：类型可以是int、str、tuple或list。表示删除的索引级别，仅从索引中删除给定级别。默认值为None，表示移除所有级别的索引。 **drop：**布尔类型，当值为False时，原有索引列会变成数据列，否则原索引列会被删除。默认值为False。 inplace：布尔类型，表示是否在原数据上操作，默认为False。 col_level：类型为int或str，默认值为0，如果列有多个级别，则确定将列标签插入到哪个级别。默认情况下，它将插入到第一级。 **col_fill：**对象，默认为空字符串''，如果列有多个级别，则确定其他级别的命名方式。如果没有，则重复索引名。  　常用于以下情况：\n 数据清洗后，会将带空值的行删除，此时DataFrame或Series类型的数据不再是连续的索引，可以使用reset_index()重置索引。 将数据转列处理后，希望将原有索引转换为数据列。  　这里我们以将原有索引转换为数据列为例，注意这里不能写train_dataB_feature.reset_index(inplace=True)，因为train_dataB_feature是Series类型，而reset_index不能将原有的Series类型转换为DataFrame类型，但是可以根据原有的Series类型创建一个新的DataFrame类型的变量。同时我们也可以看到，原来的两级索引分别对应于level_0，level_1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  train_dataB_feature = train_dataB_feature.reset_index() print(train_dataB_feature) print(type(train_dataB_feature)) print(train_dataB_feature.shape) # 输出 level_0 level_1 0 0 0 0 1 1 0 1 5 2 0 2 2 3 0 3 3 4 0 4 8 .. ... ... .. 115 11 5 2 116 11 6 7 117 11 7 1 118 11 8 6 119 11 9 5 [120 rows x 3 columns] \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; (120, 3)   　（2）设置某一列作为行索引：DataFrame.set_index()\n1  DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)    keys：列标签（如上面的level_0）/列标签列表，这一/几列将被设置为行索引。 drop：布尔类型，值为True表示设置为索引的这一列不再是数据，默认值为True。 append：布尔类型，表示是否将列附加到现有索引，即新增一级索引列，默认值为False。 inplace：布尔类型，表示是否直接在当前数据上操作，默认值False。 verify_integrity：布尔类型，没看懂，将其设置为false将提高该方法的性能，默认值为False。  　我们将上面的level_0设置为新索引，这次我们可以直接在train_dataB_feature上修改，因为它已经被修改为DataFrame类型。可以看到原有索引列被替换，并且level_0不再是数据列。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  train_dataB_feature.set_index(\u0026#39;level_0\u0026#39;,inplace=True) print(train_dataB_feature) print(train_dataB_feature.shape) # 输出 level_1 0 level_0 0 0 1 0 1 5 0 2 2 0 3 3 0 4 8 ... ... .. 11 5 2 11 6 7 11 7 1 11 8 6 11 9 5 [120 rows x 2 columns] (120, 2)   　（3）索引名称重命名：DataFrame.rename()\n　DataFrame的索引名称定义在DataFrame.index.name和DataFrame.columns.name中。行索引和列索引都可以重命名，通常行索引用index或axis=0表示，列索引用columns或axis=1表示。索引级别也可以重命名，暂时用不到，不做介绍。可以用DataFrame.rename()方法重命名索引名称。\n1  DataFrame.rename(mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors=\u0026#39;ignore\u0026#39;)   　使用时传入字典形状的参数给index就可以修改行索引，传递给columns就可以修改列索引名称，其余参数没有弄清楚文档说的是什么意思，暂时不写。\n　我们接着上面的示例，将列索引重新命名，我们将10维特征表示为一个有10个信号的时间序列特征，共12个样本，每个样本共10个信号，level_1重命名为time，表示时间步，即具有相同时间间隔的时刻，将0列重命名为signal，表示该时刻的时间信号；同时取消行索引的名称。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # 取消行索引 train_dataB_feature.index.name = None # 修改列索引名称 train_dataB_feature.rename(columns={\u0026#34;level_1\u0026#34;:\u0026#34;time\u0026#34;, 0:\u0026#34;signals\u0026#34;}, inplace=True) print(train_dataB_feature) # 输出 time signals 0 0 1 0 1 5 0 2 2 0 3 3 0 4 8 .. ... .. 11 5 2 11 6 7 11 7 1 11 8 6 11 9 5 [120 rows x 2 columns]   3.2 数据类型转换 　（1）DataFrame数据类型转换：DataFrame.astype()\n　在DataFrame中数据类型不能用强制类型转换，需要使用.astype(\u0026lt;类型\u0026gt;)方法，可以转换一列的数据类型，接上面3.1的例子，我们将signals从整型转换成float类型。\n　可以从输出中看到，signals的数据添加了小数点，而time的数据类型没有发生变化，我们用info输出数据基本信息时也可以看到，数据类型是以列为单位的，同一列的数据类型是统一的，我们不能更改某一个元素的类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  train_dataB_feature[\u0026#39;signals\u0026#39;] = train_dataB_feature[\u0026#39;signals\u0026#39;].astype(float) print(train_dataB_feature) print(train_dataB_feature.info()) # 输出 time signals 0 0 1.0 0 1 5.0 0 2 2.0 0 3 3.0 0 4 8.0 .. ... ... 11 5 2.0 11 6 7.0 11 7 1.0 11 8 6.0 11 9 5.0 [120 rows x 2 columns] \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; Int64Index: 120 entries, 0 to 11 Data columns (total 2 columns): # Column Non-Null Count Dtype  --- ------ -------------- ----- 0 time 120 non-null int64 1 signals 120 non-null float64 dtypes: float64(1), int64(1) memory usage: 2.8 KB   3.3 DataFrame操作指定行列数据  参考：\npandas获得指定行_pandas取dataframe特定行/列：https://blog.csdn.net/weixin_39586825/article/details/111758506\n 　我们以trainA.csv的数据为例进行操作，首先导入pandas包并读入trainA.csv。\n1 2 3 4 5 6  # DataFrame行列操作 import pandas as pd # 读入数据 train_dataA = pd.read_csv(\u0026#39;/kaggle/input/d/xianyuice/basic-opt/trainA.csv\u0026#39;) train_dataA.head()   3.3.1 取行/列 　DataFrame取行列数据比数组稍微复杂，具体可进入参考链接查询，这里只介绍最常用的几种操作。\n　（1）直接取列数据\n 取1列：dataframe['\u0026lt;col_name\u0026gt;']； 取2列：dataframe[[\u0026lt;col_name1\u0026gt;', \u0026lt;col_name\u0026gt;2']]。  　注意取1列时得到的数据类型是Series。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # 取1列，取feature2列 feature2 = train_dataA[\u0026#39;feature2\u0026#39;] print(feature2) print(type(feature2)) # 取2列， dataframe[[\u0026lt;col_name1\u0026gt;\u0026#39;, \u0026lt;col_name\u0026gt;2\u0026#39;]] feature23 = train_dataA[[\u0026#39;feature2\u0026#39;,\u0026#39;feature3\u0026#39;]] print(feature23) print(type(feature23)) # 输出 0 5 1 8 2 6 ... ... 11 1 Name: feature2, dtype: int64 \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; feature2 feature3 0 5 2 1 8 9 2 6 9 ... ... ... 11 1 6 \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;   　（2）通过DataFrame.loc取行列\n　用loc[]取行列时只能通过index和columns定义的索引名称获取行和列，索引名称如列名称feature1、feature2……，注意此时我们的trainA_data的行索引是没有名称的，我们可以查看验证一下：\n1 2 3 4  print(train_dataA.index.name) # 输出 None   　如果这时候使用loc取行元素会报错，因为它只能通过索引名称来获取行和列，而行索引没有名称，我们为行索引添加名称row_\u0026lt;index\u0026gt;，并再次输出行索引。（但不知道为什么rename并不会更改.name的值，.name仍然为none）\n1 2 3 4 5 6 7 8 9 10 11 12  row_idx_name_dict = {} for i in range(12): row_idx_name_dict[i] = \u0026#34;row_\u0026#34; + str(i) train_dataA.rename(index=row_idx_name_dict, inplace=True) print(train_dataA.index.name) # 输出 id feature1 feature2 feature3 feature4 feature5 feature6 ... row_0 1001 1 5 2 3 8 ... row_1 1002 2 8 9 4 4 ... None   　这时我们再用loc获取指定行和列：\n 取1行：train_dataA.loc['row_0'] 取连续行：train_dataA.loc['row_0':'row_2'] 取多个不连续行：train_dataA.loc[['row_0','row_3']] 取1列：train_dataA.loc[:,'feature2'] 取连续列：train_dataA.loc[:,'feature1':'feature3'] 取多个不连续列：train_dataA.loc[:,['feature2','feature4']]  1 2 3 4 5 6  print(train_dataA.loc[\u0026#39;row_0\u0026#39;]) print(train_dataA.loc[\u0026#39;row_0\u0026#39;:\u0026#39;row_2\u0026#39;]) print(train_dataA.loc[[\u0026#39;row_0\u0026#39;,\u0026#39;row_3\u0026#39;]]) print(train_dataA.loc[:,\u0026#39;feature2\u0026#39;]) print(train_dataA.loc[:,\u0026#39;feature1\u0026#39;:\u0026#39;feature3\u0026#39;]) print(train_dataA.loc[:,[\u0026#39;feature2\u0026#39;,\u0026#39;feature4\u0026#39;]])   　（3）通过DataFrame.iloc获取行和列\n　用loc[]取行列时只能通过数字标号获取行和列，具体操作如下：\n 取1行：train_dataA.iloc[0] 取连续行：train_dataA.iloc[0:2] 取多个不连续行：train_dataA.iloc[[0,3]] 取1列：train_dataA.iloc[:,1] 取连续列：train_dataA.iloc[:,1:3] 取多个不连续列：train_dataA.iloc[:,[2,4]]  1 2 3 4 5 6  print(train_dataA.iloc[0]) print(train_dataA.iloc[0:2]) print(train_dataA.iloc[[0,3]]) print(train_dataA.iloc[:,1]) print(train_dataA.iloc[:,1:3]) print(train_dataA.iloc[:,[2,4]])   　（4）按条件取行\n1 2 3 4 5 6 7 8 9 10 11  # 选取某些列等于某些值的行记录,不等于用!= # df.loc[df[\u0026#39;column_name\u0026#39;] == some_value] some_rows = train_dataA[train_dataA[\u0026#39;feature1\u0026#39;]==2] print(some_rows) # 多种条件的选取 用\u0026amp;连接，注意要加括号 # df.loc[(df[\u0026#39;column\u0026#39;] == some_value) \u0026amp; df[\u0026#39;other_column\u0026#39;].isin(some_values)] some_rows = train_dataA[(train_dataA[\u0026#39;feature1\u0026#39;]==2) \u0026amp; (train_dataA[\u0026#39;feature2\u0026#39;]==5)] print(some_rows)   3.3.2 添加行/列 　添加列直接Dataframe['\u0026lt;新列名\u0026gt;']=新列即可。可以看到下面的代码中成功添加new_col列。\n1 2 3 4 5 6 7 8 9  new_cow = [8,8,8,8,8,8,8,8,8,8,8,8] train_dataA[\u0026#39;new_col\u0026#39;] = new_cow train_dataA.head() # 输出 id\tfeature1\tfeature2\tfeature3\t...\tlabel new_col row_0\t1001\t1\t5\t2\t... 1 8 row_1\t1002\t2\t8\t9\t... 1 8 ...   3.3.3 删除行/列 　删除行或列使用Dataframe.drop()方法，删除行设置参数axis=0，删除列设置参数axis=1。我们将刚刚添加的new_col列删除。\n1 2 3 4 5 6 7 8  train_dataA.drop(\u0026#34;new_col\u0026#34;, axis=1, inplace=True) train_dataA.head() # 输出 id\tfeature1\tfeature2\tfeature3\t...\tlabel row_0\t1001\t1\t5\t2\t... 1 row_1\t1002\t2\t8\t9\t... 1 ...   3.4 ndarray操作指定行列数据 　ndarray取元素的操作就像数组一样，通过行列索引获取，索引从0开始，例如arr[0][2]。\n　ndarray取行列和dataframe.iloc[]方法相似，区别在于取多个不连续行列时的操作。\n　ndarray定义了特别的取整行数据和整列数据的操作，这些操作list是不支持的，可以将list转为ndarray类型再做操作，操作方法如下。但需要注意的是，不同的操作可能得到的数据内容相同，但得到的数据形状不同，在作为模型输入时可能会由于形状不同而报错，所以需要对每种操作得到的数据形状有所了解。\n ndarray[i,:]取第i行数据，ndarray[i:j, :] 取第i行到第j-1行的数据； ndarray[:,j]取第j列数据（以行形式返回），ndarray[:, i:j]取第i列到第j-1列的数据。  　我们生成一个3行4列的ndarray数组来进行上述操作，并输出得到的形状。\n1 2 3 4 5 6 7  a_ndarr = np.arange(12).reshape((3, 4)) print(a_ndarr) # 输出 [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]   　（1）取一个指定行列的数据。\n1 2 3 4  print(a_ndarr[0][2]) # 输出 2   　（2）取第i行。注意这里两种取法得到的形状不同。取法1得到的是一个一维数组，它有4个元素；而取法2得到的是一个二维数组，它有4行4列。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # 取第1行，取法1 a_row_1 = a_ndarr[1,:] print(a_row_1) print(a_row_1.shape) # 取第1行，取法2 a_row_1_new = a_ndarr[1:2,:] print(a_row_1_new) print(a_row_1_new.shape) # 输出 [4 5 6 7] (4,) [[4 5 6 7]] (1, 4)   　（3）取第j列。和取整行元素相似，如果想得到一维数组则使用标号，如果想得到二维数组则使用冒号标号。这里取法1得到了一个具有3个元素的一维数组；而取法2得到了一个有三行1列的二维数组。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # 取第2列，取法1 a_col_2 = a_ndarr[:,2] print(a_col_2) print(a_col_2.shape) # 取第2列，取法2 a_col_2_new = a_ndarr[:,2:3] print(a_col_2_new) print(a_col_2_new.shape) # 输出 [ 2 6 10] (3,) [[ 2] [ 6] [10]] (3, 1)   　（4）取i到j-1行，如果是从第0行/列开始，0可以省略。取第i-j行，不取列元素，还可以省去,和:，下面两种写法等价\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # 取1-2行 a_row_1to2 = a_ndarr[1:3,:] print(a_row_1to2) print(a_row_1to2.shape) # 取1-2行 a_row_1to2 = a_ndarr[1:3] print(a_row_1to2) print(a_row_1to2.shape) # 输出 [[ 4 5 6 7] [ 8 9 10 11]] (2, 4) [[ 4 5 6 7] [ 8 9 10 11]] (2, 4)   　（5）取i到j-1列，如果是从第0行/列开始，0可以省略。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # 取0-3列 a_cow_0to2 = a_ndarr[:,0:3] print(a_cow_0to2) print(a_cow_0to2.shape) # 取0-3列，省略0 a_cow_0to2 = a_ndarr[:,:3] print(a_cow_0to2) print(a_cow_0to2.shape) # 输出 [[ 0 1 2] [ 4 5 6] [ 8 9 10]] (3, 3) [[ 0 1 2] [ 4 5 6] [ 8 9 10]] (3, 3)   　注意3前面的:不能省略，不要写成a_ndarr[:,3]，这样是取第3列的意思，并且它的形状是一维数组。\n取连续地行和列将（4）（5）合并起来写即可，形如a_ndarr[0:2,1:3]。\n　（7）取多个不连续的指定行\n1 2 3 4 5 6 7 8 9  # 取第0行和第2行 a_row02 = a_ndarr[[0,2]] print(a_row02) print(a_row02.shape) # 输出 [[ 0 1 2 3] [ 8 9 10 11]] (2, 4)   　（8）取多个不连续的指定列\n1 2 3 4 5 6 7 8 9 10  # 取第1列和第3列 a_col13 = a_ndarr[:,[1,3]] print(a_col13) print(a_col13.shape) # 输出 [[ 1 3] [ 5 7] [ 9 11]] (3, 2)   　（9）取指定的不连续行和列，注意这里和DataFrame的iloc方法不同，比如要取0行和2行第0、1、3列的数据，不可以写成a_ndarr[[0,2], [0,1,3]]，会报错。正确写法应该先取出想要的行数据，再从行数据中取出想要的列数据，写法如下：\n1 2 3 4 5 6 7 8 9  a_row02_col013 = a_ndarr[[0,2]] a_row02_col013 = a_row02_col013[:, [0,1,3]] print(a_row02_col013) print(a_row02_col013 .shape) # 输出 [[ 0 1 3] [ 8 9 11]] (2, 3)   　（10）取多个指定元素。在（6）种我们提到的错误写法实际上也是可以运行的，但前提是a_ndarr[[list1],[list2]]中list1和list2的形状相同，我们取到的元素是a_ndarr[list_1_0][list_2_0]、a_ndarr[list_1_1][list_2_1]……。用法示例如下，可以在输出结果中看到，这样写我们取到的元素实际上是[0][0]和[2][3]两个元素。\n1 2 3 4 5 6 7  a_row02_col03 = a_ndarr[[0,2], [0,3]] print(a_row02_col03) print(a_row02_col03.shape) # 输出 [ 0 11] (2,)   4 DataFrame、ndarray、list之间的转换 　我们经常能看见机器学习数据处理当中混用pandas的DataFrame、numpy的ndarray、Python的list，读入数据时用pands，处理处理就变成了numpy，调用方法五（luan）花（qi）八（ba）门（zao）。它们的结构相似，一般被统一描述为array like。这些array like在形式上是n维向量，输出结果看起来形状相似，难以区分。但实际上由于他们属于不同的包，具有不同的类型，有些情况输入类型是array like形式时可以混用，他们最大的区别在于可以调用他们所属类独特的方法和属性。\n　我们通过几个例子来感受一下他们有什么区别，首先导入pandas包和numpy包，用pd.read_csv读入第1节中提供的数据集。\n1 2 3 4  import numpy as np import pandas as pd train_dataA = pd.read_csv(\u0026#39;trainA.csv\u0026#39;)   4.1 DataFrame \u0026lt;-\u0026gt; ndarray DataFrame -\u0026gt; ndarray （1）DataFrame.values\n　DataFrame是数据层次化结构，包含行标号、列标号、数据等信息，方便我们理解和查看，而ndarray是一个n维数组，没有行号和列号，只有数据，所以可以推测，DataFrame的value是一个ndarray类型。然后我们用.values属性进行验证。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # DataFrame -\u0026gt; ndarray train_dataA_value = train_dataA.values print(train_dataA_value) print(type(train_dataA_value)) # 输出 [[1001 1 5 2 3 8 9 2 1 1 3 1] [1002 2 8 9 4 4 4 6 1 0 7 1] [1003 5 6 9 7 4 1 2 5 1 3 1] [1004 9 5 4 1 5 6 3 2 1 4 2] [1005 2 2 2 2 2 1 4 4 4 4 2] [1006 6 4 5 8 1 1 4 3 7 2 0] [1007 2 5 8 9 1 6 1 5 6 7 0] [1008 2 5 6 7 8 9 2 1 1 1 0] [1009 3 1 2 4 1 5 6 7 1 2 1] [1010 7 9 5 1 2 3 4 4 1 8 2] [1011 5 8 7 7 5 6 1 2 2 8 1] [1012 4 1 6 8 3 2 7 1 6 5 0]] \u0026lt;class \u0026#39;numpy.ndarray\u0026#39;\u0026gt;   （2）numpy.array\n　除了通过DataFrame的.values将DataFrame转换为ndarray格式之外，还可以通过numpy.array(DataFrame)将DataFrame转换成ndarray格式，得到的输出结果和DataFrame.values相同。\n1 2 3 4 5 6  # DataFrame -\u0026gt; ndarray train_data_ndarray = np.array(train_dataA) print(train_data_ndarray) print(type(train_data_ndarray)) #输出和（1）中的方法相同   ndarray-\u0026gt;DataFrame 　ndarray转换为DataFrame格式也非常简单，只需将ndarray类型的数据传入pd.DataFrame()方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # ndarray-\u0026gt;DataFrame train_data_df = pd.DataFrame(train_data_ndarray) print(train_data_df) print(type(train_data_df)) # 输出 0 1 2 3 4 5 6 7 8 9 10 11 0 1001 1 5 2 3 8 9 2 1 1 3 1 1 1002 2 8 9 4 4 4 6 1 0 7 1 2 1003 5 6 9 7 4 1 2 5 1 3 1 3 1004 9 5 4 1 5 6 3 2 1 4 2 4 1005 2 2 2 2 2 1 4 4 4 4 2 5 1006 6 4 5 8 1 1 4 3 7 2 0 6 1007 2 5 8 9 1 6 1 5 6 7 0 7 1008 2 5 6 7 8 9 2 1 1 1 0 8 1009 3 1 2 4 1 5 6 7 1 2 1 9 1010 7 9 5 1 2 3 4 4 1 8 2 10 1011 5 8 7 7 5 6 1 2 2 8 1 11 1012 4 1 6 8 3 2 7 1 6 5 0 \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;   　需要注意的是如果是DataFrame-\u0026gt;ndarray-\u0026gt;DataFrame，会损失原有的行标号和列标号名称，只有从0开始的索引。\n4.2 ndarray \u0026lt;-\u0026gt; list 　（1）ndarray-\u0026gt;list\n　ndarray和list的相互转换非常容易，由ndarray转到list只需要调用.tolist()方法。\n1 2 3 4 5 6 7 8  # ndarray -\u0026gt; list train_data_list = train_data_ndarray.tolist() print(train_data_list) print(type(train_data_list)) # 输出 [[1001, 1, 5, 2, 3, 8, 9, 2, 1, 1, 3, 1], [1002, 2, 8, 9, 4, 4, 4, 6, 1, 0, 7, 1], [1003, 5, 6, 9, 7, 4, 1, 2, 5, 1, 3, 1], [1004, 9, 5, 4, 1, 5, 6, 3, 2, 1, 4, 2], [1005, 2, 2, 2, 2, 2, 1, 4, 4, 4, 4, 2], [1006, 6, 4, 5, 8, 1, 1, 4, 3, 7, 2, 0], [1007, 2, 5, 8, 9, 1, 6, 1, 5, 6, 7, 0], [1008, 2, 5, 6, 7, 8, 9, 2, 1, 1, 1, 0], [1009, 3, 1, 2, 4, 1, 5, 6, 7, 1, 2, 1], [1010, 7, 9, 5, 1, 2, 3, 4, 4, 1, 8, 2], [1011, 5, 8, 7, 7, 5, 6, 1, 2, 2, 8, 1], [1012, 4, 1, 6, 8, 3, 2, 7, 1, 6, 5, 0]] \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;   　（2）list-\u0026gt;ndarray\n　list转到ndarray也只需调用numpy.array(list)，就能将list类型转换为ndarray类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # list -\u0026gt; ndarray train_data_ndarray = np.array(train_data_list) print(train_data_ndarray) print(type(train_data_ndarray)) # 输出 [[1001 1 5 2 3 8 9 2 1 1 3 1] [1002 2 8 9 4 4 4 6 1 0 7 1] [1003 5 6 9 7 4 1 2 5 1 3 1] [1004 9 5 4 1 5 6 3 2 1 4 2] [1005 2 2 2 2 2 1 4 4 4 4 2] [1006 6 4 5 8 1 1 4 3 7 2 0] [1007 2 5 8 9 1 6 1 5 6 7 0] [1008 2 5 6 7 8 9 2 1 1 1 0] [1009 3 1 2 4 1 5 6 7 1 2 1] [1010 7 9 5 1 2 3 4 4 1 8 2] [1011 5 8 7 7 5 6 1 2 2 8 1] [1012 4 1 6 8 3 2 7 1 6 5 0]] \u0026lt;class \u0026#39;numpy.ndarray\u0026#39;\u0026gt;   　从上面的转换中可以看出ndarray和list在输出时，list的元素之间用,隔开，结构紧凑；而ndarray的元素用空格隔开，整齐美观。\n4.3 DataFrame -\u0026gt; list 　可以先将DataFrame转换成ndarray类型，再将ndarray类型转换为list类型，转换方法参考4.1和4.2节。\n5 处理多个数据文件  参考1：https://zhuanlan.zhihu.com/p/45442554\n参考2：https://blog.csdn.net/weixin_38168620/article/details/80663892\n 　数据集通常可能由多个csv文件，并且每个csv文件的key各不相同，其中一个的key是另一个文件的一列。pandas有类似于SQL中连接表的操作，不需要自己手动转换。Pandas.DataFrame操作表连接有三种方式：merge、join、concat。🐷很多需要的操作在这些包里都实现了，建议手动实现前看看文档。\n具体可参考：https://blog.csdn.net/weixin_38168620/article/details/80663892\n5.1 merge 　相当于SQL中的join，两张表有相同内容的列，将两张表合并到一张表；合并得到的结果集行数不会增加，列数为两个元数据的列数减去连接键的数量。\npd.merge(left, right, how=’inner’, on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=(‘_x’, ‘_y’), copy=True, indicator=False, validate=None)\r5.2 join 5.3 concat 　DataFrame.join()：将行索引相同的不同的列连接，\n1 2 3 4  data_train_label = data_train[\u0026#34;label\u0026#34;] data_train = data_train.drop(\u0026#34;label\u0026#34;, axis=1) data_train = data_train.drop(\u0026#34;heartbeat_signals\u0026#34;, axis=1) data_train = data_train.join(train_heartbeat_df)   pandas有类似于SQL中连接表的操作，不要自己转换：https://zhuanlan.zhihu.com/p/45442554\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html\n","description":"用Pandas和Numpy操作数据集","id":4,"section":"posts","tags":["机器学习"],"title":"Python：Pandas和Numpy对数据集的基本操作","uri":"https://yyyIce.github.io/zh/posts/pythonpandas%E5%92%8Cnumpy%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"content":"　前言：本文绝大部分内容摘录自《Docker——从入门到实践》，为加深我的印象整理而成（我的笔记升级了，电子笔记），在最后设置了两个docker demo作为实验结果。定制Docker镜像，一键迁移开发环镜请直接跳转至第4节。实际上我2018年就学习过Docker，看的还是语言表达能力刚及格的老版本《Docker——从入门到实践》，原理是看懂了，但完全没学会怎么使用，看着命令行眼晕，每次都卡在安装Docker，屡装屡败，但这次学起来觉得So easy，看来我的智商和理解力提升了🤓。\n1 Docker简介  来源：https://yeasy.gitbook.io/docker_practice/introduction/what\n 1.1 概述 　Docker是对进程进行封装隔离的操作系统层面的虚拟化技术，由于隔离的进程独立于宿主和其它隔离进程，也被称为容器。Docker使用Go语言开发实现。用户无需关心容器管理，操作Docker容器就像操作快速轻量级的虚拟机一样简单。\n　图1和图2形象地展示了Docker虚拟化技术和传统虚拟化技术的区别。传统虚拟化技术先对物理硬件进行虚拟化，虚拟出硬件之后，在虚拟化的硬件上运行一个完整的操作系统，是在硬件层面实现虚拟化；而容器内的应用进程仍运行在原有的操作系统上，即宿主内核上，容器没有自己的内核，它比传统的虚拟机更为轻便。\n图1 传统虚拟化技术:\r\r 图2 Docker虚拟化技术:\r\r 1.2 为什么要用Docker  参考：https://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html\n 　对于我们这些普通使用人员来说，使用Docker能够节省大量重新配置开发环境的时间，轻松备份（😓虽然但是，会耗费大量时间在配置开发环境的我们可能也并不能迅速掌握Docker，但掌握了就很方便啦）。\n　具体来说，随着各种技术的蓬勃发展，需要的开发越来越复杂，每将程序安装到一台新设备都需要重新配置开发环境。配置开发环境时非常容易出错，模块、库的安装、依赖关系、版本要求、兼容性，一旦出错，就要重来，浪费大量的时间，如果需要程序在其它设备上运行时，能够将程序需要的开发环境一起移到其他设备上，就能够节省大量时间。\n 来源：https://yeasy.gitbook.io/docker_practice/introduction/why\n 　由于Docker直接运行在物理机的操作系统上，节省了许多开销，所以Docker与传统虚拟化方式相比有巨大优势：\n 更高效地利用系统资源：容器不需要进行硬件虚拟和运行完整操作系统等额外开销，所以对物理机系统资源的利用率更高。由于Docker运行在原有操作系统上，其应用执行速度、文件存储速度、内存损耗都要比传统虚拟机技术高效，所以相比虚拟机技术，相同配置的主机，往往可以运行更多数量的应用。 更快速的启动时间：传统虚拟机技术启动服务往往需要数分钟，而Docker容器应用无需启动完整的操作系统，从而作到秒级、甚至毫秒级的启动时间。大大节约了开发、测试、部署的时间。 一致的运行环境：开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。 持续交付和部署：对开发和运维人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。使用 Dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。 更轻松地迁移：由于Docker确保了环境一致，所以可以轻易将一个平台上的应用迁移到另一个平台上。Docker 可以在很多平台上运行，如物理机、虚拟机、公有云、私有云，笔记本，Docker应用程序在这些平台上的运行结果是一致的。 更轻松地维护和扩展：分层存储和镜像技术，让应用重复部分的复用更为容易，也让应用的维护更新更加简单。  　表1将Docker与传统虚拟机进行对比总结。\n表1 Docker VS 虚拟机:\r\r 1.3 基本概念 　Docker有3个重要的基本概念：镜像（Image）、容器（Container）、仓库（Repository）。\n1.3.1 镜像（Image）  来源：https://yeasy.gitbook.io/docker_practice/basic_concept/image\n 　Docker镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。以Linux系统为例，内核启动后，会挂载root文件系统为其提供用户空间支持，而Docker镜像就相当于一个root文件系统，如ubuntu 18.0.4系统镜像，就包含了完整的ubuntu 18.0.4的root文件系统。\n 参考：http://pwn4.fun/2020/07/01/UnionFS%E6%8A%80%E6%9C%AF/\nUnion FS：联合文件系统（Union File System），可以把文件系统上的多个目录联合挂载到同一目录下，而目录的物理位置是分开的。\n可以理解成把不同目录看作在同一目录，并且能够在同一目录对他们进行操作，但实际上他们并没有存储在同一目录。\n 　由于镜像包含操作系统完整的root文件系统，体积庞大，所以Docker运用Union FS技术，设计为分层存储架构；镜像实际上不是像.iso一样的一整个打包文件，而是多层文件系统联合组成。\n　镜像逐层构建，前一层是后一层的基础，每一层构建完就不会再改变，后一层上的任何改变只发生在自己的这一层。比如，删除前一层文件，并不是将前一层的文件删除，而是在当前层标记该文件已删除，在最终运行容器时，虽然不会看到这个文件，但实际上该文件一直存在于镜像里。\n　构建镜像时，需要额外小心，每一层只尽量包含该层需要添加的东西，额外的东西应该在该层构建结束前清理掉。\n　分层存储的特征使镜像的复用、定制更容易，可以使用之前构建好的镜像作为基础层，进一步添加新的层以定制自己所需的内容。Docker提供了非常简单的机制来创建或更新现有镜像，用户可以直接从他人处下载一个已经做好的镜像来使用。\n1.3.2 容器（Container）  来源：https://yeasy.gitbook.io/docker_practice/basic_concept/container\n 容器是从镜像创建的运行实例，容器和镜像的关系就像面向对象程序中类和实例的关系一样。容器可以被创建、启动、停止、删除、暂停等。\n 参考：https://en.wikipedia.org/wiki/Linux_namespaces\n命名空间（Namespaces）是 Linux 内核的一个特性，它对内核资源进行分区，使得一组进程看到一组资源，而另一组进程看到一组不同的资源。该功能的工作原理是为一组资源和进程使用相同的命名空间，但这些命名空间引用不同的资源。资源可能存在于多个空间中。此类资源的示例包括进程 ID、主机名、用户 ID、文件名以及一些与网络访问和进程间通信相关的名称。\n 　容器的实质是进程，但容器进程运行自己的独立的命名空间，即容器进程拥有自己的文件系统、网络配置、进程空间等等。容器进程运行在一个隔离的环境中，使用起来好像在独立于宿主（物理机）的系统下操作一样，这种特性使得容器封装的应用比直接在宿主运行更加安全。\n　容器也采用分层存储，每一个容器运行时，以镜像为基础层，在其上创建一个当前容器的存储层，这个为容器运行时读写而准备的存储层称为容器存储层。\n　容器存储层的生命周期和容器相同，容器消亡时，容器存储层也消失。所以，任何保存在容器存储层的信息都会由于删除容器而消失。\n　容器不应该向存储层写入任何数据，容器存储层要保持无状态化。所有文件写入操作，都应该使用数据卷或者绑定宿主目录，这些位置的读写会跳过容器存储层，直接对宿主发生读写，性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。\n1.3.3 仓库（Repository）  来源：https://yeasy.gitbook.io/docker_practice/basic_concept/repository\n 　仓库是集中存储、分发镜像的场所，Docker Registry是一种仓库服务。一个Docker Registry可以包含多个仓库（Repository），每个仓库可以包含多个标签（Tag），每个标签对应一个镜像。\n　标签常与对应于应用的各个版本，可以通过\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt;的格式来指定具体是这个应用哪个版本的镜像。如果没有给出标签，将以latest作为默认标签。例如仓库名为ubuntu，有16.04和18.04两个版本，可通过ubuntu:16.04来指定16.04版本，如果忽略标签，使用ubuntu，则看作ubuntu:latest。\n　仓库名通常以二级路径形式出现，如jwilder/nginx-proxy，jwilder通常标识仓库服务多用户环境下的用户名，nginx-proxy通常标识应用名；具体含义取决于仓库服务。\n　仓库服务包括公开服务和私有服务。\n 公开服务： 开放给用户使用、允许用户管理镜像的仓库服务，一般允许用户免费上传、下载公开的镜像。最常使用的公开服务是官方的Docker Hub，此外还有 Red Hat 的 Quay.io、Google 的 Googluutse Container Registry（Kubernetes 的镜像使用的就是这个服务）、 GitHub 的 ghcr.io、阿里云镜像服库等。国内一些云服务商提供了对Docker Hub的加速器，如阿里云加速器。 私有服务：用户可以在本地搭建私有Docker Registry，Docker官方提供了Docker Registry镜像，可以直接使用作为私有仓库管理服务，但不包含图形界面、镜像维护、用户管理、访问控制等高级功能。  1.3.4 存储驱动（Storage Driver）  参考：https://zhangchenchen.github.io/2018/03/09/record-for-docker-storage-driver/\n 　容器和镜像的层级关系如图3，容器存储层可以读写，而镜像层是只读的。\n图3 容器和镜像的层级关系:\r\r 　存储驱动是处理Docker各个分层的驱动，负责处理容器的存储。存储驱动具体实现如图4所示的分层读写流程。\n　最底层的基础镜像是ubuntu的系统镜像，再往上是分层的镜像层（比如dockerfile中的软件安装等），这些都是只读的镜像层。最上面才是可以读写的容器存储层，不同的容器有不同的容器存储层，共用相同的镜像层。当某个容器需要写操作时，会先将写的内容从镜像层复制到容器存储层，然后再写入（也就是写时复制），读的时候会从容器层开始，如果命中则读取，没有命中则依次往下层读取。\n图4 容器读写流程:\r\r 　不同的系统支持不同的存储驱动，可以在Docker的配置项中在该系统支持的存储驱动中进行选择。如Ubuntu系统支持overlay2、aufs、btrfs。\n2 安装 　安装镜像的方法参考Docker官方文档：\n Ubuntu安装方法：https://docs.docker.com/engine/install/ubuntu/ CentOS安装方法：https://docs.docker.com/engine/install/centos/  　推荐使用仓库来装Docker。\n　（0）移除旧版本的docker，如果以前没有安装过运行此命令无影响。\n sudo apt-get remove docker docker-engine docker.io containerd runc\r　（1）设置仓库\n　更新apt的包索引，并允许apt用HTTPS使用仓库来安装包。\n1 2 3 4 5 6 7  $ sudo apt-get update $ sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg \\  lsb-release   　添加Docker官方的GPG key。\n1  $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg   　设置稳定的仓库版本。官网的安装速度实在太慢（但是也能成功），可以使用国内镜像安装（我没有验证，我用的官网给的镜像路径）。\n 参考：https://blog.csdn.net/qq_28612967/article/details/105912351\n 1 2 3 4  $ sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34;   　（2）安装Docker引擎\n　更新apt包索引，安装最新版本的Docker引擎和容器，或到下一步安装指定版本（可到官方文档查看，这里不再重复），安装的非常慢。\n1 2  $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io   　（3）运行hello-world镜像，验证Docker引擎是否安装成功。\n1  $ sudo docker run hello-world   　此命令下载测试映像并在容器中运行它。当容器运行时，它打印一条消息并退出。运行结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  Unable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:7d91b69e04a9029b99f3585aaaccae2baa80bcf318f4a5d2165a9898cd2dc0a1 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   　恭喜！安装成功👍！\n3 简单使用 　docker \u0026lt;动作\u0026gt; --help命令可以查看各个选项的含义。\n3.1 基本操作 　（1）拉取镜像：docker pull\n　选项可以通过docker pull --help命令查看。\n1 2 3 4 5 6 7 8 9 10 11 12  # 从指定地址获取镜像，[Docker Registry 地址[:端口号]/]仓库名[:标签]，例baidu.com:8888/nignx:0.21 $ docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] # 从Docker Hub获取镜像 $ docker pull [选项] \u0026lt;仓库名：标签\u0026gt; # 例如 $ sudo docker pull ubuntu:18.04 18.04: Pulling from library/ubuntu e4ca327ec0e7: Pull complete Digest: sha256:9bc830af2bef73276515a29aa896eedfa7bdf4bdbc5c1063b4c457a4bbb8cd79 Status: Downloaded newer image for ubuntu:18.04 docker.io/library/ubuntu:18.04   　拉取镜像时可能出现如下错误，解决方案任选下面之一：\n 在docker命令前加sudo； 将当前用户加入到docker组，在将当前用户切换到docker组。（具体参考：https://docs.docker.com/engine/install/linux-postinstall/）  1 2  # 错误 dial unix /var/run/docker.sock: connect: permission denied    来源：https://yeasy.gitbook.io/docker_practice/image/pull\n 　镜像是由多层存储构成，拉取镜像时也是一层一层下载，上面的例子可能看不出来分层，下面的示例可以看出镜像拉取时分层获取，并会在下载结束后给出镜像完整的sha256摘要，以确保下载的一致性。\n1 2 3 4 5 6 7 8 9  $ docker pull ubuntu:18.04 18.04: Pulling from library/ubuntu 92dc2a97ff99: Pull complete be13a9d27eb8: Pull complete c8299583700a: Pull complete Digest: sha256:4bc3ae6596938cb0d9e5ac51a1152ec9dcac2a1c50829c74abd9c4361e321b26 Status: Downloaded newer image for ubuntu:18.04 docker.io/library/ubuntu:18.04   　（2）运行容器：docker run\n　下面是一个容器运行的简单示例，运行ubuntu:18.04镜像，启动交互式shell，运行两条命令后用exit退出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # -i:交互式操作，-t:终端，--rm:容器退出后删除容器， # ubuntu:18.04:镜像，bash:放在镜像后，是希望在容器中执行的命令。 $ docker run -it --rm ubuntu:18.04 bash # 运行该命令，并在容器中运行几条命令，结果如下 $ sudo docker run -it --rm ubuntu:18.04 bash root@ae6350fe7dda:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@ae6350fe7dda:/# cat /etc/os-release NAME=\u0026#34;Ubuntu\u0026#34; VERSION=\u0026#34;18.04.5 LTS (Bionic Beaver)\u0026#34; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026#34;Ubuntu 18.04.5 LTS\u0026#34; VERSION_ID=\u0026#34;18.04\u0026#34; HOME_URL=\u0026#34;https://www.ubuntu.com/\u0026#34; SUPPORT_URL=\u0026#34;https://help.ubuntu.com/\u0026#34; BUG_REPORT_URL=\u0026#34;https://bugs.launchpad.net/ubuntu/\u0026#34; PRIVACY_POLICY_URL=\u0026#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\u0026#34; VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic root@ae6350fe7dda:/# exit exit ubuntu@VM-0-12-ubuntu:~/docker_test$   　（3）列出已下载的镜像：docker image ls\n　镜像ID是镜像的唯一标识，如果两个镜像的仓库名称不同，但Image id相同，说明它们是同一镜像。这个命令下列出的SIZE总和并不是实际所有镜像实际占用的空间大小，因为Docker使用Union FS，相同的层仅保存一份，实际镜像硬盘占用空间可能比这个列表镜像大小的总和要小得多。\n1 2 3 4 5 6 7 8 9 10 11 12  $ sudo docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.04 54919e10a95d 9 days ago 63.1MB hello-world latest d1165f221234 6 months ago 13.3kB # 查看镜像摘要 $ sudo docker image ls --digests REPOSITORY TAG DIGEST ... ubuntu 18.04 sha256:9bc830af2bef73276515a29aa896eedfa7bdf4bdbc5c1063b4c457a4bbb8cd79 ...   　如果仓库名和标签均为\u0026lt;none\u0026gt;，说明这是一个和新镜像重名的旧镜像，称为虚悬镜像（dangling image）可以随意删除。如果只有标签出现none，说明这是一个i中间层镜像，可以通过docker image ls -a查看。\n　还支持过滤器参数--filter，具体使用说明可参考官方文档或Docker——从入门到实践等。\n　（４）查看镜像、容器、数据卷所占用的空间：docker system df\n1 2 3 4 5 6 7  $ sudo docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 2 1 63.15MB 63.14MB (99%) Containers 1 0 0B 0B Local Volumes 0 0 0B 0B Build Cache 0 0 0B 0B   　（5）删除本地镜像：docker image rm\n　其中\u0026lt;镜像\u0026gt;可以是镜像名（\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt;）、镜像的完整IMAGE ID，镜像ID的前几位（足以和别的镜像区分），镜像摘要。\n1 2 3 4 5 6  $ sudo docker image rm ubuntu:18.04 Untagged: ubuntu:18.04 Untagged: ubuntu@sha256:9bc830af2bef73276515a29aa896eedfa7bdf4bdbc5c1063b4c457a4bbb8cd79 Deleted: sha256:54919e10a95d6f756f7286dc0fa6b3e25b637a7f90e361504b0b14ab6762a547 Deleted: sha256:6babb56be2593d69d561cae7ad16b670a1ee7c0a32b4fa715007fba7febd5ddb   　镜像删除时也是从上层向下层依次删除，如果其它镜像依赖当前要删除的这一层，那么这一层不会被真实删除，只有这一层和所有镜像都没有任何依赖关系时，才会被真实的删除。\n　可以配合docker image ls -q配合使用docker image rm，例如，要删除所有仓库名为redis的镜像：\n1  $ docker image rm $(docker image ls -q redis)   　（6）docker commit命令可以将容器存储层保存为镜像，从而和基础镜像分层组成新的镜像，但一般不使用此命令定制镜像，除了制作镜像的人没有人知道执行过什么命令，怎么生成的镜像，被称为黑箱镜像。该命令用于特殊场合，如入侵后保存现场。\n　（7）重命名镜像\n　我们在下载镜像时，按照镜像地址下载下来的镜像可能是这样，镜像名称非常长，不便于使用\nREPOSITORY TAG IMAGE ...\rregistry.cn-shanghai.aliyuncs.com/tcc-public/tianchi_antispam v1 2b8b72f601da ...\r　这时我们可以重命名镜像：\n1  $ docker tag 2b8b72f601da tianchi_antispam:v1   　此时会出现两个Image ID相同的镜像，我们将原来的名字很长的镜像删除：\n1  $ docker image rm registry.cn-shanghai.aliyuncs.com/tcc-public/tianchi_antispam   3.2 容器操作 　（1）新建一个容器并启动\n1 2 3  $ sudo docker run ubuntu /bin/echo \u0026#39;Hello world\u0026#39; Hello world   　当利用 docker run 来创建容器时，Docker 在后台运行时首先检查本地是否存在指定的镜像，不存在则从Registry下载，然后利用镜像创建并启动一个容器。启动容器时为容器分配一个文件系统，并在只读的镜像层外面挂载容器存储层，为容器从宿主主机配置的网桥接口中桥接一个虚拟接口，并从地址池为容器配置一个IP地址。配置完毕后执行用户指定的应用程序，应用程序执行完毕后容器终止。\n　（2）以守护态（后台）运行容器\n1 2 3 4 5  $ sudo docker run -d \u0026lt;image\u0026gt; \u0026lt;cmd\u0026gt; $ sudo docker run -d ubuntu /bin/sh -c \u0026#34;while true; do echo hello world; sleep 1; done\u0026#34; 88d0d78c3de5115c9ba18cda24110b241246534c0c03a71a2b15d8da65665860   　可以通过docker container logs命令，获取容器的输出信息。\n1 2 3 4 5 6  $ docker container logs [container ID or NAMES] hello world hello world hello world . . .   　（3）查看容器列表\n1 2 3 4 5  # 查看运行中的容器 $ sudo docker container ls $ sudo docker ps # 查看已停止的容器 $ sudo docker container ls -a   　（4）停止运行容器。NAMES是容器启动时设置的--name选项，可在容器查看命令的NAMES一列查看，container可省略。Docker容器中指定的应用终结时，容器也自动停止运行。\n1  $ sudo docker container stop [container ID or NAMES]   　（5）重新启动终止的容器，下面重启Hello world容器。\n1  $ sudo docker container start [container ID or NAMES]   　（6）删除\n　删除处于终止状态的容器。\n1  $ sudo docker container rm [container ID or NAMES]   　清理所有处于终止状态的容器。\n1  $ sudo docker container prune   　（7）导入和导出\n　导出本地某个容器快照到本地文件。\n1  $ sudo docker export [container ID] \u0026gt; [快照名].tar   　docker import从容器快照文件中导入为镜像，和导入镜像存储文件（docker load）的区别在于容器快照文件是“黑盒”，会丢失所有的历史记录和元数据信息，而镜像存储文件保存完整记录。\n1 2 3  $ sudo cat [快照文件] | docker import - \u0026lt;用户名\u0026gt;/\u0026lt;仓库名:标签\u0026gt; # 从URL或目录导入 $ sudo docker import http://example.com/exampleimage.tgz example/imagerepo   　（8）进入容器：docker exec [选项] 容器内执行的命令 [命令参数]\n1 2 3 4  # 开启一个bash对话 $ sudo docker exec -it \u0026lt;IMAGE_ID\u0026gt; bash # 例 $ docker exec -it 0d588c4d3cb7 bash   （9）文件拷贝\n从容器拷贝文件到宿主机\ndocker cp tianchi_executor.py 0d588c4d3cb7:/root/tianchi_aiflow/workflows/tianchi_main/\ndocker cp mycontainer:/opt/testnew/file.txt /opt/test/\rdocker cp 0d588c4d3cb7:/root/tianchi_entry/result.csv .\n从宿主机拷贝文件到容器\ndocker cp /opt/test/file.txt mycontainer:/opt/testnew/\rdocker cp /root/aaigcup/tianchi_executor.py 0d588c4d3cb7:/root/tianchi_aiflow/workflows/tianchi_main/\ndocker cp /root/aaigcup/tf_main.py 0d588c4d3cb7:/root/tianchi_aiflow/workflows/tianchi_main/\n3.3 Dockerfile定制镜像  来源：https://yeasy.gitbook.io/docker_practice/image/build\n 　Dockerfile就是记录镜像每层修改、安装、构建、操作命令的脚本，与docker commit构建的黑盒镜像不同，它能构建体积适当、操作记录清晰的镜像。\n　Dockerfile是一个文本文件，它由许多指令组成，每条指令构建镜像的一层，描述该层应该怎样构建。\n　定制镜像时先用指令编写Dockerfile文件，然后使用docker build命令构建镜像。\n3.3.1 定制镜像简单示例 　下面是用Dockerfile构建一个简单的定制nginx镜像：\n　（1）在空白目录中，建立一个文本文件，命名为Dockerfile，其中touch命令是快速创建一个空文件。\n1 2 3 4  $ mkdir mynginx $ cd mynginx $ touch Dockerfile $ vim Dockerfile   　Dockerfile的内容为：\n1 2  FROMnginxRUN echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html  　（2）构建镜像，在Dockerfile文件所在目录，本例中为mynginx。注意nginx:v2后的.不要漏掉，它表示当前目录。\n1 2 3 4 5 6 7 8 9 10 11  ~/mynginx$ sudo docker build -t nginx:v2 . Sending build context to Docker daemon 2.048kB Step 1/2 : FROM nginx ---\u0026gt; 822b7ec2aaf2 Step 2/2 : RUN echo \u0026#39;\u0026lt;h1\u0026gt;Hello, Docker!\u0026lt;/h1\u0026gt;\u0026#39; \u0026gt; /usr/share/nginx/html/index.html ---\u0026gt; Running in c07f4978691f Removing intermediate container c07f4978691f ---\u0026gt; 0abd2a2a7c16 Successfully built 0abd2a2a7c16 Successfully tagged nginx:v2   　可以看到Run指令先启动了一个容器c07f4978691f用于执行命令，然后提交这一层0abd2a2a7c16，随后删除了容器c07f4978691f。\n　（3）查看构建的nginx:v2镜像。\n1 2 3 4 5  $ sudo docker image ls nginx REPOSITORY TAG IMAGE ID CREATED SIZE nginx v2 0abd2a2a7c16 7 minutes ago 133MB nginx latest 822b7ec2aaf2 6 days ago 133MB   3.3.2 常用指令 　Dockerfile中每一个指令都会建立一层，层数过多会增加构建部署时间，镜像臃肿混乱，并且Union FS有最大层数限制（如AUFS不能超过127层），应当合理组织指令，构建简洁干净的镜像。有的指令具有两种格式，shell和exec格式，注意在用exec格式时一定要使用双引号\u0026quot;\u0026quot;，不要使用单引号''。\n（1）FROM\n　定制镜像一般以一个镜像为基础，在上面进行操作以构建新的镜像。FROM命令用来指定一个基础镜像，比如3.3.1示例中的nginx。\n　在Dockerfile中，必须指定基础镜像。Docker Hub提供了非常多的官方镜像，比如操作系统镜像（ubuntu、centos）、服务类镜像（nginx、redis、httpd、mongo）、语言环境镜像（node、python、golang）等。如果不想选择现有的基础镜像，可以设置基础镜像为scratch，表示镜像不以任何镜像为基础，之后的指令会作为镜像的第一层，命令如下：\n1  $ FROM scratch   　对于静态编译的程序来说，其依赖库已经包含在可执行文件中，直接将可执行文件复制进镜像会让镜像体积更加小巧，使用Go语言开发的应用很多会使用这种方式来制作镜像。\n（2）RUN\n　RUN用来执行终端命令，有shell和exec两种。\n shell格式：就像直接在终端输入命令一样，比如在3.3.1示例中的RUN指令。 exec格式：更像函数调用中的格式，RUN [\u0026quot;可执行文件\u0026quot;,\u0026quot;参数1\u0026quot;,\u0026quot;参数2\u0026quot;]。  　需要注意的是，一条RUN指令就会建立一层镜像，以下面编译、安装redis可执行文件的错误示例为例，下面的Dockerfile会导致运行时不需要的内容也被装进镜像，如编译环境、更新的软件包，导致镜像臃肿：\n1 2 3 4 5 6 7 8 9  FROMdebian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install  　上面的错误示例在基础镜像之上创建了7层镜像，非常臃肿，而代码的目标只是编译、安装redis可执行文件，只需要一层就可以了。所以应该只使用一次RUN指令，将所有命令用\u0026amp;\u0026amp;串联起来。\n　为了方便文件阅读，Dockerfile支持\\的行尾命令换行和行首#注释的格式。\n　构建每一层时还需要注意，与编写shell脚本不同的是，构建镜像时，要清理掉无关内容，比如编译构建所需要的软件、下载的文件、apt缓存文件等。我们最后编写的Dockerfile如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  FROMdebian:stretchRUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\  \u0026amp;\u0026amp; apt-get update \\  \u0026amp;\u0026amp; apt-get install -y $buildDeps \\  \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\  \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\  \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\  \u0026amp;\u0026amp; make -C /usr/src/redis \\  \u0026amp;\u0026amp; make -C /usr/src/redis install \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\  \u0026amp;\u0026amp; rm redis.tar.gz \\  \u0026amp;\u0026amp; rm -r /usr/src/redis \\  \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps  【buildDeps】是什么？build-dep参数是指建立某个要编译软件的环境，比如我们现在要手工编译apache，那如果想让编译正常通过，你可以事先把编译过程中需要用到的软件包先配置好。\n（3）COPY和ADD\n　COPY指令从上下文目录中，将\u0026lt;源路径\u0026gt;的文件复制到新一层的镜像内的\u0026lt;目标路径\u0026gt;位置。COPY也有两种格式：\n COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;源路径\u0026gt;... \u0026lt;目标路径\u0026gt; COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026quot;\u0026lt;源路径1\u0026gt;\u0026quot;,... \u0026quot;\u0026lt;目标路径\u0026gt;\u0026quot;]  　其中\u0026lt;源路径\u0026gt;可以是多个，通配符需要满足Go的filepath.Match规则；\u0026lt;目标路径\u0026gt;可以是容器内的绝对路径，也可以是相对于工作目录的相对路径，如果目标路径不存在，会在复制文件前创建确实目录。比如：\n1 2 3  COPY package.json /usr/src/app/COPY hom* /mydir/COPY hom?.txt /mydir/  　COPY指令会保留源文件的各种元数据，比如读、写、执行权限等。\n　ADD和COPY功能相似，但应尽可能地使用COPY，这里不对ADD做详细介绍。但ADD具有自动解压缩的功能，适合用于将一个压缩文件自动解压到\u0026lt;目标路径\u0026gt;中。\n（4）CMD\n　Docker容器的本质是进程，在启动容器时，需要指定所运行的程序及参数。CMD用来指定默认的容器主进程的启动命令，如/bin/bash，ubuntu镜像的CMD是/bin/bash，当我们直接运行docker run -it ubuntu时，就会直接进入bash，想要在运行时运行别的命令，需要在运行命令中指出，如docker run -it ubuntu cat /etc/os-release，此时就不会进入bash，而是输出系统版本信息。\n　CMD命令也有两种格式，\n shell 格式：CMD \u0026lt;命令\u0026gt; exec 格式：CMD [\u0026quot;可执行文件\u0026quot;, \u0026quot;参数1\u0026quot;, \u0026quot;参数2\u0026quot;...]  　一般推荐使用exec格式，使用shell形式在实际执行中也会变成sh -c的参数形式进行执行。\n1 2 3 4  # 使用shell形式CMD echo $HOME# 实际执行CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $HOME\u0026#34;]  　注意容器是进程，不是操作系统，所以它没有后台服务，其主进程运行的命令结束后容器就会退出，如果我们将启动nginx服务的命令写成下面这样，会发现容器执行完毕后立刻退出了。\n1  CMD service nginx start  　上面的命令实际执行是实际是下面这样，主进程不是service而是sh，而sh执行完service nginx start之后就结束退出了，容器也就退出了。\n1  CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;service nginx start\u0026#34;]  　所以能够达到目的的做法是执行nginx可执行文件，并且要以前台形式运行，否则该命令执行完毕容器又退出了。\n1  CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;]  （5）WORKDIR\n　WORKDIR用来指定工作目录（当前目录），之后各层的当前目录就修改为指定的工作目录，如果目录不存在，会建立该目录，格式为：WORKDIR \u0026lt;工作目录路径\u0026gt;。\n　注意和RUN cd命令的区别：RUN指令执行一次，创建一层新的镜像，所以RUN cd只能改变当前层的执行环境，而WORKDIR可以改变之后所有层的工作目录。shell中cd可以改变后续的执行环境是因为shell是一个进程，其他命令都是shell的子进程，cd 改变的是shell进程的运行环境，所以在shell中执行cd命令，会直接影响到后续命令。\n如果想要改变之后各层的工作目录的位置，应该使用WORKDIR指令。如果WORKDIR指令使用相对路径，所切换的路径与之前的WORKDIR有关。\n1 2 3 4 5  WORKDIR/aWORKDIRbWORKDIRcRUN pwd  　RUN pwd的工作目录为/a/b/c。\n（6）EXPOSE\n　EXPOSE用于声明容器运行时提供服务的端口，但它只是一个声明，在容器运行时打开这个端口的服务需要用-P命令启用映射。这个声明可以帮助镜像使用者理解镜像服务的守护端口，并在docker run -P时，会自动随机映射EXPOSE的端口。格式如下：\n1  EXPOSE\u0026lt;端口1\u0026gt; [\u0026lt;端口2\u0026gt; ...]  　注意EXPOSE和运行时使用的-p \u0026lt;宿主端口\u0026gt;:\u0026lt;容器端口\u0026gt;区分，-p是映射宿主端口和容器端口，是将容器的对应端口映射给主机，外界可以访问宿主端口从而访问到容器上的服务，而EXPOSE仅仅声明容器计划使用什么端口，并不会自动在宿主进行端口映射。\n（7）ENV\n　ENV用来设置环境变量，让这个指令后面的其他指令或运行时的应用，都可以直接使用ENV定义的环境变量。格式如下：\n1 2  ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt;ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt; ...  　定义环境变量后，用$\u0026lt;EVN_NAME\u0026gt;进行使用即可。例如官方node镜像的Dockerfile：\n1 2 3 4 5 6 7 8 9  ENV NODE_VERSION 7.2.0RUN curl -SLO \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; \\  \u0026amp;\u0026amp; curl -SLO \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\u0026#34; \\  \u0026amp;\u0026amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\  \u0026amp;\u0026amp; grep \u0026#34; node-v$NODE_VERSION-linux-x64.tar.xz\\$\u0026#34; SHASUMS256.txt | sha256sum -c - \\  \u0026amp;\u0026amp; tar -xJf \u0026#34;node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; -C /usr/local --strip-components=1 \\  \u0026amp;\u0026amp; rm \u0026#34;node-v$NODE_VERSION-linux-x64.tar.xz\u0026#34; SHASUMS256.txt.asc SHASUMS256.txt \\  \u0026amp;\u0026amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs  　官方node镜像在Dockerfile中定义了环境变量NODE_VERSION，使用$NODE_VERSION进行操作，这样在升级镜像构建版本时，只需更新ENV指令中的7.2.0即可，使Dockerfile的构建维护变得轻松了。\n　支持环境变量的指令有ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN。\n　有了环境变量指令，可以使用一份Dockerfile制作环境变量不同的镜像。\n（8）ENTRYPOINT\n　ENTRYPOINT指定容器启动程序和参数，和CMD指令相同，也可以在运行时指定容器启动程序和参数，但需要通过docker run的--entrypoint参数指定。格式如下：\n shell格式：就像直接在终端输入命令一样，比如在3.3.1示例中的RUN指令。 exec格式：更像函数调用中的格式，ENTRYPOINT [\u0026quot;可执行文件\u0026quot;,\u0026quot;参数1\u0026quot;,\u0026quot;参数2\u0026quot;]。  　当指定ENTRYPOINT后，CMD指令不运行它的命令，而是将它的命令内容作为参数传递给ENTRYPOINT指令。即实际执行时变为：\n1  ENTRYPOINT \u0026#34;CMD\u0026#34;  　ENTRYPOINT指令可以在镜像构建时补充命令参数，还可以完成应用运行前的准备工作。\n　CMD指令是执行它后面的命令，而ENTRYPOINT指令是接收CMD的命令为参数，它可以在Dockerfile中定义了命令后接收docker run时增加的CMD指令。\n　补充输入命令参数。例如下面的例子，该示例获取当前的公网IP：\n　用CMD命令的结果如下：\n1 2 3 4 5  FROMubuntuRUN apt-get update \\  \u0026amp;\u0026amp; apt-get install -y curl \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*CMD [ \u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://myip.ipip.net\u0026#34; ]  　构建镜像，启动容器：\n1 2 3 4  $ sudo docker build -t myip . $ sudo docker run myip 当前 IP：49.232.101.152 来自于：中国 北京 北京 电信/联通/移动   　如果在镜像构建结束后希望显示HTTP头部信息，需要增加-i参数，但此时如果直接添加-i参数，会报错。这是因为CMD命令指定的是默认容器启动程序和参数，在镜像名称myip后面就是CMD命令，此时CMD命令是-i，显然会报错。如果想要加入-i参数，就必须重新输入命令，这不是一个很好的解决方案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # 直接添加-i报错 $ sudo docker run myip -i docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \u0026#34;-i\u0026#34;: executable file not found in $PATH: unknown. ERRO[0000] error waiting for container: context canceled # 重新输入命令 $ sudo docker run myip curl -s http://myip.ipip.net -i HTTP/1.1 200 OK Date: Mon, 13 Sep 2021 06:39:57 GMT Content-Type: text/plain; charset=utf-8 Content-Length: 83 Connection: keep-alive X-Via-JSL: 1d9bd9a,- Set-Cookie: __jsluid_h=02f5c112cbd961d203f35fa0189db8b1; max-age=31536000; path=/; HttpOnly X-Cache: bypass 当前 IP：49.232.101.152 来自于：中国 北京 北京 电信/联通/移动   　如果使用ENTRYPOINT，就不必重新输入命令，使用ENTRYPOINT的Dockerfile如下：\n1 2 3 4 5  FROMubuntuRUN apt-get update \\  \u0026amp;\u0026amp; apt-get install -y curl \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*ENTRYPOINT [ \u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://myip.ipip.net\u0026#34; ]  　此时直接在启动容器时添加-i：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ sudo docker build -t myip . $ sudo docker run myip 当前 IP：49.232.101.152 来自于：中国 北京 北京 电信/联通/移动 $ sudo docker run myip -i HTTP/1.1 200 OK Date: Mon, 13 Sep 2021 06:58:00 GMT Content-Type: text/plain; charset=utf-8 Content-Length: 83 Connection: keep-alive X-Shadow-Status: 200 X-Via-JSL: 1d9bd9a,c06d31e,- Set-Cookie: __jsluid_h=f7e3f4eeb9b96aa3d5d0e903d3fa5dbe; max-age=31536000; path=/; HttpOnly X-Cache: bypass 当前 IP：49.232.101.152 来自于：中国 北京 北京 电信/联通/移动   　添加ENTRYPOINT后，-i会作为参数传递给ENTRYPOINT，成为curl命令的参数。\n　**应用运行前的准备工作。**启动容器就是启动主进程，但有时启动主进程前需要一些准备工作。比如数据库服务在启动服务器之前进行配置、初始化；可能在启动服务前以root身份执行一些必要的准备工作，然后切换到服务用户身份启动服务。这些准备工作与CMD指令无关，CMD指令是容器启动后在容器中执行的，而准备工作需要在启动容器前完成。\n　为了完成这种工作，可以编写一个脚本，放到ENTRYPOINT中执行，这样做会先将脚本执行完毕，再执行传入的CMD指令参数。如官方镜像redis就是这么做的。\n1 2 3 4 5 6 7 8  FROMalpine:3.4...RUN addgroup -S redis \u0026amp;\u0026amp; adduser -S -G redis redis...ENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;]EXPOSE6379CMD [ \u0026#34;redis-server\u0026#34; ]  　上面的Dockerfile对redis服务创建了redis用户，并在最后制定了ENTRYPOINT为docker-entrypoint.sh脚本：\n1 2 3 4 5 6 7 8 9  #!/bin/sh ... # allow the container to be started with `--user` if [ \u0026#34;$1\u0026#34; = \u0026#39;redis-server\u0026#39; -a \u0026#34;$(id -u)\u0026#34; = \u0026#39;0\u0026#39; ]; then find . \\! -user redis -exec chown redis \u0026#39;{}\u0026#39; + exec gosu redis \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; fi exec \u0026#34;$@\u0026#34;   　脚本的内容是根据启动容器时输入的CMD命令来判断，如果是redis-server，则切换到redis用户身份启动服务器，否则仍然使用root身份。\n1 2 3  $ sudo docker run -it redis id uid=0(root) gid=0(root) groups=0(root)   （9）HEALTHCHECK\n　HEALTHCHECK指令检查Docker容器的状态是否正常，这是Docker 1.12引入的新指令，之前的版本只能通过容器内主进程是否退出来判断容器是否状态异常，无法应对应进程未退出，但已经无法正常提供服务的状态，如死锁、死循环。它的格式如下：\n HEALTHCHECK [选项] CMD \u0026lt;命令\u0026gt;：设置检查容器健康状况的命令。 HEALTHCHECK NONE：可以屏蔽基础镜像的健康检查指令。  　CMD后面的\u0026lt;命令\u0026gt;格式和ENTRYPOINT相同，命令的返回值表示此次健康检查的结果：\n 0：成功； 1：失败； 2：保留，不要使用这个值。  　当一个镜像指定了HEALTHCHECK指令后，用其启用容器，初始状态为starting，在HEALTHCHECK指令检查成功后变为healthy，如果连续一定次数（默认为3次）失败，则状态变更为unhealthy。\n　HEALTHCHECK支持以下选项：\n --interval=\u0026lt;间隔\u0026gt;：两次健康检查的时间间隔，默认为30秒。 --timeout=\u0026lt;时长\u0026gt;：健康检查命令运行超时时间间隔，超过这个间隔，本次健康检查就视为失败，默认为30秒。 --retries=\u0026lt;次数\u0026gt;：当连续失败一定次数时，容器状态变更为unhealthy，默认3次。  　HEALTHCHECK在Dockerfile中只可以出现一次，如果出现多次，只有最后一个生效。\n　我们编写一个最简单的Web服务，并为它添加健康检查来判断它是否正常工作，可以用curl命令判断。curl命令是一个利用URL规则在命令行下工作的文件传输工具。编写的Dockerfile如下：\n1 2 3 4  FROMnginxRUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost/ || exit 1  　这个Dockerfile以niginx为基础镜像，下载并安装curl，删除缓存文件并使用curl -fs http://localhost/ || exit 1命令进行检查，每隔5秒检查一次，超过3秒没有响应就失败。-f为连接失败时不显示http错误，-s是静默模式，不输出任何内容。\n构建镜像：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  # -t表示设置名称标签为myweb:v1 $ sudo docker build -t myweb:v1 Sending build context to Docker daemon 2.048kB Step 1/3 : FROM nginx ---\u0026gt; 822b7ec2aaf2 Step 2/3 : RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* ---\u0026gt; Running in 62e9e7facf8b Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB] Get:2 http://deb.debian.org/debian buster InRelease [122 kB] Get:3 http://security.debian.org/debian-security buster/updates/main amd64 Packages [303 kB] Get:4 http://deb.debian.org/debian buster-updates InRelease [51.9 kB] Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7907 kB] Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [15.2 kB] Fetched 8465 kB in 6min 43s (21.0 kB/s) Reading package lists... Reading package lists... Building dependency tree... Reading state information... curl is already the newest version (7.64.0-4+deb10u2). 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. Removing intermediate container 62e9e7facf8b ---\u0026gt; c77f3d0c0cac Step 3/3 : HEALTHCHECK --interval=5s --timeout=3s CMD curl -fs http://localhost/ || exit 1 ---\u0026gt; Running in 7c5f96d17bc1 Removing intermediate container 7c5f96d17bc1 ---\u0026gt; 45b487bf77da Successfully built 45b487bf77da Successfully tagged myweb:v1   　启动容器：\n1 2  $ sudo docker run -d --name mywebservice -p 8080:80 myweb:v1 ebe33796114e22e0b9f97d6afd682d87c7371e10910b6a3f343284a4b9f4ef8c   　运行镜像，立刻查看容器状态：\n1 2 3 4  $ sudo docker container ls CONTAINER ID ... STATUS ebe33796114e ... Up 2 seconds (health: starting)   　过几秒再次查看发容器状态：\n1 2 3 4  $ sudo docker container ls CONTAINER ID ... STATUS ebe33796114e ... About a minute (healthy)   　查看健康检查命令的输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  $ docker inspect --format \u0026#39;{{json .State.Health}}\u0026#39; web | python -m json.tool { \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ { \u0026#34;End\u0026#34;: \u0026#34;2021-09-13T11:00:59.49479706+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\\n\u0026lt;html\u0026gt;\\n\u0026lt;head\u0026gt;\\n\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;\\n\u0026lt;style\u0026gt;\\n body {\\n width: 35em;\\n margin: 0 auto;\\n font-family: Tahoma, Verdana, Arial, sans-serif;\\n }\\n\u0026lt;/style\u0026gt;\\n\u0026lt;/head\u0026gt;\\n\u0026lt;body\u0026gt;\\n\u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt;\\n\u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and\\nworking. Further configuration is required.\u0026lt;/p\u0026gt;\\n\\n\u0026lt;p\u0026gt;For online documentation and support please refer to\\n\u0026lt;a href=\\\u0026#34;http://nginx.org/\\\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt;\\nCommercial support is available at\\n\u0026lt;a href=\\\u0026#34;http://nginx.com/\\\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\\n\\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\\n\u0026lt;/body\u0026gt;\\n\u0026lt;/html\u0026gt;\\n\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2021-09-13T11:00:59.412497775+08:00\u0026#34; }, ...... { \u0026#34;End\u0026#34;: \u0026#34;2021-09-13T11:01:19.842117465+08:00\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\\n\u0026lt;html\u0026gt;\\n\u0026lt;head\u0026gt;\\n\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;\\n\u0026lt;style\u0026gt;\\n body {\\n width: 35em;\\n margin: 0 auto;\\n font-family: Tahoma, Verdana, Arial, sans-serif;\\n }\\n\u0026lt;/style\u0026gt;\\n\u0026lt;/head\u0026gt;\\n\u0026lt;body\u0026gt;\\n\u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt;\\n\u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and\\nworking. Further configuration is required.\u0026lt;/p\u0026gt;\\n\\n\u0026lt;p\u0026gt;For online documentation and support please refer to\\n\u0026lt;a href=\\\u0026#34;http://nginx.org/\\\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt;\\nCommercial support is available at\\n\u0026lt;a href=\\\u0026#34;http://nginx.com/\\\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\\n\\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\\n\u0026lt;/body\u0026gt;\\n\u0026lt;/html\u0026gt;\\n\u0026#34;, \u0026#34;Start\u0026#34;: \u0026#34;2021-09-13T11:01:19.765572173+08:00\u0026#34; } ], \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34; }   3.3.3 镜像构建 　使用docker build命令构建镜像。\n1  $ sudo docker build [选项] \u0026lt;上下文路径/URL/-\u0026gt;   上下文路径\n　Docker在运行时分为服务端（Docker引擎）和客户端（docker指令）。客户端通过服务端提供的远程调用接口（REST API）和服务端交互，完成各种功能。虽然表面上好像是在本机执行docker指令，但实际上是以远程调用的形式在服务端完成。\n　构建镜像时，许多指令涉及到本地文件，如COPY、ADD指令，但docker build命令并非在本地构建镜像，而是在服务端构建镜像，服务端并不存在客户端的本地文件，这就需要用户在该指令处指定构建镜像的上下文路径。docker build在解析到该指令的上下文路径后，会将该路径下的所有内容打包，上传给服务端，这样服务端收到上下文包后，就能获得构建镜像所需的文件。\n　上下文路径的源文件路径都是相对路径，即./xxx/xxxx/xx，而不能是绝对路径/var/xx/xx/和../xx/xx这样的路径，它们都超出了上下文的范围，服务端不能获得这些位置的文件，如果实在需要这些文件，需要将这些文件复制到上下文目录中。\n　在3.3.1的实例中的.就是上下文目录，docker build 命令会将该目录下的内容打包发给Docker引擎（服务端），这个发送过程也输出在了屏幕上：\n1  Sending build context to Docker daemon 2.048kB   　一般会将Dockerfile置于一个空目录下或者项目根目录文件下，如果目录下有些内容不希望构建时传给Docker引擎，可以用.gitignore的语法编写一个.dockerignore，去掉不需要作为上下文传递给Docker引擎的文件。\n　还支持其它多种构建方式：\n1 2 3 4 5 6 7 8 9  # 从url构建 $ docker build -t hello-world https://github.com/docker-library/hello-world.git#master:amd64/hello-world # 从tar压缩包构建 $ docker build http://server/context.tar.gz # 从标准输入中读取Dockerfile进行构建 $ docker build - \u0026lt; Dockerfile $ cat Dockerfile | docker build - # 从标准输入中读取上下文压缩包进行构建 $ docker build - \u0026lt; context.tar.gz   多阶段构建\n　Docker 17.05开始支持Dockerfile多阶段构建镜像，可以使用一个Dockerfile解决镜像层次多、体积大、部署时间长的问题，并且无需多个Dockerfile文件。下面的示例先将app.go文件的依赖库编译测试打包后，将它拷贝到运行环境中运行：\n　app.go文件，程序输出Hello World!：\n1 2 3 4 5 6 7  package main import \u0026#34;fmt\u0026#34; func main(){ fmt.Printf(\u0026#34;Hello World!\u0026#34;); }   　Dockerfile文件，as后面是这一阶段的名称。\n1 2 3 4 5 6 7 8 9 10 11 12  FROMgolang:alpine as builderRUN apk --no-cache add gitWORKDIR/go/src/github.com/go/helloworld/RUN go get -d -v github.com/go-sql-driver/mysqlCOPY app.go .RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROMalpine:latest as prodRUN apk --no-cache add ca-certificatesWORKDIR/root/COPY --from=0 /go/src/github.com/go/helloworld/app .CMD [\u0026#34;./app\u0026#34;]  　构建镜像：\n1  $ sudo docker build -t go/helloworld .   　如果只想构建多阶段的Dockerfile中的一个阶段，可以在docker build命令后增加--target \u0026lt;阶段名\u0026gt;参数，例如：\n1  $ sudo docker build --target builder -t username/imagename:tag .   4 练习 4.1 使用容器 　（1）拉取2种ngnix镜像，可以看到分层拉取。\n1 2 3 4 5 6 7 8 9 10 11  $ sudo docker pull nginx:1.21.1 1.21.1: Pulling from library/nginx a330b6cecb98: Pull complete 5ef80e6f29b5: Pull complete f699b0db74e3: Pull complete 0f701a34c55e: Pull complete 3229dce7b89c: Pull complete ddb78cb2d047: Pull complete Digest: sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e Status: Downloaded newer image for nginx:1.21.1 docker.io/library/nginx:1.21.1   1 2 3 4 5 6  $ sudo docker pull nginx Using default tag: latest latest: Pulling from library/nginx Digest: sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e Status: Image is up to date for nginx:latest docker.io/library/nginx:latest   　查看镜像列表，发现TAG为latest和1.21.1的nginx的IMAGE ID完全相同，说明它们是同一个镜像。\n1 2 3 4 5  $ sudo docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE nginx 1.21.1 822b7ec2aaf2 5 days ago 133MB nginx latest 822b7ec2aaf2 5 days ago 133MB hello-world latest d1165f221234 6 months ago 13.3kB   　删除掉TAG为1.21.1的重复镜像。\n1  $ sudo docker image rm nginx:1.21.1   　（2）启动一个ngnix容器，命名为webserver，-p：设置端口映射，将本地的8080端口映射到容器内部的80端口。-d：设置容器在后台运行。--name设置容器名称为webserver。\n1 2  $ sudo docker run --name webserver -d -p 8080:80 nginx cd9e3dd3e7bde80c21a0bcfd766c405a1cd34143b51c8f326ba2d8047ffde8fe   　访问http://域名:8080可以看到图X所示的页面。\n图X 容器读写流程:\r\r 　（3）查看运行中的容器。\n1 2 3  $ sudo docker ps CONTAINER ID IMAGE COMMAND ... cd9e3dd3e7bd nginx \u0026#34;/docker-entrypoint.…\u0026#34; ...   　（4）停止容器运行。\n1 2  $ sudo docker container stop webserver webserver   注：docker ps和docker container ls不能查看到停止运行的容器。\n　（5）查看停止运行的容器，这个container不能省略。\n1 2 3  $ sudo docker container ls -a CONTAINER ID IMAGE COMMAND ... cd9e3dd3e7bd nginx \u0026#34;/docker-entrypoint.…\u0026#34; ...   　（6）删除容器。\n1  $ sudo docker container rm webservers   4.2 构建镜像 本练习编写一个支持L机器学习项目的镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # Use an official PyTorch runtime as a parent imageFROMpytorch/pytorch# Set the working directoryWORKDIR/docker_sample# Copy the current directory contents into the containerCOPY . /docker_sample# Install any needed packages specified in requirements.txtRUN pip install -r requirements.txt# Run when the container launchesCMD [\u0026#34;python\u0026#34;, \u0026#34;deploy_models.py\u0026#34;]  5 可能出现的错误及解决办法 　（1）Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n 解决：\nhttps://stackoverflow.com/questions/44678725/cannot-connect-to-the-docker-daemon-at-unix-var-run-docker-sock-is-the-docker\n 1  systemctl start docker   1 2 3  gpasswd -a $USER docker # 或者在命令前加sudo sudo   ","description":"Docker的学习与使用，将配置好的环境打包成Docker。","id":5,"section":"posts","tags":["虚拟化"],"title":"工具使用：Docker","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8docker/"},{"content":"Scapy的基本使用方法 1 概述  文档：https://scapy.readthedocs.io/en/latest/introduction.html\n 　Scapy是能让用户发送、监听、分析、构造网络数据包的Python程序。Scapy是交互式数据包操纵程序，能够轻松处理多种典型任务，如扫描、跟踪路由、探测、单元测试、攻击或网络发现，能够代替hping、arpspoof、arpsk、arping、p0f以及Nmap、tcpdump、tshark的部分功能。\n2 安装 （1）普通安装\n1  pip install scapy   （2）最新版本\n　通常使用普通安装的功能已经足够使用，最新版本具体安装方法可参考：https://scapy.readthedocs.io/en/latest/installation.html。\n（3）基本使用\n　Scapy交互式程序就和打开Python命令行导入scapy模块的操作是一样的。\n　打开交互式Scapy程序，创建一个包a，查看字段并发送它的操作如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  $ sudo scapy Welcome to Scapy (2.2.0-dev) \u0026gt;\u0026gt;\u0026gt; a = Ether()/IP()/UDP() \u0026gt;\u0026gt;\u0026gt; a.show() ###[ Ethernet ]### dst= ff:ff:ff:ff:ff:ff src= 00:00:00:00:00:00 type= 0x800 ###[ IP ]### version= 4 ihl= None tos= 0x0 len= None id= 1 flags= frag= 0 ttl= 64 proto= udp chksum= None src= 127.0.0.1 dst= 127.0.0.1 \\options\\ ###[ UDP ]### sport= domain dport= domain len= None chksum= None \u0026gt;\u0026gt;\u0026gt; sendp(a) . Sent 1 packets.   　用Python文件中调用Scapy模块进行同样的操作：\n1 2 3 4  # test.py from scapy.all import * a = Ether()/IP()/UDP() sendp(a)   　输出如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  $ sudo python test.py ###[ Ethernet ]### dst = ff:ff:ff:ff:ff:ff src = 00:00:00:00:00:00 type = 0x800 ###[ IP ]### version = 4 ihl = None tos = 0x0 len = None id = 1 flags = frag = 0 ttl = 64 proto = udp chksum = None src = 127.0.0.1 dst = 127.0.0.1 \\options \\ ###[ UDP ]### sport = domain dport = domain len = None chksum = None . Sent 1 packets.   （4）常用命令可以看官方文档Usage页面，API查询参考Scapy API reference页面。\n3 基本使用 3.1 组装一个数据包 　组装合法数据包需要按照据包的层级由低层到高层组装，例如UDP数据包，则需要以太帧-IP头-UDP头-数据的格式组装，如果只设置一个UDP数据包头部，则会由于没有IP头部导致UDP头部没有校验和。\n1 2  # 不同层用\u0026#34;/\u0026#34;相连接 \u0026gt;\u0026gt;\u0026gt; a = Ether()/IP()/UDP()   　各层分离地查看数据包a的各个字段，即每个层的头部相互之间没有关联。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026gt;\u0026gt;\u0026gt; a.show() ###[ Ethernet ]### dst= ff:ff:ff:ff:ff:ff src= 00:00:00:00:00:00 type= 0x800 ###[ IP ]### version= 4 ihl= None tos= 0x0 len= None id= 1 flags= frag= 0 ttl= 64 proto= udp chksum= None src= 127.0.0.1 dst= 127.0.0.1 \\options\\ ###[ UDP ]### sport= domain dport= domain len= None chksum= None   　如果想要修改字段，可以使用\u0026lt;数据包实例\u0026gt;[数据包头部].\u0026lt;属性名\u0026gt;来修改各个字段，在各属性不重名时可以省略数据包层级，例\n1 2 3  \u0026gt;\u0026gt;\u0026gt; a[Ether].dst = \u0026#34;11:22:33:44:55:66\u0026#34; \u0026gt;\u0026gt;\u0026gt; a[UDP].sport = 1222 \u0026gt;\u0026gt;\u0026gt; a[UDP].dport = 222   　如果使用a.dst，只能修改Ether()层，如果修改a.dst = '192.168.1.8'，则会将Ether()中的dst修改，从而导致错误。\n　观察用show()方法得到的结果，可以发现chksum是0，这是因为show()方法是各层独立地查看字段值。我们可以使用show2()方法查看校验和等将各层联系起来的值，它以一个整体的形式看待组装的数据包，所以能够看到计算的校验和，例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026gt;\u0026gt;\u0026gt; a.show2() ###[ Ethernet ]### dst= 11:22:33:44:55:66 src= 00:00:00:00:00:00 type= 0x800 ###[ IP ]### version= 4L ihl= 5L tos= 0x0 len= 28 id= 1 flags= frag= 0L ttl= 64 proto= udp chksum= 0x7cce src= 127.0.0.1 dst= 127.0.0.1 \\options\\ ###[ UDP ]### sport= 1222 dport= 222 len= 8 chksum= 0xfc37   　可以看到IP头部的chksum和UDP头部的chksum都有值。这个UDP数据包没有载荷，我们为它修改源IP和目的IP，再加上数据作为载荷，可以通过输入a，来查看数据包a的组成：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  \u0026gt;\u0026gt;\u0026gt; a[Ether].src=\u0026#34;11:11:11:00:55:11\u0026#34; # 等同于a.src=\u0026#34;11:11:11:00:55:11\u0026#34; \u0026gt;\u0026gt;\u0026gt; a[IP].dst = \u0026#34;192.168.1.8\u0026#34; # 不能使用a.dst = \u0026#34;...\u0026#34; \u0026gt;\u0026gt;\u0026gt; a[IP].src = \u0026#34;192.168.1.6\u0026#34; \u0026gt;\u0026gt;\u0026gt; a = a/\u0026#34;AAAA\u0026#34; \u0026gt;\u0026gt;\u0026gt; a.show2() a.show2() ###[ Ethernet ]### dst= 11:22:33:44:55:66 src= 11:11:11:00:55:11 type= 0x800 ###[ IP ]### version= 4L ihl= 5L tos= 0x0 len= 32 id= 1 flags= frag= 0L ttl= 64 proto= udp chksum= 0xf76d src= 192.168.1.6 dst= 192.168.1.8 \\options\\ ###[ UDP ]### sport= 1222 dport= 222 len= 12 chksum= 0xf450 ###[ Raw ]### load= \u0026#39;AAAA\u0026#39; \u0026gt;\u0026gt;\u0026gt; a \u0026lt;Ether dst=11:22:33:44:55:66 src=11:11:11:00:55:11 type=0x800 |\u0026lt;IP frag=0 proto=udp src=192.168.1.6 dst=192.168.1.8 |\u0026lt;UDP sport=1222 dport=222 |\u0026lt;Raw load=\u0026#39;AAAA\u0026#39; |\u0026gt;\u0026gt;\u0026gt;\u0026gt;   　可以在组装数据包的时候直接指定各字段的值，在下面的例子中b是和a完全相同的数据包：\n1 2 3  b = Ether(dst=\u0026#34;11:22:33:44:55:66\u0026#34;,src=\u0026#34;11:11:11:00:55:11\u0026#34;)/IP(src=\u0026#34;192.168.1.6\u0026#34;,dst=\u0026#34;192.168.1.8\u0026#34;)/UDP(sport=1222,dport=222)/\u0026#34;AAAA\u0026#34; \u0026gt;\u0026gt;\u0026gt; b \u0026lt;Ether dst=11:22:33:44:55:66 src=11:11:11:00:55:11 type=0x800 |\u0026lt;IP frag=0 proto=udp src=192.168.1.6 dst=192.168.1.8 |\u0026lt;UDP sport=1222 dport=222 |\u0026lt;Raw load=\u0026#39;AAAA\u0026#39; |\u0026gt;\u0026gt;\u0026gt;\u0026gt;   　注：Scapy支持多种协议，具体支持的内容（每种协议/报文的类名称）可以将协议/协议字段作为关键词，在官方文档的API（Scapy API Reference）中进行搜索，例：组装一个TCP数据包：a=Ether()/IP()/TCP()。（NS请求报文：ICMPv6ND_NS、NA应答报文：ICMPv6ND_NA）。\n　ls(\u0026lt;数据包实例\u0026gt;)可以查看该数据包对应字段的类型、当前值、默认值：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  \u0026gt;\u0026gt;\u0026gt; ls(a) dst : DestMACField = \u0026#39;11:22:33:44:55:66\u0026#39; (None) src : SourceMACField = \u0026#39;11:11:11:00:55:11\u0026#39; (None) type : XShortEnumField = 2048 (0) -- version : BitField = 4 (4) ihl : BitField = None (None) tos : XByteField = 0 (0) len : ShortField = None (None) id : ShortField = 1 (1) flags : FlagsField = 0 (0) frag : BitField = 0 (0) ttl : ByteField = 64 (64) proto : ByteEnumField = 17 (0) chksum : XShortField = None (None) src : Emph = \u0026#39;192.168.1.6\u0026#39; (None) dst : Emph = \u0026#39;192.168.1.8\u0026#39; (\u0026#39;127.0.0.1\u0026#39;) options : PacketListField = [] ([]) -- sport : ShortEnumField = 1222 (53) dport : ShortEnumField = 222 (53) len : ShortField = None (None) chksum : XShortField = None (None) -- load : StrField = \u0026#39;AAAA\u0026#39; (\u0026#39;\u0026#39;)   3.2 发送数据包 　发送数据包有两种基本方法：send()和sendp()，使用send()时从IP层开始构造，使用sendp()时从Ether层开始构造。\n send(x, iface=None, **kargs)：x：packet(s)，iface：发送数据包的网络接口（网卡名），在3层发包，程序自动处理链路层上面的 mac 地址，iface表示能够连接到外部网络设备的三层网络接口，如ens33。  　192.168.100.5向192.168.100.7发送下面的IP包，可以在192.168.100.5的 ens33接口监听到这个数据包，也可以在192.168.100.7的ens160接口上监听到这个数据包。\na = IP(dst='192.168.100.5')\rsend(a, iface=\u0026quot;ens33\u0026quot;)\r　注：网络接口名称可以通过ifconfig命令在Linux系统上查看。\n　构造一个UDP数据包，比较加上以太头和不加以太头结果的区别。\na = IP(dst='192.168.100.5')/UDP(sport=666,dport=222)/\u0026quot;test\u0026quot;\rsend(a, iface=\u0026quot;ens33\u0026quot;)\rb = Ether()/IP(dst='192.168.100.5')/UDP(sport=666,dport=222)/\u0026quot;test\u0026quot;\rsend(b, iface=\u0026quot;ens33\u0026quot;)\r　结果是数据包a可以成功在192.168.100.5接收，数据包b不可以。\n　数据包b可能由于错误的MAC地址无法成功发送，根据ifconfig命令修改数据包b中网卡接口的MAC地址，再进行发送，也无法送达。\n　所以应注意使用send()时，不要加Ether()头。\n  sendp(x, iface=None, iface_hint=None, socket=None, **kargs)：x：packet(s)，iface：发送数据包的接口，为2层网络接口，如交换机虚拟出来的2层接口veth2，在2层发包。\n构造一个UDP数据包，和send()测试结果相反地，用sendp()发送数据包时必须加上Ether()，否则数据包无法发送到目的地。192.168.100.5能够收到数据包b，而无法收到数据包a。\n  3.3 从文件读包并发送 　从pcap文件中读取数据包到变量中，依次发送。\n1 2 3  a = rdpcap(\u0026#34;icmpv6_small.pcap\u0026#34;) iface = \u0026#34;veth0\u0026#34; sendp(a, iface)   3.4 自定义协议的数据包 　创建Packet的子类，然后设置它的层关系，例如下面定义了一个计算协议，当以太类型为0x1234时，上层协议是p4calc。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from scapy.all import * class P4calc(Packet): name=\u0026#34;P4 Calculator\u0026#34; fields_desc=[ StrFixedLenField(\u0026#39;P\u0026#39;,\u0026#39;P\u0026#39;,1), StrFixedLenField(\u0026#39;Four\u0026#39;, \u0026#39;4\u0026#39;, 1), ByteField(\u0026#39;Version\u0026#39;, 1), StrFixedLenField(\u0026#39;Op\u0026#39;, \u0026#39;\\x00\u0026#39;, 1), ByteField(\u0026#39;Opcode\u0026#39;, \u0026#39;\\x00\u0026#39;, 2), IntField(\u0026#39;OperA\u0026#39;, 0), IntField(\u0026#39;OperB\u0026#39;, 0), IntField(\u0026#39;Result\u0026#39;, 0)] bind_layers(Ether, p4calc, type=0x1234) p = (Ether(dst=\u0026#34;00:11:22:33:44:55\u0026#34;, src=\u0026#34;00:aa:bb:cc:dd:ee\u0026#34;)/ P4calc(Opcode=\u0026#34;\\x01\u0026#34;, Op=\u0026#34;+\u0026#34;, OperA=10, OperB=20)   3.5 以指定速度发包  文档：https://scapy.readthedocs.io/en/latest/api/scapy.sendrecv.html\n （1）低速发包\n　在3.2中我们仅设置了send和sendp方法的两个参数，即发送的数据包列表（list）x和发送的网络接口iface，但实际上send和sendp方法还有下面几个常用参数：\n x – 待发送的数据包/数据包列表。 inter – 发送两个数据包的间隔，单位为秒，默认值为0。 loop – 默认值为0，不为0则循环发送x，按Ctrl+c停止。 count – 计划发送x的次数，默认为发送1次。 iface – 发送数据包的网络接口。  　在预期数据包速度较低时，可以使用inter参数，设置两个两个数据包的发送间隔。\n　下面的程序构造了一个包含2个ICMPv6 Echo Request数据包的数据包列表，从ens33网口循环发送这个数据包列表，总共发送5次，发送间隔为0.1s。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  from scapy.all import * print(\u0026#34;Sending ICMPv6 Echo Request packets...\u0026#34;) p1 = (Ether(dst=\u0026#34;00:11:22:33:44:55\u0026#34;, src=\u0026#34;00:aa:bb:cc:dd:e1\u0026#34;)/ IPv6(nh=58,src=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C004\u0026#34;,dst=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C001\u0026#34;)/ ICMPv6EchoRequest()) p2 = (Ether(dst=\u0026#34;00:11:22:33:44:55\u0026#34;, src=\u0026#34;00:aa:bb:cc:dd:f1\u0026#34;)/ IPv6(nh=58,src=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C003\u0026#34;,dst=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C001\u0026#34;)/ ICMPv6EchoRequest()) p = [p1,p2] sendp(p, iface=\u0026#34;ens33\u0026#34;, loop=1, inter=0.001, count=5)   　我们运行这个程序，并开启wireshark监听ens33收到的icmpv6数据包，得到的结果如下图：\n图1 用send发送数据包，发送间隔为0.1s:\r\r 　可以看到我们收到了10个数据包，其数据包的发送间隔约为0.1s。我们将inter参数修改为0.001，count参数修改为250，再用wireshark监听ens33，得到结果如下图：\n图2 用send发送数据包，发送间隔为0.001s:\r\r 　理论上应该用0.5s发完，但实际上我们用了2.42s，用sendp中的inter控制发包速度和我们的预期并不相符。如果直接构造一个包含1000个数据包的列表用sendp发送，也不能用0.5s发完。\n（2）高速发包\n　高速发包用sendpfast，需要安装tcpreplay，安装方法如下。\n　ubuntu安装tcpreplay：\n1  apt-get install tcpreplay   　sendpfast的常用参数如下：\n x – 待发送的数据包/数据包列表。 pps – 每秒发送的数据包数。 mpbs – 每秒发送的MBits。 loop – 发送数据包列表的进程数，它的值相当于send中的count，不同的地方在于它发送一次会重新创建一个发包进程，进程的启动速度很慢，高速发包时应将loop设置为1。另外它的默认值为0，如果不设置此参数会发送0次x。 iface – 发送数据包的网络接口。  下面的程序先构造了一个包含1000个数据包的列表，然后将每秒发送的数据包设置为1000。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  from scapy.all import * print(\u0026#34;Sending ICMPv6 Echo Request packets...\u0026#34;) p1 = (Ether(dst=\u0026#34;00:11:22:33:44:55\u0026#34;, src=\u0026#34;00:aa:bb:cc:dd:e1\u0026#34;)/ IPv6(nh=58,src=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C004\u0026#34;,dst=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C001\u0026#34;)/ ICMPv6EchoRequest()) p2 = (Ether(dst=\u0026#34;00:11:22:33:44:55\u0026#34;, src=\u0026#34;00:aa:bb:cc:dd:f1\u0026#34;)/ IPv6(nh=58,src=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C003\u0026#34;,dst=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C001\u0026#34;)/ ICMPv6EchoRequest()) p = [] for i in range(1000): p.append(p1) sendpfast(p, pps=1000, iface=\u0026#39;ens33\u0026#39;, loop = 1)   　我们运行这个程序并用wireshark监听ens33，得到结果如下图。\n图1 用sendpfast发送数据包，每秒发送1000个:\r\r 　从图中我们可以看到发送1000个数据包大约用了1s，和我们的预期相符。\n3.6 监听数据包  文档：https://scapy.readthedocs.io/en/latest/api/scapy.sendrecv.html\n 　在Scapy中，可以使用sniff函数调用抓包分析，并对抓到的包进行回调操作。sniff的详细参数可以查询文档，这里仅介绍几个基本参数：\n count – 捕获的数据包数量，0表示一直捕获。 prn – 对每个捕获的数据包进行的操作函数，函数的输入是捕获的数据包。 filter – 过滤规则，语法同wireshark。 iface – 指定抓包的网卡/网卡列表，不指定则监听所有网卡。  　下面的程序监听8个收到的ipv6数据包，并输出它们的目的IPv6地址\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # sniff.py from scapy.all import * def pack_callback(packet): ipv6_addr = packet[\u0026#39;IPv6\u0026#39;].dst print(ipv6_addr) def start_sniff(): print(\u0026#34;Start Listen ...\u0026#34;) filterstr=\u0026#34;ip6\u0026#34; sniff(filter=filterstr,prn=pack_callback, iface=\u0026#39;ens33\u0026#39;, count=8) if __name__ == \u0026#39;__main__\u0026#39;: start_sniff()   　我们先运行sniff.py监听网卡，然后用下面的程序发送10个IPv6数据包，查看运行结果。\n1 2 3 4 5 6 7 8 9 10 11 12  # sniff_test.py from scapy.all import * print(\u0026#34;Sending ICMPv6 Echo Request packets...\u0026#34;) p = [] for i in range(10): pkt = (Ether(dst=\u0026#34;00:11:22:33:44:55\u0026#34;, src=\u0026#34;00:aa:bb:cc:dd:e1\u0026#34;)/ IPv6(nh=58,src=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C004\u0026#34;,dst=\u0026#34;FD00:A880:0001:0020:0000:0000:0048:C00\u0026#34;+str(i))/ ICMPv6EchoRequest()) p.append(pkt) sendpfast(p, pps=1000, iface=\u0026#39;ens33\u0026#39;, loop = 1)   　运行结果如下，可以看到sniff.py监听了8个数据包。\nStart Listen ...\rfd00:a880:1:20::48:c000\rfd00:a880:1:20::48:c001\rfd00:a880:1:20::48:c002\rfd00:a880:1:20::48:c003\rfd00:a880:1:20::48:c004\rfd00:a880:1:20::48:c005\rfd00:a880:1:20::48:c006\rfd00:a880:1:20::48:c007\r参考资料 1.Scapy文档：https://scapy.readthedocs.io/en/latest/introduction.html\n2.Scapy Git：https://github.com/secdev/scapy\n3.协议头scapy.layers.inet6：https://scapy.readthedocs.io/en/latest/api/scapy.layers.inet6.html?highlight=ICMPv6EchoRequest\n4.Python3下基于Scapy库完成网卡抓包解析：https://cloud.tencent.com/developer/article/1694737\n","description":"Scapy组装数据包、构造数据包、发送数据包、监听数据包的基本方法。","id":6,"section":"posts","tags":["basic"],"title":"工具使用：Scapy","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8scapy/"},{"content":"Google Colab：https://colab.research.google.com/\n1.简介 GoogleTest遵循Abseil生态哲学，Abseil是Google开源的从Google内部代码块中抽取出来的一系列最基础的软件库。GoogleTest用于C++程序测试，它基于xUnit架构，与JUnit和PyUnit相似，不仅支持单元测试，还支持其它任意的测试。\nGoogleTest不使用ISTQB术语中的Test Case，而是使用Test Suite表达相同的含义。\n TEST() 在 Google Test中意为：\nExercise a particular program path with specific input values and verify the results。使用特定输入值执行特定程序路径并验证结果。\n GoogleTest使用可参考用户使用手册，建议从Google Test Primer开始。\n（1）Github地址：https://github.com/google/googletest\n（2）用户使用手册：https://google.github.io/googletest/\n（3）Abseil地址：https://github.com/abseil/abseil-cpp\n2.安装和简单使用 2.1 安装 （1）访问realease页面：https://github.com/google/googletest/releases，下载Source code。\n（2）构建工具支持Bazel和CMake，由于平时的使用习惯，我们选用CMake。\n 使用Cmake查阅：https://google.github.io/googletest/quickstart-cmake.html 使用Bazel查阅：https://google.github.io/googletest/quickstart-bazel.html  （3）安装cmake.\n1  $ sudo snap install cmake   2.2 简单使用 （1）为项目创建一个目录。\n1  $ mkdir my_project \u0026amp;\u0026amp; cd my_project   （2）创建一个CMakeLists.txt文件并声明GoogleTest的依赖。\n1  $ vim CMakeLists.txt   1 2 3 4 5 6 7 8 9 10 11 12 13 14  cmake_minimum_required(VERSION 3.14)project(my_project)# GoogleTest requires at least C++11 set(CMAKE_CXX_STANDARD 11)include(FetchContent)FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/609281088cfefc76f9d0ce82e1ff6c30cc3591e5.zip )# For Windows: Prevent overriding the parent project\u0026#39;s compiler/linker settings set(gtest_force_shared_crt ON CACHE BOOL \u0026#34;\u0026#34; FORCE)FetchContent_MakeAvailable(googletest)  上面的文件声明了一个从Github下载的GoogleTest依赖，609281088cfefc76f9d0ce82e1ff6c30cc3591e5是用于GoogleTest版本的git commit hash值，我们建议更新这个hash以获得最新的版本。\n注：git commit hash是每次提交会生成的hash值，获取方法见附录4.1节。\n（3）创建并运行\n将GoogleTest声明成一个依赖时，就可以在自己的项目中使用GoogleTest代码。\n在my_project目录下，创建一个名为hello_test.cc的文件，文件内容如下（声明了一些基础断言）：\n1 2 3 4 5 6 7 8 9  #include \u0026lt;gtest/gtest.h\u0026gt; // Demonstrate some basic assertions. TEST(HelloTest, BasicAssertions) { // Expect two strings not to be equal.  EXPECT_STRNE(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); // Expect equality.  EXPECT_EQ(7 * 6, 42); }   为构建代码，需要在CMakeLists.txt文件末尾添加如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13  enable_testing()add_executable( hello_test hello_test.cc )target_link_libraries( hello_test gtest_main )include(GoogleTest)gtest_discover_tests(hello_test)  在Cmake中启用了测试，并且声明了想要构建的二进制C++测试（hello_test），并且将它链接到GoogleTest（gtest_main）。最后两行使用Google Test CMake module启用CMake的测试运行器，并发现二进制文件中的测试。\n在my_project目录下构建并运行测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  $ cmake -S . -B build -- The C compiler identification is GNU 9.3.0 -- The CXX compiler identification is GNU 9.3.0 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: /usr/bin/cc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: /usr/bin/c++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Found Python: /usr/local/bin/python3.9 (found version \u0026#34;3.9.5\u0026#34;) found components: Interpreter -- Looking for pthread.h -- Looking for pthread.h - found -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Configuring done -- Generating done -- Build files have been written to: /home/xianyubing/C/my_project/build   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ cmake --build build [ 10%] Building CXX object _deps/googletest-build/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o [ 20%] Linking CXX static library ../../../lib/libgtest.a [ 20%] Built target gtest [ 30%] Building CXX object _deps/googletest-build/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o [ 40%] Linking CXX static library ../../../lib/libgtest_main.a [ 40%] Built target gtest_main [ 50%] Building CXX object CMakeFiles/hello_test.dir/hello_test.cc.o [ 60%] Linking CXX executable hello_test [ 60%] Built target hello_test [ 70%] Building CXX object _deps/googletest-build/googlemock/CMakeFiles/gmock.dir/src/gmock-all.cc.o [ 80%] Linking CXX static library ../../../lib/libgmock.a [ 80%] Built target gmock [ 90%] Building CXX object _deps/googletest-build/googlemock/CMakeFiles/gmock_main.dir/src/gmock_main.cc.o [100%] Linking CXX static library ../../../lib/libgmock_main.a [100%] Built target gmock_main   1 2 3 4 5 6 7 8 9  $ cd build \u0026amp;\u0026amp; ctest Test project /home/xianyubing/C/my_project/build Start 1: HelloTest.BasicAssertions 1/1 Test #1: HelloTest.BasicAssertions ........ Passed 0.00 sec 100% tests passed, 0 tests failed out of 1 Total Test time (real) = 0.02 sec   至此，我们已经使用GoogleTest成功构建了一个二进制测试。\n了解更多可以阅读GoogleTest Guide的GoogleTest Primer和GoogleTest Samples。\n3.使用方法 3.1 程序架构 使用GoogleTest应从写assertion开始，\nassertion：判断语句条件是否为真，结果为success、nonfatal、failure或fatal failure；当结果为fatal failure时，它将当前的函数终止，其它结果程序会继续正常执行。\ntest suite：包含一个或多个测试，测试套件应该反应测试代码的结构。\ntest fixture：同一test suite中的多个测试需要共享相同的对象和子例程时，可以将它们放到test fixture类中。\ntest program：包含多个test suite。\n3.2 断言（Assertion） googletest是和函数调用相似的宏，我们通过编写端来测试类或函数的行为。当断言失败时，googletest打印带有故障信息的断言源文件和所在行的序号，还可以提供自定义失败消息，该消息将附加到 googletest 的消息中。\n断言成对出现，用来测试相同的事物但造成不同的影响。\n ASSERT_ *：当此类断言失败时，生成fatal failures并终止当前函数，适用于发生此故障后程序没必要继续运行的情况。 EXPECT_ *：通常更倾向于使用此类断言，以便于在一个测试中报告更多故障。  由于ASSERT_ *失败时会立刻从当前函数返回，可能会跳过它之后的clean-up代码，从而造成内存泄漏，它不一定需要修复，但这可能导致我们获得除断言错误之外的一个堆检查错误。\n如果想要自定义断言故障消息，仅需要将它用\u0026lt;\u0026lt;操作符或一系列这种操作符以流的方式写入宏。下面的例子使用了ASSERT_EQ和EXPECT_EQ宏来验证值的相等。\n1 2 3 4 5  ASSERT_EQ(x.size(), y.size()) \u0026lt;\u0026lt; \u0026#34;Vectors x and y are of unequal length\u0026#34;; for (int i = 0; i \u0026lt; x.size(); ++i) { EXPECT_EQ(x[i], y[i]) \u0026lt;\u0026lt; \u0026#34;Vectors x and y differ at index \u0026#34; \u0026lt;\u0026lt; i; }   任何可以流入ostream的内容都可以流入断言宏，包括C string和string对象。如果一个宽字符（如wchar_t *，std::wstring）串流入断言，则在打印时将它转换为UTF-8类型。\nGoogleTest提供了一组断言以不同方式来验证代码行为。我们可以检查布尔条件，基于关系操作符比较值的大小，验证字符串值，浮点值等等；还可以自定义谓词实现更复杂的状态验证。由GoogleTest提供的断言可以查看 Assertions Reference。\n3.3 简单示例 创建一个测试：\n（1）使用TEST()宏对测试函数定义并命名，它们是没有返回值的C++函数。\n（2）在这个函数中，可以使用任何合法的C++语句，并使用不同的googletest断言对值进行检查。\n（3）测试结果由断言决定，如果测试中的任何一个断言失败（致命或非致命），或测试崩溃， 则整个测试失败，否则，测试成功。\n1 2 3  TEST(TestSuiteName, TestName) { ... test body ... }   TEST()参数从一般到具体。第一个参数是测试套件的名称，第二个参数是在这个测试套件内部的测试的名称。它们都必须是合法的C++标识符，并且不能包含下划线（_）。测试的全名包括它所在的测试套件名和它自己的名称。不同测试套件的测试可以具有相同的名称。\n例如下面的整型函数，返回n的阶乘：\n1  int Factorial(int n); // Returns the factorial of n   这个函数的测试套件可能像下面这样：\n1 2 3 4 5 6 7 8 9 10 11 12  // Tests factorial of 0. TEST(FactorialTest, HandlesZeroInput) { EXPECT_EQ(Factorial(0), 1); } // Tests factorial of positive numbers. TEST(FactorialTest, HandlesPositiveInput) { EXPECT_EQ(Factorial(1), 1); EXPECT_EQ(Factorial(2), 2); EXPECT_EQ(Factorial(3), 6); EXPECT_EQ(Factorial(8), 40320); }   googletest根据测试套件将测试结果分组，所以逻辑相关的测试应该在同一测试套件中；换句话说，它们TEST()的第一个参数应该相同。上面的例子中有两个测试，HandlesZeroInput和HandlesPositiveInput，它们同属于测试套件FactorialTest。\n当命名测试套件和测试时，我们应当遵循相同的函数和类命名的惯例：https://google.github.io/styleguide/cppguide.html#Function_Names。\n在Linux，Windows，Mac上可用。\n3.4 Test Fixtures 当有两个及以上的测试操作在相同的数据上时，可以使用test fixture，从而对几个不同的测试使用相同的对象配置。\n创建一个fixture：\n（1）从::testing::Test派生一个类，以protected:作为它主体的起始，因为我们希望从子类访问fixture成员。\n（2）在类的内部，声明我们想要使用的对象。\n（3）如果有必要，为每个测试编写一个默认的构造器或SetUp()函数来准备对象。注意不要将SetUp()错写成Setup()，可以在C++ 11中使用override来保证拼写正确。\n（4）如果有必要，编写一个析构器或TearDown()函数来释放在SetUp()中申请的资源，可以阅读FAQ来学习如何使用构造器/析构器以及什么时候使用SetUp()/TearDown()。\n（5）如果有需要，可以为测试定义几个用来共享的子例程。\n当使用fixture时，使用TEST_F()而不是TEST()，因为它允许我们访问text fiture中的对象和子例程。\n1 2 3  TEST_F(TestFixtureName, TestName) { ... test body ... }   和TEST()相似，第一个参数是测试套件名称，但对于TEST_F()来说，第一个参数必须是test fixture的类名称。\nC++的宏系统不允许创建一个单独宏同时处理这两种类型的测试，使用错误的宏会导致编译器报错。\n在TEST_F()中使用test fixture之前，必须定义好test fixture类，否则编译器会报错：virtual outside class declaration。\n对于TEST_F()中定义的每一个测试，googletest会创建一个新的test fixture，并立刻用SetUp()对它初始化，运行该测试，并通过调用TearDown()进行清理，然后删除该test fixture。在同一测试套件中的不同测试具有不同的test fixture对象，googletest在它创建下一个test fixture对象之前会删除一个test fixture。googletest不会对多个测试重用相同的test fixture。单个测试对fixture的任何改变都不会影响到其它测试。\n例如，下面的例子是一个类Queue（FIFO队列）的测试，具有如下接口：\n1 2 3 4 5 6 7 8 9  template \u0026lt;typename E\u0026gt; // E is the element type. class Queue { public: Queue(); void Enqueue(const E\u0026amp; element); E* Dequeue(); // Returns NULL if the queue is empty.  size_t size() const; ... };   首先定义一个fixture class。按照惯例，我们应当将其命名为FooTest，其中Foo是被测试的类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  class QueueTest : public ::testing::Test { protected: void SetUp() override { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // void TearDown() override {}  Queue\u0026lt;int\u0026gt; q0_; Queue\u0026lt;int\u0026gt; q1_; Queue\u0026lt;int\u0026gt; q2_; };   在这个例子中，由于我们不需要在每个测试后清理，所以我们不需要TearDown()，除了析构函数已经完成的操作。\n现在我们可以使用TEST_F()和这个fixture写测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  TEST_F(QueueTest, IsEmptyInitially) { EXPECT_EQ(q0_.size(), 0); } TEST_F(QueueTest, DequeueWorks) { int* n = q0_.Dequeue(); EXPECT_EQ(n, nullptr); n = q1_.Dequeue(); ASSERT_NE(n, nullptr); EXPECT_EQ(*n, 1); EXPECT_EQ(q1_.size(), 0); delete n; n = q2_.Dequeue(); ASSERT_NE(n, nullptr); EXPECT_EQ(*n, 2); EXPECT_EQ(q2_.size(), 1); delete n; }   上面的测试使用了ASSERT_ *断言和EXPECT_ *断言。经验法则是当我们希望测试在断言失败后继续显示更多错误时使用EXPECT_*，在断言失败后继续运行测试毫无意义时使用ASSERT_ *。例如，在Dequeue测试的第二条断言是ASSERT_NE(n,nullptr)，因为我们接下来要取n指针指向的值，当n为NULL时会导致段错误。\n当这些测试运行时，会发生下面的事：\n（1）googletest构造一个QueueTest对象，称为t1。\n（2）t1.SetUp()初始化t1。\n（3）第一个测试IsEmptyInitially在t1上运行。\n（4）t1.TearDown()在测试结束后进行清理。\n（5）t1被解构。\n（6）在另一个QueueTest对象上重复上述步骤，这次额运行DequeueWorks测试。\n在Linux，Windows，Mac上可用。\n3.5 调用测试 TEST()和TEST_F()用googletest隐式注册它们的测试。所以不必像其它C++测试框架那样为了运行我们定义的测试而重新列出这些测试的列表。\n在定义我们的测试以后，可以用RUN_ALL_TESTS()运行它们，如果所有测试均成功，则返回0，否则返回1。注意RUN_ALL_TESTS()运行连接单元中的所有测试，它们可以来自不同的测试套件甚至是不同的源文件。\n当RUN_ALL_TESTS()宏被调用时：\n 存储所有googletest flags的状态。 为第一个测试创建一个test fixture对象。 通过SetUp()对它初始化。 在fixture对象上运行测试。 通过TearDown()清理fixture。 删除fixture。 恢复所有googletest flags的状态。 对下一个测试重复上述所有步骤，直至测试运行完毕。  如果出现了fatal failure，那么后续步骤都会被终止。\n不能忽视RUN_ALL_TESTS()的返回值，否则编译器会报错。这种设计的基本原理是自动化测试服务根据其退出代码（而不是其 stdout/stderr 输出）来确定测试是否通过，因此main()函数必须返回RUN_ALL_TESTS()的返回值。并且只能调用RUN_ALL_TESTS() 一次，多次调用它会与一些高级 googletest 功能（例如，线程安全的死亡测试）发生冲突，因此不受支持。\n在Linux，Windows，Mac上可用。\n3.6 编写main()函数 大多用户不应该自己编写main函数，而应该用gtest_main（与gtest相反）连接，它定义了合适的入口点。本节的其余部分仅在一些需要在测试运行之前进行的、无法在fixture和测试套件框架内表达的自定义操作时才使用。\n如果自己编写main函数，它应该返回RUN_ALL_TESTS()的值。\n下面是自定义main函数的样板：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  #include \u0026#34;this/package/foo.h\u0026#34; #include \u0026#34;gtest/gtest.h\u0026#34; namespace my { namespace project { namespace { // The fixture for testing class Foo. class FooTest : public ::testing::Test { protected: // You can remove any or all of the following functions if their bodies would  // be empty.  FooTest() { // You can do set-up work for each test here.  } ~FooTest() override { // You can do clean-up work that doesn\u0026#39;t throw exceptions here.  } // If the constructor and destructor are not enough for setting up  // and cleaning up each test, you can define the following methods:  void SetUp() override { // Code here will be called immediately after the constructor (right  // before each test).  } void TearDown() override { // Code here will be called immediately after each test (right  // before the destructor).  } // Class members declared here can be used by all tests in the test suite  // for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F(FooTest, MethodBarDoesAbc) { const std::string input_filepath = \u0026#34;this/package/testdata/myinputfile.dat\u0026#34;; const std::string output_filepath = \u0026#34;this/package/testdata/myoutputfile.dat\u0026#34;; Foo f; EXPECT_EQ(f.Bar(input_filepath, output_filepath), 0); } // Tests that Foo does Xyz. TEST_F(FooTest, DoesXyz) { // Exercises the Xyz feature of Foo. } } // namespace } // namespace project } // namespace my  int main(int argc, char **argv) { ::testing::InitGoogleTest(\u0026amp;argc, argv); return RUN_ALL_TESTS(); }   ::testing::InitGoogleTest()函数解析命令行以获取 googletest flags，并删除所有已识别的标志。用户可以通过各种标志来控制测试程序的行为，AdvancedGuide 介绍了这些标志。必须在调用RUN_ALL_TESTS()之前调用此函数，否则标志将无法正确初始化。\n在 Windows 上，InitGoogleTest()也适用于宽字符串，因此它也可用于以UNICODE模式编译的程序。\n编写所有这些main函数需要花费大量时间，所以Google Test提供main()函数的基础实现，如果它符合需求，只需要将gtest_main和测试链接起来即可。\n3.7 局限性 Google Test 被设计为线程安全的。该实现在 pthreads 库可用的系统上是线程安全的。目前在其他系统（例如 Windows）上同时使用来自两个线程的 Google 测试断言是不安全的。在大多数测试中，这不是问题，因为通常断言是在主线程中完成的。\n4 附录 4.1 git commit hash获取方法 （1）首先在googletest的Git页面找到release，跳转到Release页面后点击图1红框部分（或点击红框下面的e2239ee可直接跳转到第3步）；\n图1 Release页面:\r\r （2）页面跳转后，点击图2红框部分；\n图2 Release版本对应的源码:\r\r （3）页面跳转后，图3红框中的内容即为commit hash。\n图3 Release版本对应的Commit Hash:\r\r ","description":"Google Test的初级使用方法","id":7,"section":"posts","tags":["C"],"title":"工具使用：GoogleTest","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8googletest/"},{"content":"机器学习入门 1 基本概念 　注：仅作简要介绍，帮助理解。需要阅读或学习过《机器学习》、《线性代数》课程（可以没学明白，但得上过课）。\n1.1 线性代数 　向量在机器学习中就是对象的属性列表，可以理解为一个属性list，比如[白(颜色), 体积(大), 成熟(是否成熟)]，颜色、体积、成熟程度显然没有在一个次元里，不能直接做加减乘除运算，直接运算（比如三个值相加）没有任何实际意义；为了计算出合理而有意义的值，用一个列表来表示这些属性，每一个属性处在自己的维度中，这就是线性代数，然后利用线性代数定义的规则进行运算。为了计算方便，用不同的字母表示这些属性[a, b, c]。\n　很多文章用二维三维图来帮助理解线性代数，读完以后【困惑增加了】，二维三维图是为了帮助理解线性代数的几何含义，理解这些属性在维度上的区别和联系，即虽然它们不在同一个维度里，依然能够互相影响，证明线性代数运算法则的正确性。\n1.2 监督学习 　数据集中的每条记录都包含一个标签或标志；即数据集有明确的分类结果。\n 回归：输出连续值，比如预测粗糙度。模型包括支持向量机（SVM），人工神经网络，logistic回归，决策树和随机树； 分类：输出0/1或者离散值，比如预测它不是一个好吃的瓜🍉。模型包括线性回归，贝叶斯线性回归，决策树回归。  1.3 无监督学习 　数据集中的记录不包含任何标签或标志；即数据集没有明确的分类\n 聚类：通过聚类算法把某些数据聚集在一起，表现为一团一团或一堆一堆。模型包括层次聚类，k- 均值聚类。  1.4 强化学习 　感知环境状态，根据反馈的结果(奖励，Reward)，学习出一个最大化长期总收益的策略（动作，Action），并不断强化策略。如AlphaGo。\n1.5 特征工程 　对上课内容没什么印象😅，我的理解是一种物体有很多个特征，但是不同场景下，一种物体和另一种物体区分开只需要其中几种特征（比如一个袋子中只有钢球和棉花球，只需要用软硬程度一种特征就可以区分了），特征工程就是在问题场景下，筛选出能够解决问题的特征，简化运算过程，节省计算资源。下面是几种特征分析方法的提名。\n　主成分分析（PCA）：线性降维方法，找出包含信息量较高的特征主成分。\n　前向搜索：最开始不选取任何特征，计算模型的交叉验证误差，然后选择最相关的特征，将这个特征加入到已有特征；重复选取其它所有候选特征直到达到期望数量的特征为止。\n　后向搜索：从所有特征开始。计算模型的交叉验证误差先移除最不相关的特征，对其它所有候选特征，重复这一过程直到达到期望数量的特征为止。\n1.6 模型  摘录总结自《机器学习》周志华\n 　机器学习研究的主要内容是这样一种学习算法：利用计算机，从数据中产生模型的算法。\n　模型：把经验数据提供给学习算法，学习算法根据这些数据产生模型，即从数据中学得的结果。\n1.7 查准率(Precision)和查全率(Recall) 　Precision和Recall通常用来评估机器学习模型的性能。\n　🍓查全率很多人称为召回率，但召回和其计算方式完全找不到联系，个人这个翻译不够灵活，不利于理解。\n TP：真正例(True Positive)，真值1，预测值1； FP：假正例(False Positive)，真值0，预测值1； TN：真反例(True Negative)，真值0，预测值0； FN：假反例(False Negative)，真值1，预测值0；  　查准率(Precision)计算方法：P=TP/(TP+FP)；\n　查全率(Recall)计算方法：R=TP/(TP+FN)。\n2 过程 2.1 工具 　数据：csv文件\n　编程语言：便于学习的Python。\n pandas：数据分析库，扩展了数据关系，提供了大量能使我们快速便捷地处理数据的函数和方法。可用于导入数据 numpy：是一个由多维数组对象和用于处理数组的例程集合组成的库。可完成基础数值计算。 matplotlib：用于数据可视化，与numpy一起使用，成为了有效地Matlab开源替代方案。 scikit-learn：开源的Python机器学习库，提供了大量用于数据挖掘和分析的工具，包括数据预处理、交叉验证、算法与可视化算法等一系列接口。基本功能分为六个部分，分类，回归，聚类，数据降维，模型选择，数据预处理。  　ipython notebook：是一个 Web应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。第3章中Kaggle的Code平台就是用的这个应用程序。\n2.2 机器学习方法应用流程 得到数据集，开始使用机器学习方法：\n　-\u0026gt;了解数据(箱型图、密度图/直方图、散点图)\n　-\u0026gt;数据预处理（处理缺失值和异常值；利用特征工程选出特征）\n　-\u0026gt;把数据拆分为测试集和训练集\n　-\u0026gt;挑选一个模型\n　-\u0026gt;使用管道组装\n　-\u0026gt;训练模型-\u0026gt;使用模型，输入测试集\n　-\u0026gt;评估模型并可视化\n　-\u0026gt;调整超参数（人为设置的参数）\n了解数据的方法：\n 箱型图：识别异常值 密度图/直方图：显示数据的分布 散点图：描述双变量关系  3 练习 　暂时用大家都推荐的平台进行学习和探索：https://www.kaggle.com/\n　选用kaggle的Titanic教程进行练习，参考其教程，对教程进行解析，不再重复其平台操作教程。\n　📣：可能找不到控制台，Code就是控制台，改过好多次名。\n 参考练习的教程：https://www.kaggle.com/alexisbcook/titanic-tutorial\n 3.1 问题描述 　Titanic：什么样的人更有可能生存？使用乘客数据（即姓名，年龄，性别，社会经济舱等）\n　数据集：给定了训练集train.csv和test.csv。train.csv给定了乘客的详细属性以及是否获救。test.csv没有标定乘客的获救信息。使用891个数据生成的预测模型，预测418名乘客是否获救。\n　输出：一个csv文件，包含两列，一共418行，第一列为PassengerId，第二列为Survived，值为0/1。\n　属性集：PassengerId、Survived(0-未获救，1-获救)、Pclass(社会经济地位代表，1-高，2-中，3-低)、Name、Sex、Age(20% missing)、SibSp(几个兄弟/配偶)、Parch(几个孩子/父母)、Ticket、Fare。\n3.2 了解数据 　用pandas读入数据，用scikit-learn建立机器学习模型。\n pandas文档：https://pandas.pydata.org/pandas-docs/stable/\nsklearn文档：https://scikit-learn.org/stable/\n 3.2.1 读入数据 1 2 3 4 5  #pd.read_csv读取csv文件，test_data是DataFrame对象 test_data = pd.read_csv(\u0026#34;/kaggle/input/titanic/test.csv\u0026#34;) train_data = pd.read_csv(\u0026#34;/kaggle/input/titanic/train.csv\u0026#34;) #dataFrame.head()，列出该dataFrame的前5行 test_data.head()   图1 输出test_data的前5行:\r\r 3.2.2 粗略分析数据 　下面的代码中，dataFrame.loc[行][列]表示提取指定的行和列，这里提取Sex=female的行对应的Survived列。一共有314行，women的长度为314，而获救的Survived值为1，所以可以用sum(women)快速统计获救的女性数量，从而用sum(women)/len(women)计算出女性获救的比率。\n1 2 3 4 5 6 7 8 9 10 11  #提取Sex=female的行对应的Survived列 women = train_data.loc[train_data.Sex == \u0026#39;female\u0026#39;][\u0026#34;Survived\u0026#34;] print(women) rate_women = sum(women)/len(women) print(\u0026#34;% of women who survived:\u0026#34;, rate_women) men = train_data.loc[train_data.Sex == \u0026#39;male\u0026#39;][\u0026#34;Survived\u0026#34;] rate_men = sum(men)/len(men) print(\u0026#34;% of men who survived:\u0026#34;, rate_men)   图2 输出男性和女性获救的比率:\r\r 　可以看出性别是一个很强的获救特征。而仅通过一个特征就可能做出有效的预测，如果考虑多个特征（列），可能会有更有效地预测结果，可以通过机器学习（实际就是利用线性代数进行运算）来完成综合多个列的预测。\n3.2.3 处理空值 　观察数据，发现年龄有空值，并且根据常识，年龄可能会影响获救，考虑处理年龄的空值。(根据实际情况，年龄丢失的，应当100%未获救，获救怎么可能唯独不知道年龄，除非收集好的数据在后续存储出现了丢失）\n　两种办法处理空值：（1）删除[8]；（2）填充\n　根据个人水平（很🥦），选择为年龄填充平均值的方法（猜测填充众数更合理）：\n1 2 3 4 5  #fill na age with avg #inplace : boolean, 默认值 False。如果为Ture,在原地填满。 #fill na age with avg train_data[\u0026#39;Age\u0026#39;].fillna(train_data[\u0026#39;Age\u0026#39;].mean(),inplace = True) test_data[\u0026#39;Age\u0026#39;].fillna(test_data[\u0026#39;Age\u0026#39;].mean(),inplace = True)   3.3 拆分数据集 　这里已经拆好数据集和测试集，直接使用即可。\n3.4 了解模型，选择模型 　这里没有进行比较，为教学说明，直接选择了随机森林模型。\n3.4.1 随机森林  随机森林内容引用：https://easyai.tech/ai-definition/random-forest/\n 　随机森林由许多决策树构成，不同决策树之间没有关联；当执行分类任务时，输入样本，样本进入每一棵决策树进行判断，每棵树会得到一个分类结果，最终属于哪个分类的结果最多，就把该样本判断为哪个分类。\n决策树形成：\n 一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m \u0026laquo; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。  优缺点：\n 优点：实现简单；无需降维，无需特征选择，可用于高维数据；有部分特征遗失，仍可以维持准确度；训练速度较快，容易实现并行方法。 缺点：取值划分多的属性会对随机森林产生更大的影响。  实现：\n　DolphinDB速度最快，scikit-learn效果也不错。\n　这里选用scikit-learn的python包sklearn，其随机森林类为RandomForestClassifier。\n3.5 训练并预测 　开始训练并预测：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from sklearn.ensemble import RandomForestClassifier y = train_data[\u0026#34;Survived\u0026#34;] features = [\u0026#34;Pclass\u0026#34;, \u0026#34;Sex\u0026#34;, \u0026#34;SibSp\u0026#34;, \u0026#34;Parch\u0026#34;, \u0026#34;Age\u0026#34;] #get_demmies(data,...)提取指定特征，即从train_data中提取这5个特征 X = pd.get_dummies(train_data[features]) X_test = pd.get_dummies(test_data[features]) #n_estimators：随机森林中树的个数；max_depth：树的最大深度；random_state：随机状态，默认由np.numpy生成 model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1) #fit:X是输入数据，y是标签 model.fit(X, y) #predict的返回值是数值，表示样本属于每一个类别的概率 predictions = model.predict(X_test) output = pd.DataFrame({\u0026#39;PassengerId\u0026#39;: test_data.PassengerId, \u0026#39;Survived\u0026#39;: predictions}) output.to_csv(\u0026#39;my_submission.csv\u0026#39;, index=False) print(\u0026#34;Your submission was successfully saved!\u0026#34;)   　结果出现下图，则预测成功。\n图3 训练成功:\r\r 　然后右上角点击Save Version。保存好后重新进入Code页面，选择该Work，右边栏选择Output，选中该文件，然后Submit。\n图4 Code页面:\r\r 图5 Output页面提交:\r\r 　提交后查看结果，😅。\n图6 预测结果1:\r\r 　不使用Age特征的结果，🤓。\n图7 预测结果2:\r\r 　🍰机器学习的体验过程到这里就结束了。\n参考资料 1.机器学习基础：https://developer.ibm.com/zh/articles/introduction-to-machine-learning/\n2.机器学习入门实战：https://developer.ibm.com/zh/tutorials/build-and-test-your-first-machine-learning-model-using-python-and-scikit-learn/\n3.学习机器学习之如何根据需求选择一种算法：https://blog.csdn.net/weixin_43730955/article/details/100599229\n4.如何正确选择机器学习算法：https://zhuanlan.zhihu.com/p/141985216\n5.随身GPU服务器：Kaggle中kernels的快速入门指南：https://oldpan.me/archives/kaggle-kernels-quick-introduction\n6.pandas读取或选择某几列：https://blog.csdn.net/aaa_aaa1sdf/article/details/77414387\n7.fillna方法实现部分自动填充：https://zhuanlan.zhihu.com/p/114319226\n8.[数据清洗]pandas dataframe空值的处理方法：https://zhuanlan.zhihu.com/p/35321806\n9.将DataFrame某列中的空值填充为0：https://blog.csdn.net/weixin_43474731/article/details/101689897\n","description":"包括机器学习的基本概念，运用机器学习方法的过程以及一个简单的机器学习方法运用实例","id":8,"section":"posts","tags":["分布式"],"title":"研究方法：机器学习","uri":"https://yyyIce.github.io/zh/posts/%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"content":"　论文名称：Offloading Real-time DDoS Attack Detection to Programmable Data Planes\n　会议信息：2020 IFIP Networking Conference (Networking)\n　成果：在受到DDoS攻击时，源IP地址和目标IP地址的分布往往会偏离合法模式，本文参考异常检测常用特征，由m个数据包内的源IP和目的IP的熵计算源IP和目的IP的集中趋势指数和分散指数，利用两个指数在可编程数据平面进行DDoS攻击检测。\n　前人工作不足：防御机制依赖于监视原语：分组采样(sFlow)、基于流的统计(NetFlow、OpenFlow)，监视原语带来数据包处理和资源利用的大量开销（采样和草图），控制循环长，检测等待时间长。Snoata、Marple基于可编程数据平面配置自适应过滤器，由滤波器确定需要转发到控制平面进行检查的数据包流，影响了网络使用率，用于攻击检测可能造成较高的延迟；StateSec是完全基于软件的SDN DDoS攻击检测机制，但速度较慢。\n　本文工作：在转发设备上完全实现实时DDoS攻击检测。估计传入数据包源IP和目的IP的熵，用熵值计算两个特征的阈值——集中趋势指数和分散指数，用于检测DDoS攻击。为满足转发设备的限制，使用定点表示法表示浮点数，通过专门定制的计数草图(sketch)估算不同IP地址的频数，使用LPM表查找的方式近似对数运算，利用熵值增减减少直接根据公式计算熵值的计算量。\n一、背景 　DDoS攻击检测的现有方案理论上可行，但基本存在检测时间长、对网络使用率影响大、速度慢等缺点。文章参考异常入侵检测的方法，通过观测源IP地址和目的IP地址的香农信息熵计算两个特征指数，得到源IP地址和目的IP地址的分布，从而判断是否发生DDoS攻击。\n香农信息熵 　香农信息熵：衡量信息的不确定程度，越不确定，熵越大。\n自信息量  信息出现的概率越大，不确定程度越小，不确定性函数f是概率P的减函数； 两个独立符号所产生的不确定性应该等于各自不确定性之和，即f(P1,P2)=f(P1)+f(P2)，即可加性。  　对数函数满足上述两个条件：f(P) = log(1/p)=-log(p)\n　I(X)：自信息量，I(x)=log(1/x)\n　信息熵是信息量的期望（均值），它不是针对每条信息，而是针对整个不确定性结果集而言，信息熵越大，事件不确定性就越大。单条信息只能从某种程度上影响结果集概率的分布。\n图1 信息熵:\r\r 二、本文的工作 　攻击场景和威胁模型：攻击者协调分布各地的主机，向单个受害者发送非法的服务请求\n　假设DDoS攻击的特征是大量主机流量汇聚到一个或几个受害者，在存在恶意活动的情况下，源IP地址和目的IP地址的分布往往偏离合法模式。\n　估计输入数据包流的连续分区(观察窗口)的IP地址的熵，在每个观察窗口的结尾，用流量表征单元读取熵值以生成合法的流量模型(即两个特征指数：EWMA和EWMMD)；异常检测单元根据模型计算2个值，判断是否发生DDoS攻击，如果发生DDoS攻击则发出警报。\n图2 DDOS攻击检测模型:\r\r 三、实现 1 熵估计模块 熵的推导 　设X为每个窗口中的全部IP地址，共计m个，N种，则：\n图3 m个IP地址的熵计算:\r\r 　当m能够充分代表当前源、目的IP地址的分布时，发生攻击时，源IP地址的熵随其频数的增加而增大，目的IP地址的熵随其频数的增加而降低。\n　m值增大，则每次测量需要接收更多的数据包，从而增加攻击检测延迟；m值减小，则无法区分与恶意流量相关的分布变化与合法流量的短期波动。\n熵估计的实现 图4 IP地址的熵:\r\r 　得到的熵计算式对于可编程交换机来说较为复杂，文章利用近似来简化计算过程：\n 使用Count-Sketch估计窗口中IP地址的频数 将m设置为2的乘方，以便log2(m)是整数，并且熵计算式的第二项S/m可以表示为移位计算 用熵的增量计算下一个窗口的熵值 提前计算不同频数下熵的增量值，构建LPM查找表，Key为频数  图5 熵估计流水线:\r\r Count Sketch数据结构：d个寄存器(深度)，共w个槽(宽度)，每个槽是一个IP地址的计数器和该IP所在窗口的ID\n  哈希函数h_i将IP地址映射到第i个寄存器的槽中\n  哈希函数g_i确定IP地址的计数器是递增还是递减的，处理h_i的IP地址冲突\n  定义了两个操作：\n Update(C;x)：计数器的值（哈希h_i）加上哈希g_i的结果 Estimate(C,x)：x出现的频率计数的估计值，d个哈希g_i和哈希h_i的乘积的中位数，此中位数作为频数    图6 Count-Sketch计算流程:\r频数估计方法\r\r 图7 熵值估算方法:\r\r 2 流量表征模块 　计算两个指数表征流量：\n 集中趋势指数Mn：指数加权移动平均值（EWMA） 分散指数Dn：指数加权移动平均差（EWMMD）  图8 指数计算公式:\r\r 3 异常检测模块 　成立下图中的一个条件，就会触发DDoS攻击警报：\n图9 异常检测公式:\r\r 　其中k是灵敏度系数，k值增加会使检测条件更严格，以获得更高的准确性；k值降低可能会增加误报，从而扩大异常检测的覆盖范围。\n四、评估 1 实验环境 数据集  合法流量：CAIDA Anonymized Internet Traces 2016 恶意流量：CAIDA DDoS Attack 2007  P4实现  为每个Sketch分配一个32位寄存器，并为其关联的观察窗口标识符分配一个8位寄存器。 考虑到具有4个小数位的定点表示形式，我们将S和H的估值存储在32位寄存器中。 建立LPM查找表，以确保每个条目的最大误差为2^(-4)，总共245个TCAM条目。  参数设置 图10 系统参数设置:\r\r 2 评估结果 　熵估计误差随着Sketch宽度的增加而减小，但是这种减小随着宽度的增加而衰减并稳定在接近1％的水平；Sketch深度的增加会减少碰撞概率，但增加深度会导致需要对每个IP地址处理更多的Hash函数，增加值运算的复杂性。\n图11 熵估计误差与Sketch宽度和深度的关系:\r\r 　较低的灵敏度系数值会提高检测阈值，从而导致较高的检测率，但会造成高误报率。生产网络中具有动态性，可能需要定期调整k值。\n图12 灵敏度系数和真阳性假阳性的关系:\r\r 　恶意流量所占的比例越高，检测准确率将越来越高（超过90％），该精度受计数草图宽度的影响很大。\n图13 恶意流量占比和检测准确率的关系:\r\r 　与sFlow对比，文章题出的方法均比sFlow(采样比为1:100和1:1000)好。\n图14 与sFlow对比:\r\r ","description":"在受到DDoS攻击时，源IP地址和目标IP地址的分布往往会偏离合法模式，本文参考异常检测常用特征，由m个数据包内的源IP和目的IP的熵计算源IP和目的IP的集中趋势指数和分散指数，利用两个指数在可编程数据平面进行DDoS攻击检测。","id":9,"section":"posts","tags":["P4"],"title":"论文阅读：卸载实时DDoS攻击检测到可编程数据平面","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8D%B8%E8%BD%BD%E5%AE%9E%E6%97%B6ddos%E6%94%BB%E5%87%BB%E6%A3%80%E6%B5%8B%E5%88%B0%E5%8F%AF%E7%BC%96%E7%A8%8B%E6%95%B0%E6%8D%AE%E5%B9%B3%E9%9D%A2/"},{"content":"　论文名称：Machine-learning-assisted DDoS attack detection with P4 language\n　会议信息：2020 IEEE International Conference on Communications (ICC). IEEE, 2020: 1-6.\n　成果：开发了用于SDN网络的机器学习辅助DDoS攻击检测框架，利用P4可编程数据平面完成特征提取，减少检测延迟。\n　前人工作不足：在SDN控制器上进行特征提取造成延迟较大。\n　本文工作：利用P4可编程数据平面完成特征提取，定期向控制器报告特征，在控制器上利用三种机器学习算法对特征进行二分类，做出决策，实时检测DDoS攻击（以TCP Flood攻击为例）。\n一、背景 　软件定义网络（SDN）与传统网络相比具有许多优势，如自动化网络控制、有效资源利用、灵活和快速的重配置等。由于SDN控制器的集中式特点，它容易受到DDoS攻击，发送非法数据包会使控制器超载，从而抑制数据平面设备的正常运行，即使DDoS攻击的目标不是SDN控制器，由于SDN交换机实现的默认转发策略，它也可能被淹没。\n　DDoS攻击检测（DAD）需要自动学习流量中的隐藏模式并检测异常，通常依赖于机器学习（ML）。 已有的解决方案包括使用支持向量机分类器（SVM）、使用string kernel改进SVM分类、深度递归神经网络、改进k-近邻等方法。利用状态数据平面协助DDoS攻击检测是最近的研究趋势。\n　针对网络/传输级的DDoS防御机制：基于源、基于目标、基于网络。\nTCP Flood 　攻击者伪装成多个IP地址向受害者同时发送多个SYN请求，受害者的系统发送ACK响应，等待SYN-ACK数据包，攻击者不发送SYN-ACK数据包，导致受害者一直等待，资源被大量消耗，直至死机，导致受害者无法响应其它正常用户的TCP连接请求。\n　攻击者需要注意，伪装的IP地址不能够对收到的SYN-ACK数据包进行响应。\n 防御手段参考1：https://www.cnblogs.com/sunsky303/p/11811097.html\n 减少SYN-ACK数据包的重发次数； 使用SYN Cookie； 增加backlog队列； 限制SYN并发数。  SYN Flood原理防御手段参考2：https://blog.csdn.net/shixin_0125/article/details/78829069\n　TCB(TCP 传输控制块)是一种包含一个连接所有信息的传输协议数据结构(实际上在许多操作系统中它是用于处理进站(inbound)连接请求的一个队列，该队列保存那些处于半开放(half-open)状态的TCP连接项目，以及已建立完整连接但仍未由应用程序通过accept()调用提取的项目)。\n　一个单一的TCB所占内存大小取决于连接中所用的TCP选项和其他一些功能的实现。通常一个TCB至少280字节，在某些操作系统中已经超过了1300字节。TCP的SYN-RECEIVED状态用于指出这个连接仅仅是半开连接，请求是否合法仍被质疑。注意，TCB分配空间的大小取决于接收的SYN包。\n　到达的SYN包将被分配过多的TCB而导致主机的内核内存被耗尽。为了避免这种内存耗尽，操作系统通常给监听接口关联了一个\u0026quot;backlog\u0026quot;队列参数，它同时维护连接的TCB上限数量和SYN-RECEIVED状态。尽管这种方案使主机的可用内存免遭攻击，但是backlog队列本身就带来了一个(小的)受攻击源。当backlog中没有空间时，就不可能再响应新的连接请求，除非TCB能被回收或者从SYN-RECIEVE状态中移除。\n　试图发送足够多的SYN包而耗尽backlog是TCP SYN洪泛的目的。攻击者在SYN包中加入源IP地址，这样就不会导致主机将已分配的TCB从SYN-RECEVIED状态队列中移除(因为主机将响应SYN-ACK)。因为TCP是可靠的，目的主机在断开半开连接并在SYN-RECIEVED队列中移除TCB之前将等待相当长的时间。在此期间，服务器将不能响应其他应用程序合法的新TCP连接请求。\n 　根据TCP Flood原理，基于目标的防御手段可以采用参考1中的方法，如SYN Cookie，基于网络的防御方法包括过滤（用处不大）、防火墙代理、活动监视器。\n 防火墙作为客户端和服务器之间的代理，在防火墙上配置SYN Cookies或SYN缓存，只需要防火墙顶住SYN Flood攻击。 活动监视器如果发现SYN包来源于它知道的攻击者源地址就会立刻发送伪装RST包给服务器，释放SYN连接。  二、本文的工作 　文章采用基于网络的防御机制，结合机器学习和P4有效执行DDoS攻击中的TCP Flood检测。正确的攻击检测需要耗时且昂贵的深度数据包检查，需要减轻SDN控制器的负担，所以文章使用P4可编程数据平面进行数据包处理，并将DDoS攻击检测建模为基于机器学习的分类问题。文章说他们是第一个将机器学习和P4数据平面结合的工作。\n三、实现 　攻击检测系统整体框架如下图。\n图1 整体框架:\r\r 机器学习检测模块 　DDoS攻击检测模块定期从P4交换机接收流量信息，根据该信息执行攻击检测，决定是否转发数据包。检测模块对给定时间间隔内观察到的流量（即，一个或多个数据包到达P4交换机）输出决策，即标签为攻击或无攻击表示在所考虑的时间范围内是否存在攻击。\n　检测模块由两部分组成：\n 特征提取器； 机器学习分类器。  　特征提取器：提取5个特征\n Len(t)：时间窗口中数据包的平均大小，以字节为单位； RTCP(t):TCP数据包在时间窗口总数中所占的百分比； RUDP(t)：UDP数据包在时间窗口总数中所占的百分比； RTU(t)：时间窗口（t，t + T）中TCP和UDP数据包之间的比率。 Flags(t)：具有活动SYN标志的TCP数据包在总时间窗口（t，t + T）中所占的百分比。  　机器学习算法：二分类，输出0表示no attack，1表示TCP flood。\n 随机森林（RF）； 最近邻（KNN）； 支持向量机（SVM）。  P4程序 　流量信息在滑动窗口（持续时间为T）的基础上定期（每δ秒）从P4交换机发送到检测模块，每个窗口都包含最近T秒的信息，独立于其他窗口进行；然后利用分类得到的标签执行数据包操作，如丢弃。\n　文章使用P4_14语言。\n　程序定义三层协议和四层协议的数据包头部；定义自定义头部，包括switch_id和4个2字节的字段，报告总数、TCP、UDP、TCP SYN数据包的数量。\n　程序定义4个寄存器：存储IP、UDP、TCP、SYN数据包数量。\n　程序定义4张Match-Action Table：m_ip、m_transport、m_syn、go_header。\n　各类型数据包到来时，匹配对应的Table(如m_transport)，触发相应动作，更新寄存器（如udp_action）的值；\n　在流量窗口的数据包计数(meta.counter_tot)达到允许的最大数据包数时，应用go_header表，将自定义头部插入数据包(动作为add_int)，将寄存器中的值和和结果数据包发送到检测模块，然后寄存器复位(go_read_reset)。\n图2 部分P4代码:\r\r 四、评估 实验设置 　在具有8×2 GHz处理器和8 GB RAM的桌面上，使用keras和sklearn库，已经使用基于Python的脚本实现了ML算法。\n　使用Spirent N4U流量生成器收集用于训练和测试算法的流量数据。\n　生成了30分钟的流量，其中将10、50或100 kbit/s恒定比特率的TCP Flood攻击流添加到10 Mbit/s的常规后台流量中。攻击的平均持续时间为10秒，后台流量由三种不同的流量组成：4.5 Mbit/s TCP流量，3.8 Mbit/s UDP流量和1.7 Mbit/s IP流量，数据包长度遵循iMix分配。对所有持续时间为T的窗口，并仅在其包含至少一个属于TCP泛洪攻击的数据包时才分配标签1：TCP flood，否则为该窗口分配标签0：noattack。\n　实验调整了两个连续窗口之间的距离，即参数δ，因此数据集中窗口的总数取决于δ的值，δ=1s时，窗口总数为1800，δ=0.05s时，窗口总数为36000。对于每对(T，δ)都会生成一个新的数据集，用于训练和测试算法。\n表1 数据集流量参数:\r\r 机器学习算法性能评估 　对于所有机器学习算法，采用k倍交叉验证(k=5)，计算平均分类精度，并进行超参数选择(SVM中的kernel、RF中的树数、KNN中的邻居数)。\n　评估指标：\n 分类精度 算法复杂度：训练持续时间和预测时间  窗口距离δ的影响 　固定T=1s，逐渐增加δ值，观察分类精度（a）和算法复杂度（b、c）\n图4 T不变，分类精度和算法复杂度结果:\r\r  (a)：分类精度上，RF分类器 \u0026gt; SVM分类器 \u0026gt; KNN分类器；对于所有分类器，随着δ值增加，分类精度降低，这是因为数据集较小。 (b)：在训练和测试总时间（交叉验证时间）上，它随δ的增加而减少，其中RF的降幅最小，并且交叉验证时间最短。 (c)：测试时间上，SVM在δ≥0.1时测试时间最短，而δ值不显著影响RF和KNN的测试时间。  　综合来看，随机森林分类器（RF）和支持向量机（SVM）都是DDoS攻击检测的备选算法，RF的精度更高，SVM的算法复杂度更低。\n窗口持续时间T的影响 　固定δ=0.2s，逐渐增加T值。\n　分类精度上，RF分类器 \u0026gt; SVM分类器 \u0026gt; KNN分类器；对于所有分类器，随着T值增加，分类精度降低，是因为窗口持续时间相对于攻击来说过大，与常规背景流量相比，窗口中存在的攻击数据包数量较少，这使得识别攻击特征更加困难。\n图5 δ不变，分类精度:\r\r P4交换机中实现DDoS攻击检测 　假设已经部署了最合适的机器学习算法，评估在数据平面进行特征提取引入的延迟影响。考虑3种P4交换机向分类器提供流量信息的情况：\n a：发送数据包镜像到分类器 b：发送数据包头部到分类器 c：从头部提取信息存入元数据，将元数据发送到分类器  确定三种延迟：\n t1：P4交换机处理数据包的时间，并将数据包发送到攻击检测模块的时间； t2：在攻击检测模块中提取窗口特征所需时间； t3：分类器分类时间。  　分类器选择RF和SVM算法，对两种算法进行模型选择，RF采用10种不同的树和基于Gini impurity的分割标准，SVM分类器采用基于Radial的函数内核。\n　在T=1s，δ=0.2s的参数下，使用在Linux Box上运行BMV2，在Intel Xeon CPU E5-2620 v2 @ 2.10GHz，RAM 16GB和10G以太网光接口来评估P4交换机引入的延迟，并使用Spirent N4U Traffic Generator和分析仪进行测量。并按表1所示注入流量配置文件。\n　得到的结果如表2。\n表2 不同情况下的耗时:\r\r 　P4交换机提取特征并处理数据包大概只需要110μs，而在外部特征提取模块中提取特征需要14-16s，在交换机上所需的额外时间可以忽略不计，而分类时间取决于所选用的机器学习算法。\n参考资料 Spirent N4U：https://www.spirent.com/products/testcenter-ethernet-ip-cloud-test\n","description":"利用P4和机器学习算法，以TCP Flood为例，基于网络进行DDoS攻击检测","id":10,"section":"posts","tags":["P4"],"title":"论文阅读：基于P4的机器学习辅助DDoS攻击检测","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9F%BA%E4%BA%8Ep4%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9ddos%E6%94%BB%E5%87%BB%E6%A3%80%E6%B5%8B/"},{"content":"　论文名称：P4DAD: Securing Duplicate Address Detection Using P4\n　会议信息：2020 IEEE International Conference on Communications (ICC). IEEE, 2020: 1-7.\n　成果：提出了P4DAD，是一种轻量级，可部署，健壮，安全的DAD机制。通过引入P4，P4DAD无需修改NDP或主机堆栈即可保护网络中的NDP消息。\n　前人工作不足：轻量、部署难易程度、健壮性、安全性不能兼顾。\n　本文工作：提出P4DAD的安全重复地址检测机制，并在bmv2上实现P4DAD原型，并从功能、性能、可扩展性三方面进行评估。\n一、背景 1.NDP(Neighbor Discovery Protocol) 　仅做简要介绍。\n 来源：RFC4861\n参考：https://cshihong.github.io/2018/01/29/IPv6%E9%82%BB%E5%B1%85%E5%8F%91%E7%8E%B0%E5%8D%8F%E8%AE%AE/\n NDP简介 　邻居发现协议，用于IPv6，负责在链路上发现其他节点和相应的IP地址，并确定可用路由和维护关于可用路径和其他活动节点的信息可达性。\n　和ARP的功能相似，在IPv6中通过NDP获得目标主机的链路层地址。NDP基于ICMPv6实现，以太网协议类型为0x86DD，即IPv6报文，下一个报头字段值为58，表示ICMPv6报文，NDP的所有报文都封装在ICMPv6报文中。\n　地址解析过程中使用了两种ICMPv6报文：\n 邻居请求报文NS（Neighbor Solicitation）：Type=135，Code=0，类似于ARP请求报文； 邻居通告报文NA（Neighbor Advertisement）：Type=136，Code=0，类似于ARP应答报文。  图1 NDP流程 :\r\r 　主机A向主机B发送报文之前必须知道B的链路层地址，所以主机A发送NS报文，主机A的链路层地址放在Options字段中；主机B收到NS报文以后，回应NA报文，主机B的链路层地址被放在Options字段中。\n重复地址检测(Duplicate Address Detect)  参考：https://zhuanlan.zhihu.com/p/110407399\n 　当节点获取到一个IPv6地址后，需要使用重复地址检测功能确定该地址是否已被其他节点使用（与IPv4的Gratuitous ARP功能相似）。通过NS和NA可以实现重复地址检测。\n Gratuitous ARP，无故（免费）ARP\n没有人问自己的情况下，无缘无故自问自答，是设备发送的一个发送端IP地址和目标IP地址都是本设备IP地址的ARP request/reply报文。\n如果ARP request收到了ARP reply，说明网络中存在和自己IP地址相同的设备。\n参考：https://zhiliao.h3c.com/Theme/details/27896\n 　在进行DAD检测时，一个IPv6单播地址在分配给一个接口之后且通过重复地址检测之前称为试验地址（Tentative Address）。此时该接口不能使用这个试验地址进行单播通信，但是仍然会加入两个组播组：ALL-NODES组播组和实验地址所对应的Solicited-Node组播组。\n　IPv6重复地址检测技术和IPv4中的Gratuitous ARP类似：节点向一个自己将使用的试验地址所在的Solicited-Node组播组发送一个以该实验地址为请求的目标地址的NS报文，如果收到节点回应的NA报文，就证明该地址已被网络上使用，节点将不能使用该实验地址通讯。\n图2 DAD示例 :\r\r 　PC1的IPv6地址2000::1为新配置地址，即2000::1为PC1的试验地址。PC1向2000::1的Solicited-Node组播组发送一个以2000::1为请求的目标地址的NS报文进行重复地址检测，由于2000::1并未正式指定，所以NS报文的源地址为未指定地址。当PC2收到该NS报文后，有两种处理方法：\n 如果PC2发现2000::1是自身的一个实验地址，则PC2放弃使用这个地址作为接口地址，并且不会发送NA报文。 如果PC2发现2000::1是一个已经正常使用的地址，那么PC2会向该地址的ALL-NODES组播组发送一个NA报文，该消息中会包含2000::1。这样，PC1收到这个消息后就会发现自身的实验地址是重复的，从而弃用该地址。  安全威胁  引用：https://www.secrss.com/articles/13813\n 　重复地址检测攻击：当目标节点向FF02 :: 16所有节点发送NS数据包进行重复地址检测时，攻击者可向该节点发送NA报文进行响应，并表明该地址已被自己使用。当节点接收到该地址已被占用消息后重新生成新的IPv6地址并再一次进行重复地址检测时，攻击者可继续进行NA响应实现DoS攻击。使目标节点无法配置任何地址。\n　泛洪攻击：攻击者可伪造不同网络前缀RA消息对FF02 :: 1进行泛洪攻击，接收节点将会根据不同的网络前缀进行更新，从而消耗大量的CPU资源。\n　2020年，将有超过200亿的物联网设备连接到互联网。这些物联网设备倾向于使用IPv6地址进行通信，并且它们仍需要使用重复地址检测机制来确定IPv6地址的唯一性。\n已有解决方案 　本文针对重复地址检测攻击的防御，在这个问题上基本有三种解决方案：\n 隐藏目标地址：发送NS数据包时将目标地址隐藏在NS消息中，NS消息不直接填写地址，而是填写地址的哈希值。其他主机计算自己地址的哈希值，与NS消息中的值进行比较。这需要对NDP进行修改。 验证NDP消息：将消息认证码附加到NDP消息。这需要对NDP进行修改。 统一答复NA消息：利用一个可靠的中央节点统一答复NA消息，其它节点发送的NA消息都将被忽略。它面临单点故障问题。  　表1总结了现有方案的局限性。\n表1 现有方案局限性:\r\r 二、本文的工作 　目标：增强重复地址检测的安全性，轻量（无哈希）易部署（不修改网络堆栈）且健壮（无中央节点）。\n　基于P4设计安全的重复地址检测机制。文章假设主机直接连接到交换机，主机可能是恶意的，交换机是可信任的。\n　解决两个挑战：每个主机通常配置多个IPv6地址；在本地配置和填充交换机的流规则。\n三、设计及实现 1 P4DAD设计 1）在解析器中提取NS/NA消息中的地址\n2）用有状态寄存器在数据平面中保留端口和IPv6地址的绑定，允许同一端口对应多个IPv6地址。\n3）P4DAD将所需信息的摘要以数据包形式传到控制平面，并通知控制面实时填充流规则。\nP4DAD的总体设计如图3。\n图3 P4DAD总体设计:\r\r NS/NA数据包经过的动作匹配表如表2。\n表2 NS/NA消息经过的功能:\r\r 2 P4程序实现 　解析器解析数据包头部字段。\n　ipv6_reg：记录主机IPv6地址与交换机端口的映射关系。使用2个寄存器，寄存器1的槽中存储IPv6地址，寄存器2中存储IPv6地址状态（暂定tentative，首选preferred，不推荐使用deprecated）\n　寄存器索引计算：索引号 = 主机IPv6地址个数 × 交换机端口号 + 该端口已保存的IPv6地址数量。如图4。\n图4 P4DAD寄存器设计:\r\r 　Match-Aciton表包括MAC-learning表、Target-Address-Query表、Forwarding表\n　Mac-learning表：使用packet digest，将摘要发送到控制平面实现MAC学习。\n　Target-Address-Query表：\n 如果数据包是NS包，且Target Address不在表中，则将其IPv6值和状态存入寄存器1和寄存器2中(default action)，并通知控制平面为表添加条目； 如果数据包是NA包，并且Target Address不在表中，说明发生了欺骗，丢弃。 如果数据包是NA包，并且Target Address在表中，则获取其索引(action)，根据索引从寄存器1和寄存器2中获取IPv6地址及其状态；  如果IPv6地址的状态为tentative，则P4DAD会重置对应的寄存器1和寄存器2。    3 P4DAD工作流程 　NS/NA消息通过匹配Target-Address-Query表防御重复地址检测DoS攻击。\n　主机1和主机2（恶意主机）分别配置了IPv6地址，因此IPv6 1和IPv6 2的绑定条目已在P4交换机1中建立。主机3和主机4正在进行DAD进程。\n　主机3尝试配置与主机1相同的IPv6地址，主机4配置新地址。\n　当P4交换机2接收到主机3和主机4发送的NS数据包时，将建立IPv6 1和IPv6 3的绑定条目，并将相应的IPv6地址状态设置为tentative。\n　主机1将回复以IPv6 1为源地址和目标地址的NA数据包。但恶意主机2将分别发送两个以IPv6 1和IPv6 3为Target Address的欺骗性NA数据包。当P4交换机1接收到欺骗性的NA数据包时，将对它们进行直接过滤，因为Target-Address-Query表中没有该Target Address的表项，即连接该交换机的主机没有此IPv6地址。这意味着可以有效地防止DAD受到DoS攻击。\n　当P4交换机2收到主机1发送的NA报文时，将清除IPv6 1的绑定表项。当P4交换机2在特定时间未收到有关IPv6 3的NA数据包时，相应的IPv6地址状态将设置为preferred。\n图5 P4DAD工作流程:\r\r 4 原型实现 　P4DAD原型由数据平面程序和控制平面程序组成。\n 数据平面程序由P4_16实现，可以直接在bmv2上编译。 控制平面程序是用Python编写的，它通过notifications-addr与数据平面通信。  四、评估 　实验在Intel®Core™i7-7700 CPU和7840 MB RAM的Dell OptiPlex 7050服务器上进行。\n功能测试 　将安装有parasite6工具的两个虚拟机（受害者和攻击者）连接到由bmv2实现的P4软件交换机。\n　攻击者随机发送欺骗性NS/NA消息，而受害者随机发送标准NS/NA消息。\n　计算由攻击者和受害者分别发送的数据包数量以及由P4DAD接收和过滤的数据包数量， 结果发现P4DAD过滤的数据包数量等于攻击者发送的数据包数量，这意味着P4DAD可以有效识别和过滤欺骗的NS/NA消息。\n性能测试 　选择原始NDP的性能（不进行任何更改）作为基准，并比较P4DAD和基准之间NS/NA消息的处理时间。\n　NS-For-DAD和NS-For-Others分别表示为用于处理DAD和其他NDP功能的NS消息，如图6所示。\n　NS-for-DAD和NA-for-DAD的P4DAD的处理时间大于基线，而NS-for-Others和NA-for-Others的P4DAD的处理时间非常接近基线，原因是NS-for-DAD和NA-for-DAD的操作比基线多三到四次。\n图6 交换机处理NS/NA消息的时间:\r\r 　文章测量从数据平面摘要数据包信息的开始到控制平面填充流规则结束的时间，平均时间为4.126毫秒。\n通常，DAD仅出现在地址配置过程中，而控制平面和数据平面之间的通信仅需要在DAD的过程中进行。因此，P4DAD带来的处理开销可以忽略不计。\n可扩展性测试 　增加主机数量时，测量P4交换机的内存消耗以评估P4DAD的可伸缩性。\n　P4DAD的内存消耗随主机数量线性增加，表明P4DAD具有令人满意的可扩展性。\n","description":"提出了P4DAD，是一种轻量级，可部署，健壮，安全的DAD机制。通过引入P4，P4DAD无需修改NDP或主机堆栈即可保护网络中的NDP消息。","id":11,"section":"posts","tags":["P4"],"title":"论文阅读：基于P4的安全重复地址检测","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9F%BA%E4%BA%8Ep4%E7%9A%84%E5%AE%89%E5%85%A8%E9%87%8D%E5%A4%8D%E5%9C%B0%E5%9D%80%E6%A3%80%E6%B5%8B/"},{"content":"　论文名称：Defeating Protocol Abuse with P4: Application to Explicit Congestion Notification\n　会议信息：2020 IFIP Networking Conference (Networking)\n　成果：提出进行网络验证的EFSM通用抽象，实现安全监视功能，以ECN为例在P4交换机中实现该抽象。\n　前人工作不足：P4CEP通过使用常规的有限状态机描述复杂的事件处理功能来简化P4的使用，可伸缩性不足，可能面临状态空间爆炸的危险。\n　本文工作：提出EFSM抽象，监视任何协议行为并对之做出反应；并将将EFSM模型映射到P4，以ECN为例对模型进行验证，对效果进行评估，建议部分部署EFSM。\n一、背景 1.ECN(Explicit Congestion Notification)  来源：RFC3168\n参考：https://blog.csdn.net/z1143709608/article/details/52682150\n 　在网络中的路由器的转发队列通常实现了Random Early Detection (RED)功能，即，路由器会根据当前转发队列的平均长度来做丢包决策，并且随机的丢弃一些TCP流量的报文，而不是等待队列溢出后丢弃全部的报文，这样能够很好的避免所有TCP同时超时的问题。由于按照队列的平均长度来进行丢包，而不是队列满长，所以会引起一部分TCP的退避，让一部分TCP先放缓，保证另一些TCP的通畅。并且使用随机丢弃，针对所有TCP连接来说是相对公平的。\n　在RFC3168中定义了ECN的设计目标，是通过TCP发送端和接收端以及中间路由器的配合，感知中间路径的拥塞，并主动的减缓TCP的发送速率，从而从早期避免拥塞而导致的丢包，实现网络性能的最大利用。能够解决的问题如下：\n 所有TCP发送端能够早期感知中间路径拥塞，并主动放缓发送速率，预防拥塞发生。 在中间路由器上转发的队列上，对于超过平均队列长度的TCP报文进行ECN标记，并继续进行转发，不再丢弃报文。避免了报文的丢弃和TCP的重传。 由于减少了丢包，TCP不需要经过几秒货几十秒的重传定时器出发报文重传，提高了时延敏感应用的用户感受。 与没有部署ECN功能的网络相比，网络的利用率更好，不再在过载和轻载之前来回震荡。  1.1 报文格式 IP头部的修改 　IP头部的TOS字段中的第7bit和8bit的res字段被重新定义为ECN字段，其中有四个取值\n 00代表该报文并不支持ECN，所以路由器的将该报文按照原始非ECN报文处理即可，即，过载丢包。 01和10这两个值针对路由器来说是一样的，都表明该报文支持ECN功能。01和10的具体区别请参考RFC3168。 如果发生拥塞，则ECN字段的这两个将修改为11来表示报文经过了拥塞，并继续被路由器转发。  　所以路由器转发侧要支持ECN，需要有以下新增功能：\n 当拥塞发生时，针对ECN=00的报文，走原有普通非ECN流程，进行RED丢包。 当拥塞发生时，针对ECN=01(ECT=1)或ECN=10(ECT=0)的报文，都需要修改为ECN=11，并继续转发流程。 当拥塞发生时，针对ECN=11(CE)的报文，需要继续转发。 为了保证与不支持ECN报文的公平性，在队列超过一定长度时，需要考虑对支持ECN报文的丢弃。  TCP头部的修改 　在主机端，将TCP头部的6位保留字段的后两位设置为CWR和ECE(即URG的前两位)。\n CWR：拥塞窗口缩小 ECE：ECN回显  　在RFC3168中设计如下：\n 在TCP接收端收到的IP头中有ECN=11标记，则在回复ACK时将ECE bit置1。并在后续的ACK总将ECE bit置1。 在TCP发送端收到ECE bit置1的ACK报文时，需要将自己的发送速率减半，并在发送下一个报文时，将CWR bit置1。 在接收端收到CWR bit置1的报文时，后续的ECE bit将不再置1。直到再次收到IP头部ECN=11时，重复上述过程。 TCP发送端在收到一个ECE=1时，缩小发送窗口，并且在本次RTT时间内将不再缩小发送窗口。 TCP接收端向发送端回应ACK时，如果该ACK是一个不带数据的“纯”ACK，那么IP头部必须设置ECN=00，因为TCP没有机制对纯ACK进行响应，就无法针对纯ACK发送拥塞通知。即握手阶段ECN=00。 对于支持IP ECN的主机，TCP层在发送报文时需要将IP首部中的ECN置为01或10。  安全问题  参考：TCP报文到达确认机制：http://blog.csdn.net/wjtxt/article/details/6606022\n　TCP数据包中的序列号（Sequence Number）不是以报文段来进行编号的，而是将连接生存周期内传输的所有数据当作一个字节流，序列号就是整个字节流中每个字节的编号。一个TCP数据包中包含多个字节流的数据（即数据段），而且每个TCP数据包中的数据大小不一定相同。在建立TCP连接的三次握手 过程中，通信双方各自已确定了初始的序号x和y，TCP每次传送的报文段中的序号字段值表示所要传送本报文中的第一个字节的序号。\n　TCP的报文到达确认（ACK），是对接收到的数据的最高序列号的确认，并向发送端返回一个下次接收时期望的TCP数据包的序列号（Ack Number）。例如， 主机A发送的当前数据序号是400，数据长度是100，则接收端收到后会返回一个确认号是501的确认号给主机A。\n　TCP提供的确认机制，可以在通信过程中可以不对每一个TCP数据包发出单独的确认包（Delayed ACK机制），而是在传送数据时，顺便把确认信息传出， 这样可以大大提高网络的利用率和传输效率。同时，TCP的确认机制，也可以一次确认多个数据报，例如，接收方收到了201，301，401的数据报，则只 需要对401的数据包进行确认即可，对401的数据包的确认也意味着401之前的所有数据包都已经确认，这样也可以提高系统的效率。\n　若发送方在规定时间内没有收到接收方的确认信息，就要将未被确认的数据包重新发送。接收方如果收到一个有差错的报文，则丢弃此报文，并不向发送方发送确认信息。因此，TCP报文的重传机制是由设置的超时定时器来决定的，在定时的时间内没有收到确认信息，则进行重传。这个定时的时间值的设定非常重要，太大会使包重传的延时比较大，太小则可能没有来得及收到对方的确认包发送方就再次重传，会使网络陷入无休止的重传过程中。接收方如果收到了重复的报文，将会丢弃重复的报文，但是必须发回确认信息，否则对方会再次发送。\n  终端主机宣布自己具有ECN功能但忽略来自交换机的拥塞通知，即收到ECN=11时，仍将ECE置0，导致发送方不减少拥塞窗口，造成拥塞； 在发送方已经减少拥塞窗口后，接收者仍然继续发送设置了ECE=1的数据包，导致拥塞窗口加倍减小，浪费带宽； 发送方在收到ECE=1之后，不将CWR置1，即不缩小拥塞窗口，造成拥塞。  　上述这三种不当行为都会严重降低网络性能。图1中箭头上蓝色字表示正确的ECN行为，红色的字表示错误的ECN行为。\n图1 ECN流程 :\r\r 二、本文的工作 　目标：实时监视网络流量识别协议滥用。\n　将协议规范转为扩展有限状态机(EFSM)，并用P4程序实现EFSM模型，编译P4程序并安装在PISA交换机上，通过维护每个连接的当前状态以及EFSM的所有相关变量来在线跟踪所有流。当EFSM模型进入标记为不当行为的状态时，程序可以触发不同的动作，如丢弃数据包、生成警报等。\n　以ECN为例，对此方法进行评估。\n图2 整体设计 :\r\r 三、实现 1 模型设计 　文章使用7元组(S, E, A, I, V, C, T)表示EFSM，各元含义如图3。\n图3 EFSM7元组:\r\r 　ECN的EFSM7元组对应实例如图4。\n图4 ECN的EFSM7元组:\r\r 　完整的ECN EFSM图模型如图5，然后使用P4交换机的Extern组件和P4程序实现这个模型。\n图5 ECN EFSM:\r\r 2 P4程序实现 2.1 Externs 　寄存器存储流状态：用寄存器存储连接状态，对五元组\u0026lt;dstIp、srcIp、dstPort、srcPort、协议\u0026gt;做HASH运算，得到该流五元组对应的寄存器索引，对于TCP连接的两个端点，如果dspIp \u0026gt; srcIp，按照\u0026lt;dstIp、srcIp、dstPort、srcPort、协议\u0026gt;的顺序做HASH运算，否则按照\u0026lt; srcIp、 dstIp、 srcPort、 dstPort、srcPort、协议\u0026gt;的顺序做HASH运算。一共6个状态，对应寄存器槽内的值是对应的状态，初始时为init状态。\n2.2 元数据  congestion：来自交换队列的占用级别 头部字段：标志位、校验和  2.3 匹配-动作表 　文章没有具体描述其设计，这里给出一个我理解的Naive 版本，如果仔细构思一下可能有更节省空间、更优雅的实现方式。\n   Key Action     流状态为init, ECT=1, congestion=1 CE=1, forward, 流状态改为congestion   流状态为congestion, ECE = 1 forward, 流状态改为notified   流状态为congestion, ECE = 0 ECE=1, update_chk, forward, 流状态改为notified(修正了misbehaving状态)   流状态为notified, CWR=1 forward, 流状态改为win_reduction   流状态为notified, CWR=0 drop, 流状态改为misbehaving   流状态为win_reduction, ECE=0 forward, 流状态改为notification_stopped   流状态为win_reduction, ECE=1 ECE=0, update_chk, forward, 流状态改为notification_stopped    2.4 动作  update_chk：使用增量过程计算数据包校验和，无需重新计算校验和。  TCP新校验和=旧校验和-TCP值字段的16位旧值的补码-16位字段的新值   forward drop CE置1 ECE置0  四、评估 　使用mininet构建网络，并使用P4-Bmv2交换机进行实验。构建的网络拓扑如图6。所有主机都具有ECN功能，并且链路延迟设置为1ms，限制了输出队列速率（默认为2000个数据包/秒）。根据先前工作，10 Gbits / s网络的情况下，65个数据包在交换机中排队后就发生了拥塞，对于mininet，观察到的最大吞吐量约为40 Mbits/s，因此队列中超过3个数据包就被视为拥塞，将触发事件congestion，从而离开EFSM的init状态。\n图6 网络拓扑图:\r\r  h1：不回显拥塞信息，即不设置TCP ECE标志，使h1成为异常主机； h1-\u0026gt;h2：异常流，在h1上使用iperf生成TCP流发送给h2。 h3-\u0026gt;h4：正常流，h3上使用iperf生成TCP流发送给h4。  　结果如图7， 其中noMisbehaving-forward表示h1-\u0026gt;h2，h3-\u0026gt;h4都是正常流；Misbehaving-noReaction表示h1行为异常，但交换机不做处理；Misbehaving-ECEreaction表示只实现了ECE纠正，未实现其它纠正；Misbehaving-fullReaction完全实现了图5的EFSM状态机。\n图7 ECN行为对带宽的影响:\r\r 　对于Misbehaving-noReaction，可以看到异常流h1-\u0026gt;h2消耗了大部分可用带宽，流h3-\u0026gt;h4受到了损失。实际上，正常流（h3-\u0026gt;h4）响应拥塞信号并降低其传输速率，从而为行为异常的流留出更多的队列空间，从而获得了更多的带宽份额。但是，有效吞吐量仍然受到发送方容量的限制，因此行为异常流的增加低于响应流吞吐量的损失。\n　对于Misbehaving-ECEreaction，重新正常的分配带宽，正常流的损失降低，但整体吞吐量下降。\n　对于Misbehaving-fullReaction，吞吐量严重下降，甚至不如Misbehaving-noReaction。\n　根据上述实验结果，建议部分部署EFSM。\n　在部署EFSM之前，必须先验证是否适合网络条件；EFSM越复杂，开销越高。（EFSM虽然有效但不是特别好）\n","description":"建立ECN滥用防御的EFSM(有限状态机)模型，并将其映射到P4程序上，实现数据平面的ECN滥用防御","id":12,"section":"posts","tags":["P4"],"title":"论文阅读：基于P4的ECN滥用防御","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9F%BA%E4%BA%8Ep4%E7%9A%84ecn%E5%8D%8F%E8%AE%AE%E6%BB%A5%E7%94%A8%E9%98%B2%E5%BE%A1/"},{"content":"1 概述 　S3: Simple Storage Service 是对象存储服务，具体来说是可以通过一个URL（如http://endpoint_url/\u0026lt;bucket_name\u0026gt;/\u0026lt;key\u0026gt;）获取存储对象，服务对外提供HTTP接口，可以通过PUT请求上传对象，GET请求获取对象等。\n　应用场景（大数据分析）：内容存储和分发；用于数据分析的存储；备份、存档与灾难恢复；静态网站托管。\n　从任意位置存储和检索任意数量的数据而构建的对象存储服务，是面向对象的文件系统，适合静态文件一次写入，不适合经常修改的文件。\n　S3中的概念Bucket 、Object 、Endpoint 、AccessKey 和SecretKey ：\n Bucket：存放Object的容器，不能嵌套。 Object：对象，包含key和value。 Endpoint：对外服务的访问域名。 AccessKey：ASCII字符串，用于标识客户身份。 SecretKey：ASC字符串，作为私钥存储在客户服务器，不在网络中传递；使用AccessKey进行身份识别，用SecretKey进行数字签名，从而完成应用接入与认证授权；用于访问桶、获取桶所在的数据中心。  2 对象存储 　扁平化的命名空间：将数据以对象（Object）形式存储在桶（Bucket）中，通过桶名称和对象的键名（Bucket_name + Key_name）来定位一个对象的最终路径。\n　对象（Object）：由键名（Key）、键值（Value）、访问控制列表（ACL）、元数据（Metadata）组成。\n图1 S3对象:\r\r 3 Amazon S3的使用 3.1 注册 　(1)普通免费一年的Amazon账号需要绑定银行卡，可能会产生奇怪的费用，所以我们使用学生账号，从https://www.awseducate.com/注册，注册后点击登录。\n图2 登录:\r\r 　(2)右上导航栏\u0026quot;AWS Acount\u0026quot;，点击“AWS Educate Starter Account”\n图3 进入学生账号:\r\r 　(3)进入下图页面后，点击AWS Console，即可进入AWS控制台\n图4 进入AWS控制台:\r\r 　(4)在控制台界面搜索S3，进入S3控制台\n图5 进入S3控制台:\r\r 　(5)创建一个Bucket用于测试\n图6 创建一个桶:\r\r 3.2 获取AcessKey和SecretKey 　在客户端使用S3时，需要配置认证Key，即AccessKey和SecretKey，才能和S3建立连接，普通用户在控制台主页搜索IAM，按照网上的解决方案生成AccessKey即可。教育账号需要在下图这个界面点击“Account Details”\n图7 进入AWS控制台:\r\r 　在弹出的子窗口中点击“show”，则能够获得AccessKey和SecretKey，其有效期是三小时\n图8 获取AccessKey:\r\r 3.3 使用python操作S3 　boto3是S3官方建议使用的python包，安装：\n1  $ pip install boto3   　连接到s3：使用上面2中获得的AccessKey和SecretKey就可以，不报错就连上了，注意一定要有session_token，否则会查找不到AccessKey。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #!/usr/bin/python #encoding:utf-8  import boto3 from boto3.session import Session session = Session(aws_access_key_id=\u0026#39;ASIASR3FIDV6NEAZGO3V\u0026#39;, aws_secret_access_key=\u0026#39;1WuKe42HfTslTtMuH885D442t1OWVo/vg59Uk/N3\u0026#39;, aws_session_token=\u0026#39;\u0026lt;太长了，略过不写\u0026gt;\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39;) s3 = session.resource(\u0026#39;s3\u0026#39;) client = session.client(\u0026#39;s3\u0026#39;) #上传 data = open(\u0026#39;pcapng-demo.pcap.pcapng\u0026#39;, \u0026#39;rb\u0026#39;) file_obj = s3.Bucket(\u0026#39;mesalybtest\u0026#39;).put_object(Key=\u0026#39;pcapng-demo.pcap.pcapng\u0026#39;, Body=data) #获取URL presigned_url = client.generate_presigned_url( \u0026#39;get_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: file_obj.bucket_name, \u0026#39;Key\u0026#39;: file_obj.key}, ExpiresIn= 3600*30*12 ) print(presigned_url)   4 参考资料 1.一文读懂AWS S3：https://zhuanlan.zhihu.com/p/112057573\n2.对象存储概念：https://cloud.tencent.com/developer/article/1073409\n3.AWS S3开发指南：https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/VirtualHosting.html#path-style-access\n4.KS3对象存储概念：https://docs.ksyun.com/documents/2286\n5.file_transfer：https://docs.aws.amazon.com/code-samples/latest/catalog/python-s3-file_transfer-demo_file_transfer.py.html\n6.boto3官方使用：https://github.com/boto/boto3\n7.使用Python操作Amazon S3：https://xingzuoshe.cn/python-operate-s3.html\nbin/kafka-console-consumer.sh \u0026ndash;bootstrap-server localhost:9092 \u0026ndash;topic test \u0026ndash;from-beginning\n","description":"S3服务的使用","id":13,"section":"posts","tags":["分布式"],"title":"工具使用：S3","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8s3/"},{"content":"　论文名称：Distributed SIP DDoS Defense with P4\n　会议信息：2019 IEEE Wireless Communications and Networking Conference (WCNC)\n　成果：利用可编程交换机缓解SIP INVITE DDoS攻击\n　贡献：通过对以太网交换机的数据平面编程（P4）和控制平面编程进行实验，为每个交换端口的SIP INVITE DDoS攻击提供第一跳检测和缓解功能。\n　前人工作不足：基于目标的方法需要网络设备上大量CPU和内存资源；缺少第一跳检测和缓解，检测和识别SIP事务的传感器太少；缓解点数量有限。\n　本文工作：通过使用以太网交换机的数据平面编程进行SIP DDoS检测，对以太网交换机上每个端口的第一跳SIP DDoS攻击检测和缓解功能以及分布式SIP在网络边缘使用以太网交换机的DDoS防御方法。\n一、背景 1.SIP Flood攻击  来源：https://forum.huawei.com/enterprise/zh/thread-298913.html\n 　SIP(Session Initiation Protocol)是一个应用层的信令控制协议。用于创建、修改和释放一个或多个参与者的会话。这些会话可以是Internet多媒体会议、IP电话或多媒体分发。例如，SIP服务提供商可以建立包含语音、视频和聊天内容的全新媒体。\n1.1 SIP协议流程简介  参考：https://blog.csdn.net/c80486/article/details/43391961\n参考：https://www.w3cschool.cn/session_initiation_protocol/session_initiation_protocol_messaging.html\n 　SIP协议工作的流程和人打电话的过程是一样的，所以也称为软电话，不同之处不是通过讲话而是通过IP数据包来传递信息。\n　SIP默认端口为5060，默认采取UDP传输。\n　SIP账号：采用URI表示方法，即sip:username@ip:port，其中ip也可以填写服务器所属域名。\n　SIP消息：用来传递通话信号的IP数据包。包括6中核心消息：INVITE、BYE、REGISTER、CANCEL、ACK、OPTIONS。\n　SIP通话过程如图1。\n图1 SIP通话过程 :\r\r  Alice向Bob发送一个请求SIP消息INVITE，邀请Bob参与通话 Bob振铃，向Alice回复响应180 Ringing，通知Alice自己正在振铃，请Alice等待 Bob接起软电话，向Alice发送响应200 OK，通知Alice可以通话了 Alice向Bob发送请求SIP消息ACK，通话正式建立 然后开始通话，通过RTP或TCP Bob挂掉软电话，向Alice发送请求SIP消息BYE，通知Alice通话结束 Alice向B回复一个200 OK，通话结束。  1.2 SIP Flood的源认证防御原理  来源：https://blog.csdn.net/c80486/article/details/43391961\n 　攻击者通过发送大量的INVITE消息或REGISTER消息到SIP服务器，导致被攻击SIP服务器分配大量的资源用以记录和跟踪会话，直到资源耗尽而无法响应合法用户的呼叫请求；或者针对VoIP设备在SIP协议实现上的漏洞构造并发送相应的畸形SIP报文，从而导致SIP服务器拒绝服务。\n　OPTIONS方法用于SIP协议通信双方查询对方支持的方法、内容类型、扩展等。SIP源认证是发送OPTIONS请求报文作为探测报文探测源的真实性，如果发送端的源IP真实存在，就会对该探测报文进行回应，Anti-DDoS设备检查回应报文是否正确，正确则允许通过，将源IP地址加入白名单，否则丢弃报文。具体处理过程如图2所示。\n图2 SIP源认证 :\r\r 二、本文的工作 　P4控制器通过TCP套接字连接控制P4交换机。\n　交换机上的每个端口都可以对INVITE或REGISTER执行数据包检查，每个端口都有一个计数器，对收到的INVITE或REGISTER的数量进行计数。\n　P4控制器每秒都对这个计数器进行评估，确定是否发生了SIP INVITE Flood攻击，当确定发生了攻击时，它向P4交换机发送一条命令，以从该特定端口丢弃后续的INVITE或REGISTER数据包。当攻击停止1分钟后，P4控制器向P4交换机发送命令，从而恢复到正常处理模式。\n图3 系统设计 :\r\r 三、实现 1.1 P4控制器：Python脚本 　初始化：初始化表项，在sipinvite_table表中添加\u0026lt;入口端口 SIP数据包第一行\u0026gt;的表项，并设置默认操作为_nop（即无操作），例：table_add sipinvite_table _nop 2 0x494e56495445 =\u0026gt;\n　评估sipinvite计数器：计数器每秒检查每个交换机端口上的sipinvite_counter来实现攻击检测。例：counter_read sipinvite_counter 2，检查端口2接收到的SIP INVITE数据包的数量。\n　通过或放弃：当每秒超过10个SIP INVITE数据包时，将检测到SIP DOS攻击，为该端口激活DOS模式，即向P4交换机发送命令将表sipinvite_table的默认动作改为drop。当一分钟内SIP INVITE数据包的个数都低于阈值时，将端口设置为正常模式，即向P4交换机发送命令将表sipinvite_table的默认动作改为_nop。攻击停止后，允许合法用户在接下来的60秒内发起呼叫。\n图4 sipinvite_table表:\r\r 1.2 P4程序  定义以太网帧、IPv4、UDP、SIP数据包，在解析器中进行解析； 设置ipv4_lpm、forward、sipinvite_table表，前两个用于常规IP数据包转发，第三个表用于查找SIP INVITE，key是ingress_port和SIP INVITE。 定义了一个计数器来记录每个端口接收到的SIP INVITE数据包的数量，连接到sipinvite_table。  四、评估 　使用具有2GB RAM和2个CPU的Ubuntu虚拟机来运行mininet，在Ubuntu虚拟机上创建3个虚拟主机和3个虚拟交换机拓扑。\n SIP代理（10.0.0.1）连接到交换机1的端口1 攻击者（10.0.0.2）连接到交换机2的端口2 合法用户（10.0.0.3）连接到交换机- 3端口3  　控制器是python脚本，在主机上运行，并通过套接字连接与三个交换机通信（交换机1的端口TCP/22221，交换机2的TCP/22222和交换机3的TCP/ 22223）。\n　使用bmv2 P4兼容虚拟交换机，而不是mininet附带的默认虚拟交换机；用p4c-bm的P4配置生成器用于将P4代码转换为bmv2所需的JSON配置文件。\n　SIPp是用于生成SIP流量的SIP数据包生成工具。对于SIP代理，它设置为在用户代理服务器（UAS）模式下运行，而对于SIP客户端，它在用户代理客户端（UAC）模式下运行。\n　数据包捕获（pcap）实用工具tshark用于验证SIP代理（在交换机1：端口1）和SIP客户端（在交换机2：端口2）发送和接收的数据包。\n图5 测试环境:\r\r 　评估主要分为三个方面：检测和缓解能力；检测和缓解时间；系统资源消耗\n　评估跟踪变量：发送和接收的SIP INVITE数据包数；发送和接收的ICMP数据包数；CPU和内存消耗\n　评估结果表明：\n P4交换机仅丢弃SIP INVITE数据包，ICMP数据包不受影响，如果计算机感染了僵尸网络/恶意软件，则在阻止由恶意软件生成的SIP INVITE数据包的同时，用户仍然可以使用其他应用程序。 开始阻止SIP INVITE数据包所花费的时间为1.47秒。 在攻击期间，平均CPU利用率为24.49％（峰值为71％）。内存消耗最高达到3％。  ","description":"利用可编程交换机和P4语言实现基于源的SIP Flood攻击防御和缓解","id":14,"section":"posts","tags":["P4"],"title":"论文阅读：基于P4的SIP DDoS攻击防御","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9F%BA%E4%BA%8Ep4%E7%9A%84sip-ddos%E6%94%BB%E5%87%BB%E9%98%B2%E5%BE%A1/"},{"content":"1 概述 　Kafka是消息系统。\n  消息系统：在应用程序之间发送消息\n  应用场景：日志收集系统和消息系统，采用发布-订阅模式\n  常用的Message Queue：RabbitMQ(重量级)，Redis，ZeroMQ，ActiveMQ，Kafka\n  　kafka工作示意图：\n图1 kafka工作示意图:\r\r 　topic：消息类别\n　partition：topic中的数据分割成partition，partition中的数据由多个segment文件存储，partition中的数据（消息）是有序的；同一个Partition的Replica尽量分散到不同的机器上。\npartition中的0，1，2，3，是offset，也就是消息位置，由Consumer决定，Consumer消费完一条消息，会递增offset。\n　broker：存储partition的服务器节点，一个broker存储topic的一个partition；broker接收到生产者发送的消息后，将该消息追加到当前用于追加数据的segment文件中。\n　Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。（Push模式：主动发送内容更新；Pull模式：询问是否有更新）\n　Producer发送消息到broker，会根据Partition机制选择将其存储到哪个Partition，如果Partition机制设置合理，所有消息可以均匀分布到不同Partition中。在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断将这条消息发送到哪个Partition。\n　同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。\n　广播：每个Consumer有一个独立的Group，单播：所有的Consumer在同一个Group里。\n　对于kafka而言，pull模式更合适，使Consumer自主控制消费消息的速率。\n　kafka默认保证 At least one，即消息绝不会丢，但可能会重复：体现在设置参数为auto_offset_reset='earliest'，如果Consumer Group中的一个Consumer挂了，剩余的Consumer会重新从头消费消息。\n　Producer和Consumer只与Leader交互，其他Replica作为Follower从Leader中复制数据。\n2 安装  安装方法：https://blog.csdn.net/qq_42881421/article/details/86741617\n软链接用法：https://blog.csdn.net/Leonis_v/article/details/52353706\n 安装环境：ubuntu16.0.4\n　(1)下载地址：https://kafka.apache.org/downloads\n　如：kafka_2.12-2.7.0.tgz，其中2.12是Scala版本号，2.7.0是kafka版本号。\n　(2)解压安装包\n1 2  $ cd soft/ $ tar -zxf kafka_2.11-0.10.2.0.tgz   　(3)配置环境变量\n1  $ nano ~/.bashrc   　添加下列语句：\n1 2  export KAFKA_HOME=~/soft/kafka export PATH=$PATH:$KAFKA_HOME/bin   　(4)保存后，让环境变量立即生效\n1  $ source ~/.bashrc   3 简单使用 3.1 使用官方提供的Shell脚本操作kafka 　在kafka的安装目录下：\n　启动Zookeeper：bin/zookeeper-server-start.sh config/zookeeper.properties\n1  bin/zookeeper-server-start.sh config/zookeeper.properties   　启动kafka：bin/kafka-server-start.sh config/server.properties\n1  bin/kafka-server-start.sh config/server.properties   　创建topic：bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor \u0026lt;\u0026gt; --partitions \u0026lt;partition_number\u0026gt; --topic \u0026lt;topic_name\u0026gt;\n1  bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test   　启动生产者：bin/kafka-console-producer.sh --broker-list \u0026lt;produce_servers\u0026gt; --topic \u0026lt;topic_name\u0026gt;\n1  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test   　启动消费者：bin/kafka-console-consumer.sh --bootstrap-server \u0026lt;consume_servers\u0026gt; --topic \u0026lt;topic_name\u0026gt;--from-beginning\n1  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning   3.2 使用Python操作Kafka 消费者 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  #!/usr/bin/python #encoding:utf-8  from kafka import KafkaConsumer import json \u0026#39;\u0026#39;\u0026#39; 消费者 消费test主题中的数据 注意事项：如需以json格式读取数据需加上value_deserializer参数 \u0026#39;\u0026#39;\u0026#39; def consume_to_kafka(): consumer = KafkaConsumer(\u0026#39;test1\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;, bootstrap_servers=[\u0026#39;localhost:9092\u0026#39;], value_deserializer=lambda m: json.loads(m.decode(\u0026#39;ascii\u0026#39;)) ) for message in consumer: test_data = message.value url = test_data[\u0026#39;packet_url\u0026#39;] print(url) if __name__ == \u0026#34;__main__\u0026#34;: try: consume_to_kafka() except KeyboardInterrupt: print(\u0026#34;Quit.\u0026#34;)   生产者 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  #!/usr/bin/python #encoding:utf-8  from kafka import KafkaProducer import json def produce_one_item(topic, producer): pkt = { \u0026#34;id\u0026#34;: 1, \u0026#34;nicname\u0026#34;:\u0026#34;enp6s0\u0026#34;, \u0026#34;raw_packet\u0026#34;: \u0026#34;TOu9fJMVMKrkRBubCABFIAAoEh0AAHQGjPrYOtKuwKg8BwG75osRSr9s/dLbA1AQAQl1XgAAAAAAAAAA\u0026#34; } producer.send(topic, pkt) producer.close() def produce_more_items(count,topic,producer): i = 0 for i in range(count): pkt = { \u0026#34;id\u0026#34;: 1, \u0026#34;nicname\u0026#34;:\u0026#34;enp6s0\u0026#34;, \u0026#34;raw_packet\u0026#34;: \u0026#34;TOu9fJMVMKrkRBubCABFIAAoEh0AAHQGjPrYOtKuwKg8BwG75osRSr9s/dLbA1AQAQl1XgAAAAAAAAAA\u0026#34; } producer.send(topic, pkt) i = i + 1 #注意全发完了再close producer.close() if __name__ == \u0026#34;__main__\u0026#34;: producer = KafkaProducer( value_serializer=lambda v: json.dumps(v).encode(\u0026#39;utf-8\u0026#39;), bootstrap_servers=\u0026#34;localhost:9092\u0026#34; ) #produce_one_item(\u0026#39;test1\u0026#39;,producer) produce_more_items(2000,\u0026#39;test1\u0026#39;, producer)   3.3 kafka消费者分组与分区数（Partition）的关系  来源：https://blog.csdn.net/gdkyxy2013/article/details/86644919\n  消费者数 ＞ 分区数：多余的消费者空闲 消费者数 ＜ 分区数：多消费者对应多个分区 消费者数 = 分区数：一个消费者对应一个分区  参考资料 1.kafka学习之路一：https://www.cnblogs.com/qingyunzong/p/9004509.html\n2.kafka学习之路二：https://www.cnblogs.com/qingyunzong/archive/2004/01/13/9004593.html\n3.kafka学习之路三：https://www.cnblogs.com/qingyunzong/archive/2004/01/13/9004703.html\n4.kafka中文文档：https://kafka.apachecn.org/intro.html\n5.kafka安装：https://www.cnblogs.com/qingyunzong/archive/2004/01/13/9005062.html\n6.kafka-python的基本使用：https://zhuanlan.zhihu.com/p/38330574\n7.通过python操作kafka：https://developer.aliyun.com/article/584524\n","description":"Kafka的安装与使用，仅做简单的测试用，用法较为简单","id":15,"section":"posts","tags":["分布式"],"title":"工具使用：Kafka","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8kafka/"},{"content":"　论文名称：NetCache: Balancing Key-Value Stores with Fast In-Network Caching\n　会议信息：SOSP ’17, October 28, 2017, Shanghai, China\n　成果：利用ToR可编程交换机做缓存层，实现机架规模的数十亿吞吐量的读密集型键值存储系统\n　贡献：使用缓存解决负载不均衡问题，利用可编程交换机的线速处理，解决了缓存和存储层之间没有明显速度差别的问题；利用了可编程交换机的特性，是可编程交换机的一个应用实例，详细讲解了可编程交换机的使用方法和思路\n　前人工作不足：传统的键值存储当存储层也在内存时，缓存和存储层之间没有明显速度差别；为保证各个副本间的数据迁移，数据一致性和查询路由使得系统更加复杂并引入了开销\n　本文工作：利用ToR可编程交换机做键值存储系统的缓存层，设计NetCache数据包，在可编程交换机上实现路由模块、键值存储模块（入口的Lookup Table和出口的Value Table）、查询统计模块（Count-Min草图和Bloom Filter）；客户端实现查询API，服务端把NetCache数据包转换成查询；NetCache适用范围和不足的讨论；性能评估，主要与NoCache对比。\n　详细阅读可以参考演示页面下的NetCache，本文只简单地进行一下总结。\n一、背景 1.内存键值存储 　现代网络服务（如科研、社会网络和电子商务）高度依赖于高性能键值存储。系统运营商需要内存中的键值存储系统（Memcached、Redis）来满足必要的吞吐量和延迟需求。\n　现代网络服务的热点事件查询次数远超其它事件，并且热点事件的集合处于快速变动的状态中，这种偏斜的负载导致严重的系统性能下降。\n　缓存是均衡负载的有效方法。传统的键值存储（基于闪存、基于硬盘）使用快速内存缓存层均衡负载。而基于服务的缓存由于其存储层就在内存中，基于内存的缓存层并不生效。\n1.1 缓存大小 　根据论文Small Cache, Big Eect: Provable Load Balancing for Randomly Partitioned Cluster Services.得知，平衡N个存储节点的hash分区键值集群，只需要设置能够存储O(NlogN)个项目的缓存。\n1.2 缓存放置的位置 　客户端\u0026mdash;\u0026gt;网络\u0026mdash;\u0026gt;服务端\n 放在客户端上：难以保证缓存一致性；客户端访问热点时间的公共集合并不对每个客户端都是热点事件 放在服务端上：存储层本身就在内存中，基于内存的缓存层并不生效 放在网络中：由于硬件的发展，可编程交换机的芯片资源足够存储O(NlogN)项，并且它每秒能够处理数十亿数据包  　所以考虑将缓存层构建到网络(可编程交换机)中。\n二、本文的工作 1.系统设计 　系统设计见图1，系统设计主要有3个点，前两点较为巧妙，第三点比较普通，具体实现参考演示页面下的NetCache，这里仅概括核心思想，不再详细表述。\n图1 NetCache设计:\r\r 1.1 实现变长键值存储，并最小化开销  使用多个阶段把值连接起来构成大的值； 在查找表(Lookup Table)使用位图索引到ValueTable，动作数据只有位图和槽索引，节省了查找表(Lookup Table)的空间；读取给定寄存器的索引槽(idx)，Key是位图的一个位，节省了值表(Value Table)的空间； 用重复的动作定义（代码的增加）代替存储空间的增加，多个寄存器的相同索引的槽可以存储不同Key的Value，从而能够大限度地将所有寄存器的槽占用，充分利用了空间。  1.2 标记热点事件  使用Count-Min草图对Key的查询频率进行计数，不存储key，每次对key做Hash运算，映射到4个寄存器数组的4个索引上，4个索引中的值加1，4个值中最小的值作为查询频率，如果最小值超过了控制器配置的阈值，就把它标记为热点，发送给控制器。 使用Bloom Filter检测该Key是否已经上报给控制器。  1.3 缓存更新 　Heavy Hitter向控制器报告访问次数超过了阈值的键；\n　控制器从缓存中采样很少的Key，将他们的计数器与HH报告的进行比较，驱逐不流行的Key，插入更流行的Key\n 驱逐Key即将其各阶段寄存器的对应的索引槽中的值全清零； 插入Key是背包问题，采用最先适合算法分配寄存器的索引槽，然后从存储服务器获取这些Key的值。  2.流水线布局 　流水线布局如图2，具体实现参考演示页面下的NetCache。\n图2 NetCache流水线布局:\r\r 三、实现 　各部分实现的简洁图示参考演示页面下的NetCache。\n　交换机数据平面用P4编写，并用Barefoot Capilano软件套件编译到Barefoot Tofino ASIC上。缓存查找表的16字节键有64K条目。值表和寄存器数组分布在8个阶段。每个阶段提供64K 16字节的槽。这缓存的总大小是8MB，其值大小为16字节，最大为128字节。Count-Min草图包括4个寄存器组，每个有64K 16位的槽。布隆过滤器包括3个寄存器数组，每个有256K 1位的槽。Count-Min草图和布隆过滤器使用Tofino ASIC提供的哈希函数，执行键字段的位的任意异或运算。\n　控制器可以配置重置Count-Min草图和布隆过滤器的频率。文章在实验中每秒重置它们一次。总的来说，我们的数据平面实现使用的片上内存少于Tofino ASIC提供的50%，留下的空间足够用于传统网络处理。我们使用标准L3路由作为路由模块，基于目的IP地址转发数据包。\n　控制器使用Python编写。P4编译器通过交换机驱动在运行时为控制器生成Thrift API。控制器从数据平面使用这些API像4.3节那样来接收heavy hitter报告，获取计数器，并更新缓存的事项。\n　为优化IO性能，客户端库和服务器代理用Intel DPDK和C语言实现。客户端库提供键值接口，并将API调用转换为NetCache数据包。根据混合读写操作的Zipf版本，客户端可以在文章的服务器上用40G的网卡生成多达35MQPS的键值查询。\n四、涉及到的基础知识 　本文讲述的非常清晰，几乎不需要额外查找相关资料。\n","description":"利用高速缓存解决内存键值存储系统的负载不均衡问题，通过将缓存放置在可编程交换机上实现“高速”","id":16,"section":"posts","tags":["P4"],"title":"NetCache ：基于网络内缓存快速均衡键值存储负载","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBnetcache/"},{"content":"Some photos.\n","description":"随便拍拍，记录快乐","id":18,"section":"gallery","tags":null,"title":"随拍","uri":"https://yyyIce.github.io/zh/gallery/%E9%9A%8F%E6%8B%8D/"},{"content":"随便画画。\n","description":"随便画点什么","id":19,"section":"gallery","tags":null,"title":"随笔","uri":"https://yyyIce.github.io/zh/gallery/%E9%9A%8F%E7%AC%94/"},{"content":"　Mininet是一个网络模拟器，可以创建虚拟主机，交换机，控制器和连接的网络。\n　Mininet主机运行标准的Linux网络软件，支持OpenFlow。支持开发、学习、测试、调试，让使用者在笔记本电脑上就可以拥有完整的实验网络。在Mininet上为OpenFlow控制器，修改后的交换机或主机开发和测试的代码可以迁移到具有较少更改的实际系统中。通常Mininet中运行的设计可以直接移植到硬件交换机，以进行线速数据包转发。基于Mininet的网络不能超过当前CPU或在单个服务器上所获得的带宽。\n1.安装   mininet的官方页面：http://mininet.org/\n  mininet的github：https://github.com/mininet/mininet\n  1.1 虚拟机整机   下载地址：https://github.com/mininet/mininet/releases\n  amd64是64位的ubuntu；i386是32位的ubuntu。\n  用户名：mininet 密码：mininet\n  获取root权限：\n  1  sudo -i   1.2 在已有虚拟机上安装  安装参考：https://blog.csdn.net/AsNeverBefore/article/details/78916645\n (1)获取源码\n1  git clone git://github.com/mininet/mininet   (2)进入mininet目录，可通过查看INSTALL文件查询版本目录\n1  cd mininet   (3)安装Mininet，根据./mininet/util/install.sh -h命令选择参数进行安装\n1  util/install.sh -n3V 2.5.9   (4)安装成功，并查看mininet的版本\n1 2 3 4  mn --version #测试Mininet是否安装成功 sudo mn --test pingall   图1 安装成功:\r\r 　注：尽量不要用-a选项进行安装。我用了，装了3天，最后还不好用。\n1.3 可能出现的问题 　(1)由于频繁结束安装进程可能出现Could not get lock /var/lib/apt/lists/lock - open (11: Resource temporarily unavailable)，解决办法：https://blog.csdn.net/weixin_42489042/article/details/81296472 或重启。\n　(2)mininet用./mininet/miniedit.py做的图形化界面，pingall一个都不通；版本2.3.0d6\n图2 ping不通:\r\r 　解决方案：菜单栏Edit-\u0026gt;Preferences，只勾选OpenFlow1.0，或同时勾选OpenFlow1.0和OpenFlow1.3，只勾选1.3就会出现上面的现象。\n2.使用 2.1 网络构建 2.1.1 利用topo参数构建 可以快速构建一个交换机连接两台主机的拓扑\n1  sudo mn   (1)单一拓扑\n1 2 3 4  sudo mn --topo=single,3 # 构建拓扑的连线 (h1, s1) (h2, s1) (h3, s1)   (2)线性拓扑\n1 2 3 4  sudo mn --topo=linear,4 # 构建拓扑的连线 (h1, s1) (h2, s2) (h3, s3) (h4, s4) (s2, s1) (s3, s2) (s4, s3)   (3)树形拓扑\n1 2 3 4  sudo mn --topo=tree,depth=2,fanout=2 # 构建拓扑的连线 (s1, s2) (s1, s3) (s2, h1) (s2, h2) (s3, h3) (s3, h4)   (4)自定义拓扑\n1  sudo mn --custom file.py --topo mytopo   2.1.2 利用文件构建 (1)写好一个mininet的python脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95  #!/usr/bin/python import mininet.term from mininet.net import Mininet from mininet.node import Controller, RemoteController, OVSController from mininet.node import CPULimitedHost, Host, Node from mininet.node import OVSKernelSwitch, UserSwitch from mininet.node import IVSSwitch from mininet.cli import CLI from mininet.log import setLogLevel, info from mininet.link import TCLink, Intf from subprocess import call import os def myNetwork(): net = Mininet( topo=None, build=False, ipBase=\u0026#39;10.0.0.0/8\u0026#39;) info( \u0026#39;*** Adding controller\\n\u0026#39; ) c0=net.addController(name=\u0026#39;c0\u0026#39;, controller=RemoteController, ip=\u0026#39;127.0.0.1\u0026#39;, protocol=\u0026#39;tcp\u0026#39;, port=6633) info( \u0026#39;*** Add switches\\n\u0026#39;) s1 = net.addSwitch(\u0026#39;s1\u0026#39;, cls=OVSKernelSwitch, dpid=\u0026#39;0000000000000001\u0026#39;) s2 = net.addSwitch(\u0026#39;s2\u0026#39;, cls=OVSKernelSwitch, dpid=\u0026#39;0000000000000002\u0026#39;) s3 = net.addSwitch(\u0026#39;s3\u0026#39;, cls=OVSKernelSwitch, dpid=\u0026#39;0000000000000003\u0026#39;) s4 = net.addSwitch(\u0026#39;s4\u0026#39;, cls=OVSKernelSwitch, dpid=\u0026#39;0000000000000004\u0026#39;) info( \u0026#39;*** Add hosts\\n\u0026#39;) h1 = net.addHost(\u0026#39;h1\u0026#39;, mac=\u0026#39;00:00:00:00:00:01\u0026#39;, cls=Host, ip=\u0026#39;10.0.0.1\u0026#39;, defaultRoute=None) h2 = net.addHost(\u0026#39;h2\u0026#39;, mac=\u0026#39;00:00:00:00:00:02\u0026#39;, cls=Host, ip=\u0026#39;10.0.0.2\u0026#39;, defaultRoute=None) h3 = net.addHost(\u0026#39;h3\u0026#39;, mac=\u0026#39;00:00:00:00:00:03\u0026#39;, cls=Host, ip=\u0026#39;10.0.0.3\u0026#39;, defaultRoute=None) h4 = net.addHost(\u0026#39;h4\u0026#39;, mac=\u0026#39;00:00:00:00:00:04\u0026#39;, cls=Host, ip=\u0026#39;10.0.0.4\u0026#39;, defaultRoute=None) info( \u0026#39;*** Add links\\n\u0026#39;) net.addLink(s1, h1) net.addLink(s1, h2) net.addLink(s1, s3) net.addLink(s1, s4) net.addLink(s2, h3) net.addLink(s2, h4) net.addLink(s2, s3) net.addLink(s2, s4) info( \u0026#39;*** Starting network\\n\u0026#39;) net.build() info( \u0026#39;*** Starting controllers\\n\u0026#39;) for controller in net.controllers: controller.start() info( \u0026#39;*** Starting switches\\n\u0026#39;) net.get(\u0026#39;s1\u0026#39;).start([c0]) net.get(\u0026#39;s2\u0026#39;).start([c0]) net.get(\u0026#39;s3\u0026#39;).start([c0]) net.get(\u0026#39;s4\u0026#39;).start([c0]) info( \u0026#39;*** Post configure switches and hosts\\n\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s1 in_port=1,dl_src=00:00:00:00:00:01,action=output:3\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s1 in_port=2,dl_src=00:00:00:00:00:02,action=output:3\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s1 in_port=3,dl_src=00:00:00:00:00:03,action=output:1\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s1 in_port=3,dl_src=00:00:00:00:00:04,action=output:2\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s2 in_port=1,dl_src=00:00:00:00:00:03,action=output:3\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s2 in_port=2,dl_src=00:00:00:00:00:04,action=output:3\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s2 in_port=3,dl_src=00:00:00:00:00:01,action=output:1\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s2 in_port=3,dl_src=00:00:00:00:00:02,action=output:2\u0026#39;) \u0026#39;\u0026#39;\u0026#39; os.system(\u0026#39;ovs-ofctl add-flow s1 in_port=1,action=output:3\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s1 in_port=3,action=output:1\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s2 in_port=1,action=output:3\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s2 in_port=3,action=output:1\u0026#39;) \u0026#39;\u0026#39;\u0026#39; os.system(\u0026#39;ovs-ofctl add-flow s3 in_port=1,action=output:2\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s3 in_port=2,action=output:1\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s4 in_port=1,action=output:2\u0026#39;) os.system(\u0026#39;ovs-ofctl add-flow s4 in_port=2,action=output:1\u0026#39;) CLI(net) #makeTerm(h1) net.stop() if __name__ == \u0026#39;__main__\u0026#39;: setLogLevel( \u0026#39;info\u0026#39; ) myNetwork() #term.makeTerm(h1)   (2)用sudo运行python脚本\n1  sudo python filename.py   2.1.3 利用miniedit构建 (1)进入mininet的examples目录，运行\nsudo ./miniedit.py\r(2)从左边图标拖动图标，构建网络拓扑，在对应元件上长按鼠标右键，滑到Properties可以设置原件属性\n图3 Properties:\r\r 图4 设置控制器:\r\r 图5 设置交换机:\r\r 图6 设置主机:\r\r (3)左上角Edit-\u0026gt;Preference可以进行全局配置，注意不能只勾选OpenFlow 1.3\n图7 全局配置:\r\r (4)勾选Start CLI，点击左下角的Run，可以在终端看到CLI。\n图8 Run:\r\r (5)File-\u0026gt;Export Level 2 Script，保存为Python脚本，然后下面命令即可创建网络拓扑\n1 2  chmod -R 777 test.py ./test.py   图9 保存为Python脚本:\r\r 2.1.4 清除配置信息 如果拓扑建立失败，提示已被占用，可以用下列命令清空配置\n1  mn -c   2.2连接控制器 (1)如果没有指定控制器，会使用mininet中默认的控制器。\n(2)下列命令可以 连接指定ip:port的控制器\n1  sudo mn --controller=remote, --ip=\u0026lt;controller_ip\u0026gt;,--port=[port]   (3)启动mininet，--controller=remote可以连上启动的RYU控制器。\n因为启动RYU控制器，启动在6653端口，remote默认连接6653端口，所以mininet的网络可以连接上RYU。\n","description":"mininet的安装与使用，以及踩过的坑","id":20,"section":"posts","tags":["sdn"],"title":"工具使用：mininet","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8mininet/"},{"content":"1 协程 　协程是并发的一种实现方式。和多线程的抢占式不同，它是当前运行的协程A在需要等待第三方结果时，主动让出控制权，然后由其它空闲的就绪协程B继续执行，当B也需要等待结果时，再主动让出控制权，让其它协程继续执行。也就是说，协程的运行方式是一组协程轮流获得控制权，以充分利用等待时间，也就是小学学过的“时间统筹”，在电饭煲煮饭的同时可以切菜，在洗衣机洗衣服的同时可以炒菜。\n　所有协程运行在单进程的一个线程上，所以不需要对资源加锁，同一时刻仅会有一个进程访问资源，减少了锁的开销，也减少了程序的复杂度和奇怪的错误。但注意考虑控制权移交前后的同步资源问题。\n　由于协程的主要原理是在协程A等待的时间让出控制权让不需等待结果的协程B继续运行，所以等待时间较长的任务上更适合协程，即IO密集型任务，如发送网络请求（网络IO），文件读写等等。\n2 asyncio \rasyncio的官方文档地址\n\r \r　我们这里只讲基于原生协程（通过async def声明）的协程，不讲基于生成器的协程（即通过装饰器@asyncio.coroutine声明的协程），因为基于生成器的协程将在Python3.10中废弃。\n　第1节中提到，协程的运行方式是主动让出控制权，那么就需要有一个能够让出控制权和分配控制权给其它协程的机制。asyncio实现这种机制的方式是一个事件循环。它将所有协程放入一个事件循环中，按照某种顺序依次询问协程是否空闲，空闲的协程A获得控制权并开始运行，直到A开始等待，A主动交出控制权，事件循环再次询问协程是否空闲，直至询问到下一个空闲的协程，直至所有的协程都运行完毕。\n一个简单的协程示例 　用此程序来体会一下协程。\n 程序来源：https://realpython.com/async-io-python/ countasync.py\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # simple_asyncio.py import asyncio async def count(): print(\u0026#34;One\u0026#34;) await asyncio.sleep(1) print(\u0026#34;Two\u0026#34;) async def main(): await asyncio.gather(count(), count(), count()) if __name__ == \u0026#34;__main__\u0026#34;: import time s = time.perf_counter() asyncio.run(main()) elapsed = time.perf_counter() - s print(f\u0026#34;{__file__} executed in {elapsed:0.2f} seconds.\u0026#34;)   　运行结果：\n1 2 3 4 5 6 7 8  $ python3.7 simple_asyncio.py One One One Two Two Two simple_asyncio.py executed in 1.00 seconds.   　可以看到上面的程序运行了三次而实际用时却只用了1s。\n协程函数的定义 　协程通过async/await语法进行声明，需要使用Python3.7以上的版本，使用async/await语法的时候需要引用asyncio包：\n1  import asyncio   　声明一个协程函数在def前加上async关键字即可。其中的内容需要有能够主动让出主动权的语句，如await、return、yield，才能够有条件让其它函数在其等待期间运行，达到并发效果。但是请注意async def声明的协程函数内部不能有yield from。不再讲述return和yield。await后面必须是一个可等待对象，意思是等待这个协程运行完毕。\n　在下面这个程序中，asyncio.sleep(delay)可以替换成任意可等待对象：\n1 2 3  async def say_after(delay, what): await asyncio.sleep(delay) print(what)   \r\r 通过装饰器@asyncio.coroutine声明的协程内部的yield from就相当于async def声明的协程内部的await。 但是，async def声明的协程函数内部不能有yield from。  \r \r可等待对象 主要有三种类型，协程、任务、Future。\n 协程：协程函数（用async def定义的函数），协程对象（调用协程函数所返回的对象） 任务：用来放入调度槽，准备立即运行 Future：特殊的低层级可等待对象，表示一个异步操作的最终结果，一般不出现在应用层级代码中。  协程的运行 　运行协程通常会设定一个main()入口，在main()中安排、调用各个具体的协程任务，通过asyncio.run(main())进入协程。\n　在下面的示例（来自于官方文档）中，定义了协程函数say_after()和协程函数main()，其中main()作为协程入口，用asyncio.run(main())进入main()，在main()中调用两次协程函数say_after()。\n\r asyncio.run(coro, ***, debug=False)\n执行 coroutine coro 并返回结果。\n 此函数会运行传入的协程，负责管理 asyncio 事件循环，终结异步生成器，并关闭线程池。 当有其他 asyncio 事件循环在同一线程中运行时，此函数不能被调用。 如果 debug 为 True，事件循环将以调试模式运行。 此函数总是会创建一个新的事件循环并在结束时关闭。它应当被用作 asyncio 程序的主入口点，理想情况下应当只被调用一次。   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # asyncio_no_concurrency.py import asyncio import time async def say_after(delay, what): await asyncio.sleep(delay) print(what) async def main(): print(f\u0026#34;started at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) await say_after(1, \u0026#39;hello\u0026#39;) await say_after(2, \u0026#39;world\u0026#39;) print(f\u0026#34;finished at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) asyncio.run(main())   \r　运行上面的示例我们会得到如下结果：\n1 2 3 4 5  $python3.7 asyncio_no_concurrency.py started at 11:31:11 hello world finished at 11:31:14   \r　可以发现虽然用了协程，但是并没有并发执行，运行的总时间还是3秒。如何并发运行协程呢？\n协程的并发运行 　并发运行协程需要把协程封装为Task对象（asyncio.create_task(\u0026lt;async def\u0026gt;)），此时协程已经开始运行，可以分别await这些Task对象以获取其执行结果；或者使用await asyncio.gather(\u0026lt;async def1\u0026gt;, \u0026lt;async def2\u0026gt;, ...)。asyncio.gather()会把协程当作任务，加入日程。\n　也就是说asyncio.create_task提交任务，此时任务在后台运行，不会输出结果，可以用await等待结果；await asyncio.gather()提交任务并等待结果。\n asyncio.create_task(coro, ***, name=None)\n将 coro 协程 打包为一个 Task 排入日程准备执行。返回 Task 对象。\n 　asyncio.create_task()返回一个Task对象，它的参数是协程。可以用await获取它运行时的一些结果；如果该Task对象已经运行结束了，仍会保留其结果，还可以通过await语句查看。\n awaitable asyncio.gather(*aws, loop=None, return_exceptions=False)\n并发运行aws序列中的可等待对象。\n 如果所有可等待对象都成功完成，结果将是一个由所有返回值聚合而成的列表。结果值的顺序与 aws 中可等待对象的顺序一致。 如果 aws 中的某个可等待对象为协程，它将自动作为一个任务加入日程。 loop在Python3.8中已经废弃，3.10中将被删除。 如果 return_exceptions 为 False (默认)，所引发的首个异常会立即传播给等待 gather() 的任务。aws 序列中的其他可等待对象 不会被取消 并将继续运行。  如果 return_exceptions 为 True，异常会和成功的结果一样处理，并聚合至结果列表。 如果 gather() 被取消，所有被提交 (尚未完成) 的可等待对象也会 被取消。     　asyncio.gather()的参数是可等待对象，所以可以将asyncio.create_task()返回的Task对象作为它的参数。\n　运行下面的程序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # asyncio_concurrency_await.py import asyncio import time async def nested(): print(\u0026#34;Coro nested Started.\u0026#34;) await asyncio.sleep(3) print(\u0026#34;Coro nested Finished.\u0026#34;) async def main(): print(\u0026#34;Coro main Started.\u0026#34;) task = asyncio.create_task(nested()) print(\u0026#34;Coro main Finished.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: print(f\u0026#34;started at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) asyncio.run(main()) print(f\u0026#34;finished at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;)   　会得到这样的结果：\n1 2 3 4 5 6  $python3.7 asyncio_concurrency_await.py started at 15:11:25 Coro main Started. Coro main Finished. Coro nested Started. finished at 15:11:25   　可以看到是从协程main()进入的，并且由于没有await，main()在return前不会交出控制权，所以在输出Finieshed以后，nested()获得控制权，运行到sleep，交出控制权，回到___main__中，运行到最后一句，然后直接退出。如果想要看到nested()的结果，需要await nested()。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # asyncio_concurrency_await.py import asyncio import time async def nested(): print(\u0026#34;Coro nested Started.\u0026#34;) await asyncio.sleep(3) print(\u0026#34;Coro nested Finished.\u0026#34;) async def main(): print(\u0026#34;Coro main Started.\u0026#34;) task = asyncio.create_task(nested()) await task #await asyncio.gather(nested()) print(\u0026#34;Coro main Finished.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: print(f\u0026#34;started at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) asyncio.run(main()) print(f\u0026#34;finished at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;)   　将得到下面的输出结果：\n1 2 3 4 5 6 7  $python3.7 asyncio_concurrency_await.py started at 15:17:53 Coro main Started. Coro nested Started. Coro nested Finished. Coro main Finished. finished at 15:17:56   　当然也可以直接将这两句替换为await asyncio.gather(nested())。\n　所以在有多个任务时，通常使用asyncio.create_task()并发启动任务，用await asyncio.gather()语句等待所有任务的执行结果。用asyncio.create_task()方便创建task list，因为直接创建在list中会顺序运行这些程序，而用推导式（列表生成式）和asyncio.create_task()可以使代码美观：\n1 2 3 4 5 6 7 8 9 10  #程序来自于附录[1]的asyncq.py ... producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)] consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)] await asyncio.gather(*producers) ... #这样写会导致顺序运行produce(0,q)，produce(1,q)，然后一直等待，无法获得一个任务列表 producers = [produce(n, q) for n in range(nprod)] #如果不使用create_task()，就要写成下面的格式，这很糟糕 await asyncio.gather(produce(0,q), produce(1,q),...)   控制并发量 　一次并发太多可能会导致意外的结果，比如并发量为5000，但服务器可能检测到2000个请求就将你标记为可疑。可以通过asyncio.Semaphore(\u0026lt;num\u0026gt;)控制在事件循环中的最大任务数。用法如下：\n1 2 3 4 5 6  #最多并发10个任务，超过10个的任务将等待 sem = asyncio.Semaphore(10) # ... later async with sem: # work with shared resource   \r class asyncio.Semaphore(value=1, ***, loop=None)\nSemaphore会管理一个内部计数器，该计数器会随每次 acquire() 调用递减并随每次 release()调用递增。计数器的值永远不会降到零以下；当 acquire()发现其值为零时，它将保持阻塞直到有某个任务调用了 release()\nSource\n asyncio.Semaphore(\u0026lt;num\u0026gt;)推荐使用async with语句，上面的示例代码等价于：\n1 2 3 4 5 6 7 8  sem = asyncio.Semaphore(10) # ... later await sem.acquire() try: # work with shared resource finally: sem.release()   总结 　Python官方提供的异步IO包为asyncio，需要Python3.7以上版本。\n　对于单个协程来说，使用async def定义，同时协程函数体中应有交出控制权的关键词，如await、yield、return。\n　对于所有协程来说，需要一个协程的入口main()，也被称作包装器（wrapper），通过asyncio.run(main())启动事件循环，并执行main()。在main()中把要执行的协程用asyncio.create_task()封装为task()对象，并发执行，然后使用await asyncio.gather()获得他们的执行结果。\n　可以定义一个程序模板：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import asyncio async def coro1(): await asyncio.sleep(5) async def coro2(): ... async def main(): task1 = asyncio.create_task(coro1()) ... await asyncio.gather(task1,...) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main())   \r\r 给定协程中的任何行(hang)都会阻塞其它协程，除非该行使用yield，await或return。所以写成更适用于IO密集型任务，把IO任务封装为协程函数，提高CPU利用率。 不是在所有函数前加上async它本身就能够异步运行了，需要该函数的实现支持异步，如发送HTTP请求需要aiohttp包。更多可以参考附录[1]的文末。 要并发的任务务必使用.create_task()，然后全部放入.gather()中等待，否则不会并发执行，仍是串行执行。  \r \r3 常见用法及注意事项 asyncio.Queue 　文档地址：https://docs.python.org/3.9/library/asyncio-queue.html?highlight=asyncio%20queue#asyncio.Queue\n class asyncio.Queue(maxsize=0, ***, loop=None)\n先入先出队列\nmaxsize：队列中最大元素个数，maxsize ≤ 0，队列大小是无限的，如果是大于0的正整数，当队列长度到达maxsize时，await put()将会被阻塞，直到其中一项被get()移出队列。\n和线程库中的queue不同，此队列的大小总是已知的，并且通过调用qsize()方法返回队列大小。\n  empty()：如果队列为空，返回True，否则返回false。 full()：如果队列中的元素个数达到了maxsize，则返回True，如果队列maxsize=0，则full()永远也不会返回True。 get_nowait()：如果有立即可获得的元素，则返回该元素，否则raise QueueEmpty。 coroutine get()：从队列中移除一个元素，并将返回该元素。如果队列为空，则等待到存在可用的元素。 put_nowait()：无阻塞地向队列中放入一个元素。如果没有可用的槽，则raise QueueFull。 coroutine put()：把元素放到队列中。如果队列满了，一直等待直到队列中有空的槽，然后将元素添加到队列。 task_done()：表示先前入队的任务已经完成。由队列消费者调用。对于每个获取任务的get()，后续都要调用task_done()来告诉队列，该任务的处理已完成。如果join()当前处于阻塞状态，则将在处理完所有项目后恢复运行（意味着已放入队列的每个项目都会接收到task_done()调用）。如果被调用的次数超过队列中放置的项目的次数，则引发ValueError。 coroutine join()：一直阻塞，直到队列中的所有元素被接收和处理完毕。每当将项目添加到队列时，未完成任务的数量就会增加。每当消费者协程调用task_done()表示已检索到该物品并且该物品的所有工作已完成时，该计数就会减少。当未完成的任务数降至零时，join()解除阻止。  　下面的程序展示了在有20个元素的队列中，使用3个协程共同消费队列中元素，程序源于官方文档\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  import asyncio import random import time async def worker(name, queue): while True: # Get a \u0026#34;work item\u0026#34; out of the queue. sleep_for = await queue.get() # Sleep for the \u0026#34;sleep_for\u0026#34; seconds. await asyncio.sleep(sleep_for) # Notify the queue that the \u0026#34;work item\u0026#34; has been processed. queue.task_done() print(f\u0026#39;{name} has slept for {sleep_for:.2f} seconds\u0026#39;) async def main(): # Create a queue that we will use to store our \u0026#34;workload\u0026#34;. queue = asyncio.Queue() # Generate random timings and put them into the queue. total_sleep_time = 0 for _ in range(20): sleep_for = random.uniform(0.05, 1.0) total_sleep_time += sleep_for queue.put_nowait(sleep_for) # Create three worker tasks to process the queue concurrently. tasks = [] for i in range(3): task = asyncio.create_task(worker(f\u0026#39;worker-{i}\u0026#39;, queue)) tasks.append(task) # Wait until the queue is fully processed. started_at = time.monotonic() await queue.join() total_slept_for = time.monotonic() - started_at # Cancel our worker tasks. for task in tasks: task.cancel() # Wait until all worker tasks are cancelled. await asyncio.gather(*tasks, return_exceptions=True) print(\u0026#39;====\u0026#39;) print(f\u0026#39;3 workers slept in parallel for {total_slept_for:.2f} seconds\u0026#39;) print(f\u0026#39;total expected sleep time: {total_sleep_time:.2f} seconds\u0026#39;) asyncio.run(main())   　time.monotonic()用来测量准确的程序执行时间，它返回一个不会倒退的时间，不受系统时间同步更新影响。\n\r　报错：got Future attached to a different loop\n　Queue必须在loop中定义，即在运行asyncio.run()之后定义，否则Queue会创建一个新的事件循环，然后asyncio.run()也会创建一个新的循环，Queue和asyncio.run()中的函数就处于不同的循环中，导致报错。\n　参考：https://stackoverflow.com/questions/53724665/using-queues-results-in-asyncio-exception-got-future-future-pending-attachedhttps://docs.python.org/zh-cn/3/library/asyncio.html?highlight=asyncio#module-asyncio)\n\r Task.cancel() 　取消一个正在运行的Task对象，使该Task对象抛出一个CancelledError异常给打包的协程。如果取消期间协程正在等待一个Future对象，该Future对象也将被取消。\n　下面的程序中，如果不捕获异常，会报Traceback的错，因为取消任务发生在cancel_me()的await期间。程序源于官方文档。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # asyncio_task_cancel.py async def cancel_me(): print(\u0026#39;cancel_me(): before sleep\u0026#39;) try: # Wait for 1 hour await asyncio.sleep(3600) except asyncio.CancelledError: print(\u0026#39;cancel_me(): cancel sleep\u0026#39;) raise finally: print(\u0026#39;cancel_me(): after sleep\u0026#39;) async def main(): # Create a \u0026#34;cancel_me\u0026#34; Task task = asyncio.create_task(cancel_me()) # Wait for 1 second await asyncio.sleep(1) task.cancel() try: await task except asyncio.CancelledError: print(\u0026#34;main(): cancel_me is cancelled now\u0026#34;) asyncio.run(main()) # Expected output: # # cancel_me(): before sleep # cancel_me(): cancel sleep # cancel_me(): after sleep # main(): cancel_me is cancelled now    exception asyncio.CancelledError\n该操作已被取消，取消asyncio任务时，可以捕获此异常以执行自定义操作。在几乎所有情况下，都必须重新引发异常。\n 　只有在await等待的任务没有执行完时才会抛出CancelledError异常，不然只会取消任务，所以.cancel()常用于取消处于无限循环状态的任务（比如第1节中的task.cancel()）。\nEvent Loop 　注：应用开发者应使用高层级的asyncio函数，如asyncio.run()，应尽量减少循环对象的使用。\n　官方文档：https://docs.python.org/zh-cn/3/library/asyncio-eventloop.html?highlight=loop%20run_until_complete#asyncio.loop.run_until_complete\n　asyncio.get_event_loop()：获取当前事件循环，如果没有，则创建一个新的事件循环并将其设置为当前事件循环；注意一般不在协程和回调中使用。\n　asyncio.get_running_loop()：返回当前系统线程正在运行的事件循环。在协程和回调中使用该函数获取事件循环。\n　asyncio.set_event_loop(loop)：将loop设置为当前系统线程的事件循环。\n　asyncio.new_event_loop()：创建一个新的事件循环。\n　loop.run_until_complete(future)：运行future实例，直到future实例被完成。如果参数是coroutine object，则将被隐式调度为asyncio.Task来运行。\nasyncio.run() 　源代码：https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py\n　由于支持异步的第三方库会使用event_loop，较为底层，考虑替换为asyncio.run()，所以简单探究一下asyncio.run()和event_loop的关系。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67  __all__ = \u0026#39;run\u0026#39;, from . import coroutines from . import events from . import tasks def run(main, *, debug=None): \u0026#34;\u0026#34;\u0026#34;Execute the coroutine and return the result. This function runs the passed coroutine, taking care of managing the asyncio event loop and finalizing asynchronous generators. This function cannot be called when another asyncio event loop is running in the same thread. If debug is True, the event loop will be run in debug mode. This function always creates a new event loop and closes it at the end. It should be used as a main entry point for asyncio programs, and should ideally only be called once. Example: async def main(): await asyncio.sleep(1) print(\u0026#39;hello\u0026#39;) asyncio.run(main()) \u0026#34;\u0026#34;\u0026#34; if events._get_running_loop() is not None: raise RuntimeError( \u0026#34;asyncio.run() cannot be called from a running event loop\u0026#34;) if not coroutines.iscoroutine(main): raise ValueError(\u0026#34;a coroutine was expected, got {!r}\u0026#34;.format(main)) loop = events.new_event_loop() try: events.set_event_loop(loop) if debug is not None: loop.set_debug(debug) return loop.run_until_complete(main) finally: try: _cancel_all_tasks(loop) loop.run_until_complete(loop.shutdown_asyncgens()) loop.run_until_complete(loop.shutdown_default_executor()) finally: events.set_event_loop(None) loop.close() def _cancel_all_tasks(loop): to_cancel = tasks.all_tasks(loop) if not to_cancel: return for task in to_cancel: task.cancel() loop.run_until_complete( tasks.gather(*to_cancel, loop=loop, return_exceptions=True)) for task in to_cancel: if task.cancelled(): continue if task.exception() is not None: loop.call_exception_handler({ \u0026#39;message\u0026#39;: \u0026#39;unhandled exception during asyncio.run() shutdown\u0026#39;, \u0026#39;exception\u0026#39;: task.exception(), \u0026#39;task\u0026#39;: task, })   　根据源代码可以看出，asyncio.run()完成了创建事件循环，设置事件循环并运行main实例。\naiokafka  官方文档：https://aiokafka.readthedocs.io/en/stable/\n 　kafka的简易安装和使用可以参考另一篇安装及使用教程（暂时还没写）。\n消费者(Consumer) 　可以异步消费bootstrap_servers中的topic列表，即下面程序的test3和test4。\n　aiokafka给出的官方示例程序引用了asyncio.get_event_loop()，这是Python官方在应用程序中不建议使用的，所以我在asyncio.run()小节中对此句进行了修改。\n　官方示例程序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  from aiokafka import AIOKafkaConsumer import asyncio loop = asyncio.get_event_loop() async def consume(): consumer = AIOKafkaConsumer( \u0026#39;test3\u0026#39;, \u0026#39;test4\u0026#39;, loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;) # Get cluster layout and join group `my-group` await consumer.start() try: # Consume messages async for msg in consumer: print(\u0026#34;consumed: \u0026#34;, msg.topic, msg.partition, msg.offset, msg.key, msg.value, msg.timestamp) finally: # Will leave consumer group; perform autocommit if enabled. await consumer.stop() loop.run_until_complete(consume())   　上述代码使用底层API启动该任务，而consume()是协程，考虑使用应用级API进行修改。asyncio.get_event_loop()创建并设置当前事件循环，loop.run_until_complete(consume())运行协程直到它结束。\n 使用asyncio.run()创建事件循环并设置事件循环； 创建入口协程main()，用asyncio.run()启动； 由于consumer需要当前事件循环，所以在main()中使用asyncio.get_running_loop()获取当前事件循环（注意不要使用.get_event_loop()），然后将consume(loop)封装为task启动协程，并使用await等待运行。  　替换后的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # aioconsumer.py from aiokafka import AIOKafkaConsumer import asyncio async def consume(loop): # loop 在此处获取也可以 consumer = AIOKafkaConsumer( \u0026#39;test3\u0026#39;, \u0026#39;test4\u0026#39;, loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;) # Get cluster layout and join group `my-group` await consumer.start() try: # Consume messages async for msg in consumer: print(\u0026#34;consumed: \u0026#34;, msg.topic, msg.partition, msg.offset, msg.key, msg.value, msg.timestamp) finally: # Will leave consumer group; perform autocommit if enabled. await consumer.stop() async def main(): loop = asyncio.get_running_loop() task = asyncio.create_task(consume(loop)) await task asyncio.run(main())   生产者(Producer) 　允许生产消息到Topic时交出控制权。同样地，也对官方示例程序进行了修改，不再使用get_event_loop。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  # aioproducer.py from aiokafka import AIOKafkaProducer import asyncio import json async def produce(): loop = asyncio.get_running_loop() producer = AIOKafkaProducer( loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, value_serializer=lambda v: json.dumps(v).encode(\u0026#39;utf-8\u0026#39;)) # Get cluster layout and initial topic/partition leadership information await producer.start() try: # Produce message pkt = { \u0026#34;type\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;http://test.com/\u0026#34; } await producer.send_and_wait(\u0026#34;test3\u0026#34;, pkt) finally: # Wait for all pending messages to be delivered or expire. await producer.stop() async def main(): task = asyncio.create_task(produce()) await task asyncio.run(main())   生产者消费者通过队列通信 　程序流程和第2节中总结部分的程序模板一致。运行程序时可使用上面的生产者先向kafka中生产一部分消息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  # aiokafka_queue.py from aiokafka import AIOKafkaConsumer from aiokafka import AIOKafkaProducer import asyncio import json import time import base64 async def consume(consumer, queue): # Get cluster layout and join group `my-group` count = 0 await consumer.start() try: # Consume messages async for msg in consumer: count += 1 data = msg.value data.pop(\u0026#39;type\u0026#39;) data[\u0026#39;id\u0026#39;] = count await queue.put(data) finally: # Will leave consumer group; perform autocommit if enabled. await consumer.stop() async def produce(producer, queue): # Get cluster layout and initial topic/partition leadership information await producer.start() while True: try: # Produce message data = await queue.get() await producer.send_and_wait(\u0026#34;test7\u0026#34;, data) print(f\u0026#34;produed at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) queue.task_done() except: # Wait for all pending messages to be delivered or expire. await producer.stop() async def main(): loop = asyncio.get_running_loop() queue = asyncio.Queue(5000) consumer = AIOKafkaConsumer( \u0026#39;test3\u0026#39;, loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;, value_deserializer=lambda m: json.loads(m.decode(\u0026#39;ascii\u0026#39;))) producer = AIOKafkaProducer( loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, value_serializer=lambda v: json.dumps(v).encode(\u0026#39;utf-8\u0026#39;)) consume_task = asyncio.create_task(consume(consumer, queue)) produce_task = asyncio.create_task(produce(producer, queue)) await queue.join() await asyncio.gather(consume_task, produce_task) if __name__ == \u0026#34;__main__\u0026#34;: print(f\u0026#34;started at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) asyncio.run(main()) print(f\u0026#34;finished at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;)   消费的同时生产 　也可以将异步Kafka生产语句嵌入消费者中，但一定注意，要将await producer.start()语句移出消费者（不移出电脑会Boom）。在下面的程序示例中，不可以将await producer.start()放置在produce()协程函数中。\n　异步消费的同时生产也可以使用同步Kafka进行生产，减少交出控制权的次数（此处不再列出程序，可以参考第4节地址中的aiokafka_consume_and_synproduce.py）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  # aiokafka_consume_and_produce.py from aiokafka import AIOKafkaConsumer from aiokafka import AIOKafkaProducer import asyncio import json import time import base64 async def consume(consumer, producer): # Get cluster layout and join group `my-group` count = 0 await consumer.start() try: # Consume messages async for msg in consumer: count += 1 data = msg.value data.pop(\u0026#39;type\u0026#39;) data[\u0026#39;id\u0026#39;] = count await produce(producer, data) finally: # Will leave consumer group; perform autocommit if enabled. await consumer.stop() async def produce(producer, msg): try: await producer.send_and_wait(\u0026#34;test7\u0026#34;, msg) print(f\u0026#34;{msg[\u0026#39;id\u0026#39;]} produced at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) except: # Wait for all pending messages to be delivered or expire. await producer.stop() async def main(): loop = asyncio.get_running_loop() consumer = AIOKafkaConsumer( \u0026#39;test3\u0026#39;, loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;, value_deserializer=lambda m: json.loads(m.decode(\u0026#39;ascii\u0026#39;))) producer = AIOKafkaProducer( loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, value_serializer=lambda v: json.dumps(v).encode(\u0026#39;utf-8\u0026#39;)) # 此句一定不能在produce中 await producer.start() consume_task = asyncio.create_task(consume(consumer, producer)) await asyncio.gather(consume_task) if __name__ == \u0026#34;__main__\u0026#34;: print(f\u0026#34;started at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) asyncio.run(main()) #print(f\u0026#34;finished at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;)   aiohttp  官方文档：https://docs.aiohttp.org/en/stable/\n 并发发送HTTP请求 　下面的代码是利用HTTP接口上传文件到S3服务，u可替换成S3服务地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  # aiohttp_send_req.py import sys import time import aiohttp import asyncio from aiohttp import ClientSession async def put_pcapng(url,session,semaphore,data,headers): async with semaphore: async with session.put(url,data=data,headers=headers) as resp: if resp.status == 200: return await resp.read() else: print(f\u0026#34;ERROR: {time.strftime(\u0026#39;%X\u0026#39;)} {resp.status} {url}.\u0026#34;) async def main(urls, data, headers): #设置最大并发量，详细见第2节\u0026#34;控制并发量\u0026#34; semaphore =asyncio.Semaphore(500) async with ClientSession() as session: tasks = [] for url in urls: tasks.append( put_pcapng(url=url, session=session, semaphore=semaphore,data=data,headers=headers) ) await asyncio.gather(*tasks) if __name__ == \u0026#34;__main__\u0026#34;: assert sys.version_info \u0026gt;= (3, 7), \u0026#34;Script requires Python 3.7+.\u0026#34; with open(\u0026#39;test.pcapng\u0026#39;, \u0026#39;rb\u0026#39;) as f: data = f.read() headers = { \u0026#34;Content-Type\u0026#34;:\u0026#34;pcapng\u0026#34;, \u0026#34;token\u0026#34;:\u0026#34;xxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; } u = \u0026#39;http://endpoint_url/\u0026lt;bucket_name\u0026gt;/\u0026lt;key\u0026gt;\u0026#39; urls = [] for i in range(1,4001): url = u + \u0026#39;test\u0026#39; + str(i) + \u0026#39;.pcapng\u0026#39; urls.append(url) print(f\u0026#34;started at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) asyncio.run(main(urls,data,headers)) print(f\u0026#34;finished at {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;)   并发发送HTTP请求并解析HTML页面 　可以参考附录1的areq.py，程序的详细解析可以看附录的1。为节省篇幅，此处不再放置此程序。\naiokafka、aiohttp、asyncio.queue综合应用 　协程1从kafka的consume_topic中消费数据，修改后放入队列，然后生产到produce_topic中；协程2(100个)从队列中取数据，上传到S3服务。\n　生产者用AIOKafkaProducer：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116  # aiokafka_to_s3.py import json import time from aiokafka import AIOKafkaConsumer, AIOKafkaProducer import asyncio import base64 import aiohttp from aiohttp import ClientSession headers = { \u0026#34;Content-Type\u0026#34;:\u0026#34;pcapng\u0026#34;, \u0026#34;token\u0026#34;:\u0026#34;xxxxxxxxxxxxxx\u0026#34; } endpoint_url = \u0026#39;http://\u0026lt;endpoint_url\u0026gt;/\u0026lt;bucket_name\u0026gt;/\u0026#39; with open(\u0026#39;test.pcapng\u0026#39;, \u0026#39;rb\u0026#39;) as f: pcapng_data = f.read() total_s3 = 0 total_prod = 0 async def produce(producer, msg): # Get cluster layout and initial topic/partition leadership information try: await producer.send_and_wait(\u0026#34;test6\u0026#34;, msg) #print(f\u0026#34;INFO: {time.strftime(\u0026#39;%X\u0026#39;)} {msg[\u0026#39;id\u0026#39;]} produced. \u0026#34;) except: # Wait for all pending messages to be delivered or expire. await producer.stop() async def consume(consumer, producer, msg_q): # Get cluster layout and join group `my-group` global total_prod await consumer.start() try: # Consume messages async for msg in consumer: data = msg.value data.pop(\u0026#39;type\u0026#39;) data[\u0026#39;id\u0026#39;] = total_prod data[\u0026#39;url\u0026#39;] = endpoint_url + str(data[\u0026#39;id\u0026#39;]) + \u0026#39;.pcapng\u0026#39; await produce(producer, data) # json中不能有byte类型 data[\u0026#39;pcapng\u0026#39;] = pcapng_data await msg_q.put(data) total_prod += 1 finally: # Will leave consumer group; perform autocommit if enabled. await consumer.stop() async def upload_to_s3(session,semaphore,queue): global total_s3 while True: data = await queue.get() url = data[\u0026#39;url\u0026#39;] async with semaphore: async with session.put(url,data=data[\u0026#39;pcapng\u0026#39;],headers=headers) as resp: if resp.status == 200: queue.task_done() total_s3 += 1 #print(f\u0026#34;INFO: {time.strftime(\u0026#39;%X\u0026#39;)} finish upload {data[\u0026#39;id\u0026#39;]} {url}. \u0026#34;) else: # 此处错误没有做重传处理，可以采用再放回队列的方式 print(f\u0026#34;ERROR: {time.strftime(\u0026#39;%X\u0026#39;)} {resp.status} {url}. Retried. \u0026#34;) async def print_metric(start_time): while True: t_time = time.time() - start_time s3_speed = total_s3/t_time prod_speed = total_prod/t_time print(f\u0026#34;upload to s3: {s3_speed}\u0026#34;) print(f\u0026#34;produce to kafka: {prod_speed}\u0026#34;) await asyncio.sleep(1) async def main(): #初始化工作 pcapng_q = asyncio.Queue(500) semaphore =asyncio.Semaphore(500) start_time = time.time() async with ClientSession() as session: loop = asyncio.get_running_loop() consumer = AIOKafkaConsumer( \u0026#39;test3\u0026#39;, loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;, value_deserializer=lambda m: json.loads(m.decode(\u0026#39;ascii\u0026#39;))) producer = AIOKafkaProducer( loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, value_serializer=lambda v: json.dumps(v).encode(\u0026#39;utf-8\u0026#39;)) await producer.start() consume_task = asyncio.create_task(consume(consumer, producer, pcapng_q)) speed_task = asyncio.create_task(print_metric(start_time)) print(f\u0026#34;started {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) tasks_s3 = [] for i in range(100): tasks_s3.append(asyncio.create_task(upload_to_s3(session,semaphore,pcapng_q))) await pcapng_q.join() await asyncio.gather(consume_task, speed_task, *tasks_s3) if __name__ == \u0026#34;__main__\u0026#34;: try: asyncio.run(main()) except KeyboardInterrupt: print(\u0026#34;Quit.\u0026#34;)   \n　生产者用KafkaProducer：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  # aiokafka_to_s3_synp.py  import json import time from aiokafka import AIOKafkaConsumer from kafka import KafkaProducer import asyncio import base64 import aiohttp from aiohttp import ClientSession headers = { \u0026#34;Content-Type\u0026#34;:\u0026#34;pcapng\u0026#34;, \u0026#34;token\u0026#34;:\u0026#34;xxxxxxxxxxxxxxxxxx\u0026#34; } endpoint_url = \u0026#39;http://\u0026lt;endpoint_url\u0026gt;/\u0026lt;bucket_name\u0026gt;/\u0026#39; with open(\u0026#39;test.pcapng\u0026#39;, \u0026#39;rb\u0026#39;) as f: pcapng_data = f.read() total_s3 = 0 total_prod = 0 async def consume(consumer, producer, msg_q): # Get cluster layout and join group `my-group` global total_prod await consumer.start() try: # Consume messages async for msg in consumer: data = msg.value data.pop(\u0026#39;type\u0026#39;) data[\u0026#39;id\u0026#39;] = total_prod data[\u0026#39;url\u0026#39;] = endpoint_url + str(data[\u0026#39;id\u0026#39;]) + \u0026#39;.pcapng\u0026#39; producer.send(\u0026#39;test6\u0026#39;, data) # json中不能有byte类型 data[\u0026#39;pcapng\u0026#39;] = pcapng_data await msg_q.put(data) total_prod += 1 finally: # Will leave consumer group; perform autocommit if enabled. await consumer.stop() async def upload_to_s3(session,semaphore,queue): global total_s3 while True: data = await queue.get() url = data[\u0026#39;url\u0026#39;] async with semaphore: async with session.put(url,data=data[\u0026#39;pcapng\u0026#39;],headers=headers) as resp: if resp.status == 200: total_s3 += 1 queue.task_done() #print(f\u0026#34;INFO: {time.strftime(\u0026#39;%X\u0026#39;)} finish upload {data[\u0026#39;id\u0026#39;]} {url}. \u0026#34;) else: # 此处错误没有做重传处理，可以采用再放回队列的方式 print(f\u0026#34;ERROR: {time.strftime(\u0026#39;%X\u0026#39;)} {resp.status} {url}. Retried. \u0026#34;) async def print_metric(start_time): while True: t_time = time.time() - start_time s3_speed = total_s3/t_time prod_speed = total_prod/t_time print(f\u0026#34;upload to s3: {s3_speed}\u0026#34;) print(f\u0026#34;produce to kafka: {prod_speed}\u0026#34;) await asyncio.sleep(1) async def main(): #初始化工作 pcapng_q = asyncio.Queue(500) semaphore =asyncio.Semaphore(500) start_time = time.time() async with ClientSession() as session: loop = asyncio.get_running_loop() consumer = AIOKafkaConsumer( \u0026#39;test3\u0026#39;, loop=loop, bootstrap_servers=\u0026#39;localhost:9092\u0026#39;, auto_offset_reset=\u0026#39;earliest\u0026#39;, value_deserializer=lambda m: json.loads(m.decode(\u0026#39;ascii\u0026#39;))) producer = KafkaProducer( value_serializer=lambda v: json.dumps(v).encode(\u0026#39;utf-8\u0026#39;), bootstrap_servers=\u0026#39;localhost:9092\u0026#39; ) consume_task = asyncio.create_task(consume(consumer, producer, pcapng_q)) speed_task = asyncio.create_task(print_metric(start_time)) print(f\u0026#34;started {time.strftime(\u0026#39;%X\u0026#39;)}\u0026#34;) tasks_s3 = [] for i in range(100): tasks_s3.append(asyncio.create_task(upload_to_s3(session,semaphore,pcapng_q))) await pcapng_q.join() await asyncio.gather(consume_task, speed_task, *tasks_s3) if __name__ == \u0026#34;__main__\u0026#34;: try: asyncio.run(main()) except KeyboardInterrupt: print(\u0026#34;Quit.\u0026#34;)   　实际测试生产者用KafkaProducer速度更快，约是AIOKafkaProducer的2倍。\n4.参考代码 　地址：https://github.com/yyyIce/py_exercise/tree/main/asyncio\n　有帮助的话可以点个Star⭐哦~\n附录 　1.一篇asyncio教程，原文：https://realpython.com/async-io-python/\n　2.一篇关于asyncio.gather()的回答：https://stackoverflow.com/questions/62528272/what-does-asyncio-create-task-do\n","description":"对Python asyncio异步编程的理解和应用","id":21,"section":"posts","tags":["python"],"title":"Python：asyncio协程理解及用法","uri":"https://yyyIce.github.io/zh/posts/pythonasyncio%E5%8D%8F%E7%A8%8B%E7%90%86%E8%A7%A3%E5%8F%8A%E7%94%A8%E6%B3%95/"},{"content":"　网络空间测绘乍一听很像新闻联播里的句子，可以在这篇综述中得到能够让人理解的解释。\n阅读小贴士：本文内容主要摘抄于参考文献[1]，作为网络空间测绘背景的快速回顾，可以以文献[1]为线索，进行相关方面资料的查找。\n1.网络空间(Cyberspace) 　网络空间可以理解为看不见的意识空间、用户、网络中的信息流、网络链路、网络设备组成的空间。看不见的意识空间可以简单描述为空间中用户的操作、用户的想法以及链路、设备之间的逻辑结构。如下图所示。\n图1 网络空间:\r\r 　文献[1]中对网络空间要素进行了描述：\n　网络空间的组成要素分为4 种类型: 载体、信息、主体和操作。\n  　载体：网络空间的软硬件设施,是提供信息通信的系统层面的集合;\n  　信息：是在网络空间中流转的数据内容, 包括人类用户及机器用户能够理解、识别和处理的信号状态;\n  　主体：互联网用户, 包括传统互联网中的人类用户以及未来物联网中的机器和设备用户;\n  　操作：对信息的创造、存储、改变、使用、传输、展示等活动。\n  2.网络空间资源 　按文献[1]中的定义，可以将网络空间资源分为实体资源和虚拟资源，如下图所示。\n图2 网络空间资源:\r\r 　网络空间资源用属性来进行区分和识别，是指网络空间资源具备的所有共同性质或独有性质的总和。比如虚拟用户，拥有共同的属性为性别、用户名、年龄等，不同的用户类别可能有独有的属性，比如只允许18岁以上的用户可见部分内容。\n关于计算机网络方向的研究内容主要围绕资源分类图展开。\n3.网络空间资源测绘 3.1 网络空间资源测绘概念 　地理测绘较为容易理解：在(x,y)处，有一个湖泊，面积多大，有什么水产品，周围人如何如何，位置和占地面积记录在地图上，其余相关信息记录在信息库里。\n　网络空间资源测绘就是记录网络空间中各个元素的位置、属性，将整个空间绘制出来。\n　“最初的网络空间测绘的概念主要是采用一些技术方法，来探测全球互联网空间上的节点分布情况和网络关系索引，构建全球互联网图谱的一种方法。”\n　而现在网络空间资源测绘是全面掌握网络空间资源的属性和状态，绘制网络空间资源全息地图，即：\n　“对网络空间中的各类虚实资源及其属性进行探测、分析和绘制的全过程。”\n3.2 网络空间资源测绘方法 　网络空间资源测绘体系是一个“ 探测(Detecting)、分析(Analyzing)、绘制(Visualizing)、应用(Applying)”的循环过程(DAVA Loop)；\n  对各种网络空间资源进行协同探测, 获取探测数据, 对这些数据进行融合分析和多域映射, 形成网络空间资源知识库;\n  在此基础上, 通过多域叠加和综合绘制来构建网络空间资源全息地图;\n  最后, 根据不同的场景目标按需应用这一全息地图, 通过迭代演进使得测绘能力不断提升。\n\r  图3 网络空间资源测绘技术体系图:\r\r 　测绘技术主要分三大方面：协同探测、融合分析、全息绘制\n3.3 网络空间资源测绘应用 　一是区域资产发现、识别和风控，我暂时没有研究内容与其相关；二是服务测绘，服务测绘在平时研究过程中有所涉及。\n4.服务测绘 　“网络空间服务是指网络空间软件设施中的各种泛在应用，其中最典型的一种应用就是网站。”\n　网络空间服务包括属性(内在)，外延的关系。\n图4 网络空间服务结构图:\r\r 　网络空间服务测绘的目标：\n　就是利用主被动协同探测和智能分析手段, 发现动态、时变、隐匿的服务属性和关系, 通过“地图”的方式进行可视化展示,以支撑网络空间安全的各种应用。\n　服务画像：包括基础属性、网络属性、位置属性、通联属性等。例如：\n  基础属性包括企业法人、主办单位名称、网站备案/许可证号、网站名称、网站首页网址等;\n  网络属性包括网站IP、网站域名、AS 号码等; 位置属性包括国家、区域代码、所在地、经纬度、地址等;\n  通联属性包括外链网址, 服务使用者IP 等。\n  　服务信息在时空范围内可与其他资源信息等叠加,按需展示关注点。\n\r图5 基于DAVA Loop循环的服务测绘体系结构图:\r\r 　服务测绘应用场景：\n　(1) 特定服务发现和识别场景: 通过主动扫描、流量监控等多种探测方式, 获取特定服务的信息。对关注区域内的特定服务进行分析与统计, 便于安全监管部门。\n　(2) 区域服务状态评估场景: 绘制网络空间上服务影响范围状态图, 在网络攻防的实践中, 为网络靶场等应用提供精准的攻击效果评估。\n　(3) 特定服务的用户分析场景: 绘制网络空间上服务和用户的连接关系图, 对特定服务的用户以及潜在用户进行群体分析。\n　(4) 特定用户的服务推荐场景: 绘制网络空间上服务和服务的连接关系图, 对特定用户进行服务推荐。\n参考资料 　[1]郭莉, 曹亚男, 苏马婧, et al. 网络空间资源测绘:概念与技术[J]. 信息安全学报, 2018, 3(4).\n","description":"简单了解一下测绘方向，对网络空间有个整体模糊的认识","id":22,"section":"posts","tags":["网络测绘"],"title":"网络空间测绘概述","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E6%B5%8B%E7%BB%98/"},{"content":"\r Python文档：https://realpython.com/async-io-python/ 可以先参看他人博客，大概了解以后看官方文档进行使用，不建议直接看文档进行学习。  \r \r1 yield 　yield在文档中的位置：https://docs.python.org/zh-cn/3/reference/expressions.html#yield-expressions\n 在一个函数体内使用 yield 表达式会使这个函数变成一个生成器，并且在一个 async def 定义的函数体内使用 yield 表达式会让协程函数变成异步生成器。\n 　即yield只能在函数内部使用，并且会使该函数变成生成器函数。如下面示例，含有yield语句的函数返回值是generator类型，不包含yield语句的函数其类型是return值的类型，如无返回值则为NoneType。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  \u0026gt;\u0026gt;\u0026gt; def foo(): ... for i in range(10): ... yield i ... \u0026gt;\u0026gt;\u0026gt; type(foo()) \u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; def boo(): ... for i in range(10): ... pass ... return i ... \u0026gt;\u0026gt;\u0026gt; type(boo()) \u0026lt;class \u0026#39;int\u0026#39;\u0026gt;   \r简单示例  当一个生成器函数被调用的时候，它返回一个迭代器，称为生成器。然后这个生成器来控制生成器函数的执行。\n当这个生成器的某一个方法被调用的时候，生成器函数开始执行。这时会一直执行到第一个 yield 表达式，在此执行再次被挂起，给生成器的调用者返回 expression_list 的值。\n挂起后，我们说所有局部状态都被保留下来，包括局部变量的当前绑定，指令指针，内部求值栈和任何异常处理的状态。\n通过调用生成器的某一个方法，生成器函数继续执行。\n此时函数的运行就和 yield 表达式只是一个外部函数调用的情况完全一致。恢复后 yield 表达式的值取决于调用的哪个方法来恢复执行。\n如果用的是 __next__() (通常通过语言内置的 for 或是 next() 来调用) 那么结果就是 None. 否则，如果用 send(), 那么结果就是传递给send方法的值。\n 　当生成器函数被调用时，它返回一个可迭代对象——生成器。生成器控制生成器函数的执行。即生成器是生成器函数的返回值。下面以一个例子说明yield函数的执行过程，解释一下Python文档的内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  #!/usr/bin python3 #yieldtest.py def foo(): for i in range(2,6): r = (yield i) print(f\u0026#34;i: {i}, r: {r}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: generator_foo = foo() print(next(generator_foo)) print(next(generator_foo)) print(generator_foo.send(66)) print(generator_foo.send(88)) print(next(generator_foo))   　示例首先定义了一个生成器函数foo()（因为它包含yield语句），该函数内部有一个for循环，i的取值为2，3，4，5，用yield返回每个i的取值，并输出i和r的取值，来观察程序执行过程。\n　（1）在__main__中，先定义了一个生成器变量，获取foo()函数返回的生成器，然后用next(\u0026lt;generator\u0026gt;)启动生成器，控制foo()函数的执行。第一步next(generator_foo)，开始运行foo()，此时i=2，程序运行到yield i，从foo()中跳出，保存foo()的运行进度，返回2到__main__中，输出next(generator_foo)的值，即yield i的i值，当前i为2，这句会在屏幕上输出2。\n　如果此处要用.send()方法启动生成器，则send()中参数必须为None。\n1  print(generator_foo.send(None))   　（2）运行第二句next(generator_foo)，恢复foo()的运行进度，从上次停止处继续运行，即r=(yield i)处，上次运行到yield i，这次要对r进行赋值。r的值与调用foo()的方式有关，如果是用next(\u0026lt;generator\u0026gt;)，则r的值为None，如果是用\u0026lt;generator\u0026gt;.send(xxx)，则r的值是xxx。此处用next(\u0026lt;generator\u0026gt;)调用foo()，所以r的值为None。所以输出i: 2, r: None。之后for循环继续，i=3，程序再次运行到yield语句，流程与（1）相同，这一句在屏幕上输出3。\n　（3）运行到第三句generator_foo.send(66))，恢复foo()的运行进度，从上次停止处继续运行，这次与（2）中不同，这次是用.send(66)调用foo()，所以r的值为66。在屏幕输出i: 3, r: 66。之后for循环继续，i=4，程序再次运行到yield语句，流程与（1）相同，这一句在屏幕上输出4。\n　（4）运行到第四句generator_foo.send(88)，和（3）流程相同，r的值为88，在屏幕上输出i: 4, r: 88，然后继续，此时i=5，再次运行到yield i语句，跳出foo()，返回到__main__，输出5。\n　（5）运行到第五句next(generator_foo)，通过next调用foo()，r值为None，所以输出i: 5, r: None，此时生成器没有下一个值，引发StopIteration异常，该异常表示该迭代器不能产生下一项。\n　所以上面例子的输出为：\n1 2 3 4 5 6 7 8 9 10 11 12 13  $ python3.7 yieldtest.py 2 i: 2, r: None 3 i: 3, r: 66 4 i: 4, r: 88 5 i: 5, r: None Traceback (most recent call last): File \u0026#34;yieldtest.py\u0026#34;, line 15, in \u0026lt;module\u0026gt; print(next(generator_foo)) StopIteration   文档中的示例 generator.\u0026lt;method()\u0026gt;：\n generator.__next__() generator.send(value) generator.throw(type[, value[, traceback]])：在生成器暂停的位置引发 type 类型的异常，并返回该生成器函数所产生的下一个值（value和traceback是可选参数，通常就这么写）。 generator.close()：在生成器函数暂停的位置引发GeneratorExit。如果生成器不产生值，则正常退出，产生值引发 RuntimeError。  具体说明可以查阅文档6.2.9.1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \u0026gt;\u0026gt;\u0026gt; def echo(value=None): ... print(\u0026#34;Execution starts when \u0026#39;next()\u0026#39; is called for the first time.\u0026#34;) ... try: ... while True: ... try: ... value = (yield value) ... except Exception as e: ... value = e ... finally: ... print(\u0026#34;Don\u0026#39;t forget to clean up when \u0026#39;close()\u0026#39; is called.\u0026#34;) ... \u0026gt;\u0026gt;\u0026gt; generator = echo(1) \u0026gt;\u0026gt;\u0026gt; print(next(generator)) Execution starts when \u0026#39;next()\u0026#39; is called for the first time. 1 \u0026gt;\u0026gt;\u0026gt; print(next(generator)) None \u0026gt;\u0026gt;\u0026gt; print(generator.send(2)) 2 \u0026gt;\u0026gt;\u0026gt; generator.throw(TypeError, \u0026#34;spam\u0026#34;) TypeError(\u0026#39;spam\u0026#39;,) \u0026gt;\u0026gt;\u0026gt; generator.close() Don\u0026#39;t forget to clean up when \u0026#39;close()\u0026#39; is called.   2 yield from 　官方文档：https://docs.python.org/zh-cn/3/whatsnew/3.3.html#pep-380\n　PEP380添加了yield from表达式，允许生成器把它的部分操作委托给另一个生成器。这让一部分包含yield的代码分解出来并放入另一个生成器中。此外，子生成器可以有返回值，发出委托的生成器可以获取该值。\n　虽然主要用于委托给一个子生成器，yield from表达式实际上也允许委托给任意的子迭代器。\n　对于简单的迭代器，yield from iterable基本上仅仅是for item in iterable:yield item的缩写：\n1 2 3 4 5 6  \u0026gt;\u0026gt;\u0026gt; def g(x): ... yield from range(x, 0, -1) ... yield from range(x) ... \u0026gt;\u0026gt;\u0026gt; list(g(5)) [5, 4, 3, 2, 1, 0, 1, 2, 3, 4]   　然而，与普通循环不同，yield from允许子生成器直接从调用中接收发送的和抛出的值，然后返回一个最终的值给外部的生成器：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  \u0026gt;\u0026gt;\u0026gt; def accumulate(): ... tally = 0 ... while 1: ... next = yield ... if next is None: ... return tally ... tally += next ... \u0026gt;\u0026gt;\u0026gt; def gather_tallies(tallies): ... while 1: ... tally = yield from accumulate() ... tallies.append(tally) ... \u0026gt;\u0026gt;\u0026gt; tallies = [] \u0026gt;\u0026gt;\u0026gt; acc = gather_tallies(tallies) \u0026gt;\u0026gt;\u0026gt; next(acc) # Ensure the accumulator is ready to accept values \u0026gt;\u0026gt;\u0026gt; for i in range(4): ... acc.send(i) ... \u0026gt;\u0026gt;\u0026gt; acc.send(None) # Finish the first tally \u0026gt;\u0026gt;\u0026gt; for i in range(5): ... acc.send(i) ... \u0026gt;\u0026gt;\u0026gt; acc.send(None) # Finish the second tally \u0026gt;\u0026gt;\u0026gt; tallies [6, 10]   　accumulate()是子生成器函数，gather_tallies()是外部生成器函数。acc是gather_tallies()返回的生成器。通过next(acc)启动acc。执行到yield from accumulate()，执行到accumulate()中的yield返回；然后通过acc.send(i)控制函数执行，发送0给gather_tallies()，但函数执行到accumulate()的next = yield，所以发送的值由next变量接收，因为子生成器可以直接接收发送的值。\n　发送到3以后，发送None，accumulate()中的next变量收到None，执行到return tally，tally的值为1+2+3=6，返回到gather_tallies()，tally的值为6，然后tallies.append(tally)将6加入到tallies列表中，此时tallies=[6,]。\n　然后再调用acc.send()，仍由accumulate()的next变量接收，发送到4以后，发送None，accumulate()中的next变量收到None，执行到return tally，tally的值为1+2+3+4=10，返回到gather_tallies()，tally的值为10，然后tallies.append(tally)将10加入到tallies列表中，此时tallies=[6, 10]。\n\ryield from语句其实就是表明后面是一个生成器，使代码看起来更整洁。\n\r \r　做出这种改变的主要原理是允许生成器分割成多个子生成器和把大的函数分解成多个子函数一样简单，通过send和throw来实现分割。\n","description":"对Python yield的理解和应用","id":23,"section":"posts","tags":["python"],"title":"Python：yield 用法和理解","uri":"https://yyyIce.github.io/zh/posts/pythonyield%E7%94%A8%E6%B3%95%E5%92%8C%E7%90%86%E8%A7%A3/"},{"content":"\r Python文档：https://docs.python.org/zh-cn/3/library/threading.html 可以先参看他人博客，大概了解以后看官方文档进行使用，不建议直接看文档进行学习。  \r \r1 threading  线程类表示在单独的控制线程中运行的活动。有两种指定活动的方法：\n 传递一个可调用对象给构造器； 在子类中重写run()方法。  子类中不应该覆盖其他方法（构造函数除外）。换句话说，只允许重写这个类的__init__()和run()\n 　可调用对象：可以被调用执行的对象，并且可以传入参数，如函数、类、类中的函数、实现了__call__方法的实例对象。\n线程对象Thread  class threading.Thread(group=None, target=None, name=None, args=(), kwargs={}, ***, daemon=None)\n  group：不必管，为了扩展ThreadGroup类实现而保留 target：run()方法调用的可调用对象 name：线程名称，默认格式为Thread-N args：调用目标函数的参数元组 kwargs：用于调用目标函数的关键字参数字典 daemon：设置线程是否是守护模式，不建议使用setDaemon()  创建一个线程：\n1 2  from threading import Thread t = Thread(target=\u0026lt;可调用对象\u0026gt;,args=(\u0026lt;可调用对象的参数\u0026gt;),...)   启动一个线程：\u0026lt;thread_instance\u0026gt;.start()\nt.start()\r开始线程活动：\u0026lt;thread_instance\u0026gt;.run()\n　在不重写线程run()方法的情况下，就是普通地运行一下target（可调用对象），一般指定target就行，需要人为控制暂停、恢复和退出的需要自定义线程类。\n　__init__()和.run()方法的源代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  #Lib/threading.py def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None): \u0026#34;\u0026#34;\u0026#34;This constructor should always be called with keyword arguments. Arguments are: *group* should be None; reserved for future extension when a ThreadGroup class is implemented. *target* is the callable object to be invoked by the run() method. Defaults to None, meaning nothing is called. *name* is the thread name. By default, a unique name is constructed of the form \u0026#34;Thread-N\u0026#34; where N is a small decimal number. *args* is the argument tuple for the target invocation. Defaults to (). *kwargs* is a dictionary of keyword arguments for the target invocation. Defaults to {}. If a subclass overrides the constructor, it must make sure to invoke the base class constructor (Thread.__init__()) before doing anything else to the thread. \u0026#34;\u0026#34;\u0026#34; assert group is None, \u0026#34;group argument must be None for now\u0026#34; if kwargs is None: kwargs = {} self._target = target self._name = str(name or _newname()) self._args = args self._kwargs = kwargs if daemon is not None: self._daemonic = daemon else: self._daemonic = current_thread().daemon self._ident = None if _HAVE_THREAD_NATIVE_ID: self._native_id = None self._tstate_lock = None self._started = Event() self._is_stopped = False self._initialized = True # Copy of sys.stderr used by self._invoke_excepthook() self._stderr = _sys.stderr self._invoke_excepthook = _make_invoke_excepthook() # For debugging and _after_fork() _dangling.add(self) def run(self): \u0026#34;\u0026#34;\u0026#34;Method representing the thread\u0026#39;s activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object\u0026#39;s constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. \u0026#34;\u0026#34;\u0026#34; try: if self._target: self._target(*self._args, **self._kwargs) finally: # Avoid a refcycle if the thread is running a function with # an argument that has a member that points to the thread. del self._target, self._args, self._kwargs   线程使用示例：\n 程序来源于https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p01_start_stop_thread.html\n 1 2 3 4 5 6 7 8 9 10 11 12  #simple_thread.py import time from threading import Thread def countdown(n): while n \u0026gt; 0: print(\u0026#39;T-minus\u0026#39;, n) n -= 1 time.sleep(5) #注意args=(10,)的逗号不能省，不然不是tuple是int t = Thread(target=countdown, args=(10,)) t.start()   等待线程.join()\n　在A线程中调用B.join()，则A线程在B.join()之后的语句直到B线程执行完毕之后才会执行，例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # thread_join.py import time from threading import Thread def countdown(n, id): while n \u0026gt; 0: print(f\u0026#39;T{id}-minus {n}\u0026#39;) n -= 1 time.sleep(5) #注意args=(10,)的逗号不能省，不然不是tuple是int t1 = Thread(target=countdown, args=(3,1)) t2 = Thread(target=countdown, args=(3,2)) t1.start() t2.start() t1.join() print(\u0026#34;after join.\u0026#34;) #输出 T1-minus 3 T2-minus 3 T1-minus 2 T2-minus 2 T1-minus 1 T2-minus 1 after join.   注释掉t1.join()后的输出如下，主线程无需等待线程1运行完毕再向后执行。\n1 2 3 4 5 6 7  T1-minus 3 T2-minus 3 after join. T1-minus 2 T2-minus 2 T1-minus 1 T2-minus 1   守护模式daemon\n 参考：https://blog.csdn.net/dongfuguo/article/details/53899426\n 　默认值为False。\n　如果在主线程中创建了子线程，当主线程结束时根据子线程daemon属性值的不同可能会发生下面的两种情况之一：\n 如果某个子线程的daemon属性为False，主线程结束时会检测该子线程是否结束，如果该子线程还在运行，则主线程会等待它完成后再退出； 如果某个子线程的daemon属性为True，主线程运行结束时不对这个子线程进行检查而直接退出，同时所有daemon值为True的子线程将随主线程一起结束，而不论是否运行完成。  　需要注意的是现在已经不提倡使用t.setDaemon(True)来设置，而是直接在初始化时设置线程是否是守护模式，设置方法：\n1 2 3 4  from threading import Thread t = Thread(target=\u0026lt;可调用对象\u0026gt;,args=(\u0026lt;可调用对象的参数\u0026gt;),daemon=True) #或下面这种方式，注意要在start()之前设置 t.daemon=True   　daemon使用效果示例如下，t1线程是守护线程，主线程运行完毕后不用理会t1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # thread_daemon.py import time from threading import Thread def countdown(n, id): while n \u0026gt; 0: print(f\u0026#39;T{id}-minus {n}\u0026#39;) n -= 1 time.sleep(5) #t1 = Thread(target=countdown, args=(3,1), daemon=False) t1 = Thread(target=countdown, args=(3,1), daemon=True) t1.start() print(\u0026#34;Main Thread End.\u0026#34;) #输出 T1-minus 3 Main Thread End.   　如果daemon值为False，则应该输出下面的内容，程序会在t1线程运行完毕再退出。\n1 2 3 4  T1-minus 3 Main Thread End. T1-minus 2 T1-minus 1   2.自定义线程类 　自定义的线程可以控制线程的暂停、恢复、退出，Thread()方法定义的线程如果是while True的消费者就没法结束它了。\n　第1节中提到，继承只允许修改__init__方法和run()方法，所以自定义线程类中仅需要重写这两项，个人感觉初学情况下重写的类还不如原生的。\n　自定义线程类示例，实现的功能和上面的功能相同：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # thread_custom.py import threading import time #继承Thread类，创建自定义线程类 class mythread(threading.Thread): def __init__(self, num, threadname): #super会将后面的方法委托给父类，等价于threading.Thread.__init__(self, name=threadname) super().__init__(self, name=threadname) self.num = num #重写run()方法 def run(self): while self.num \u0026gt; 0: print(f\u0026#39;{self.name}-minus {self.num}\u0026#39;) self.num -= 1 time.sleep(5) t1 = mythread(3, \u0026#39;t1\u0026#39;) t1.start() #输出 t1-minus 3 t1-minus 2 t1-minus 1   自定义线程控制线程暂停、恢复、退出  参考：https://www.cnblogs.com/scolia/p/6132950.html\n 　利用线程的事件对象threading.Event。\n 是线程间通信的机制：一个线程发出时间信号，其他线程等待信号。一个事件（Event）对象管理一个内部标识。\nset()：将此事件的内部标识置为True，所有等待这个事件的线程会被唤醒。\nclear()：将此事件的内部标识置为False，之后调用wait()的线程会被阻塞。\nwait(timeout=None)：如果此事件的内部标识为True，立即返回；否则阻塞线程，直到标识为True。\nis_set()：当且仅当内部标识为True时返回True。\n 　线程暂停与恢复：可以将事件理解为一个信号，在线程内部设置一个事件，在暂停时调用clear()，设置内部标识为False，恢复时设置内部标识为True。在run()函数中调用wait()让事件的阻塞效果生效。\n　线程退出：在线程内部设置另一个事件，在run()中设置while \u0026lt;stop_event\u0026gt;.is_set():循环，将目标函数放在循环内，初始化时将该事件set()，退出时将该事件clear()，此时由于不满足while循环条件，run()函数执行完毕，线程结束。\n　程序示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  # thread_pause.py import threading import time class myThread(threading.Thread): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.__flag = threading.Event() # 用于暂停线程的标识 self.__flag.set() self.__running = threading.Event() # 用于结束线程的标识 self.__running.set() def run(self): while self.__running.isSet(): self.__flag.wait() # 如果__flag=False则阻塞 print(time.time()) time.sleep(1) def pause(self): self.__flag.clear() # 设置为False, 让线程阻塞 def resume(self): self.__flag.set() # 设置为True, 唤醒线程 def stop(self): self.__flag.set() # 如果线程暂停则先恢复 self.__running.clear() # 设置为False a = myThread() a.start() time.sleep(3) a.pause() time.sleep(5) a.resume() time.sleep(3) a.pause() time.sleep(2) a.stop()   　**为什么选用这种方法：**这种方法虽然比较慢但是比较安全，让run()方法的自然执行完毕退出，强行杀死线程可能会遇到数据丢失等离谱的错误。\n3.多线程 　Python中多线程直接创建多个线程实例然后用.start()方法启动就行了。由于实际应用中没用到过这部分，所以仅作简单介绍，以后有深刻理解再补充。而且由于Python有全局锁，其中多线程会强制在单核上运行，所以Python的多线程是并发而不是并行，我更推荐使用协程和多进程。\n3.1 线程调度　  来源：https://my.oschina.net/u/240562/blog/137040\n线程切换由Python决定，内部维护着一个数值，这个数值是python的内部时钟，默认值是100，也就是说python在执行完100条语句之后，开始进行线程调度。也使用这个数值检查是否有异步事件发生。需要处理。\npython控制着什么时候进行线程调度，当一个线程获得访问python解释器的所必需的GIL并进入解释器后，即当这个线程执行了100条语句后，python解释器将强制挂起当前线程，开始切换到下一个处于等待的线程。\n究竟哪个线程会被执行，在这个问题上python是不会插手的，而是交给底层的操作系统来解决，python借用底层的操作系统的线程调度机制来决定下一个进入python解释器的线程究竟是谁。\n所以python的线程实际就是操作系统所支持的原生线程，python的多线程机制建立在操作系统的原生线程机制之上，不同的操作系统有不同的实现。\n然而最终，在不同的操作系统的原生线程之上，python提供了一套统一的抽象机制，给python的使用者一个多线程的工具箱，就是python的Thread和Threading。\n 　总结一下，就是Python多线程的切换由操作系统控制，可以简单理解为分时，每个线程占据一定的时间片，时间片运行完的线程被操作系统切换掉。　　线程调度由Python控制，且一般情况下在单核上执行。Python实现了跨平台的多线程机制，但本质上激活等待线程与操作系统有关，不同操作系统可能由不同的结果。线程切换的开销小于进程切换。\n3.2 锁对象 　class threading.Lock是实现原始锁对象的类，用于同步。一旦一个线程获得一个锁，会阻塞随后尝试获得锁的线程，直到它被释放；任何线程都可以释放它。\n   acquire(blocking=True, timeout=-1)\n可以阻塞或非阻塞地获得锁。\nblocking：当调用时参数 blocking 设置为 True （缺省值），阻塞直到锁被释放，然后将锁锁定并返回 True 。在参数 blocking 被设置为 False 的情况下调用，将不会发生阻塞。如果调用时 blocking 设为 True 会阻塞，并立即返回 False ；否则，将锁锁定并返回 True。\ntimeout：当浮点型 timeout 参数被设置为正值调用时，只要无法获得锁，将最多阻塞 timeout 设定的秒数。timeout 参数被设置为 -1 时将无限等待。当 blocking 为 false 时，timeout 指定的值将被忽略。如果成功获得锁，则返回 True，否则返回 False (例如发生 超时 的时候)。\n  release()\n释放一个锁。这个方法可以在任何线程中调用，不单指获得锁的线程。当锁被锁定，将它重置为未锁定，并返回。如果其他线程正在等待这个锁解锁而被阻塞，只允许其中一个重置。在未锁定的锁调用时，会引发 RuntimeError 异常。没有返回值。\n  locked()\n如果获得了锁则返回真值。\n   创建一个锁对象：\n1 2  import threading my_lock = threading.Lock()   获得锁和释放锁：\n1 2 3 4  #获得锁 my_lock.acquire() #释放锁 my_lock.release()   　当操作共享的数据时，需要保证原子操作，否则可能会得到意想不到的结果，锁用来保证原子操作。下面的示例代码展示了当没有锁保证原子性操作时可能得到的错误结果。\n　下面的代码预期的结果是0，但不加锁的情况下会导致出现意外的输出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  # -*- coding: utf-8 -*- #thread_lock.py #来源：https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/chapter2/06_Thread_synchronization_with_Lock_and_Rlock.html import threading shared_resource_with_lock = 0 shared_resource_with_no_lock = 0 COUNT = 100000 shared_resource_lock = threading.Lock() # 有锁的情况 def increment_with_lock(): global shared_resource_with_lock for i in range(COUNT): shared_resource_lock.acquire() shared_resource_with_lock += 1 shared_resource_lock.release() def decrement_with_lock(): global shared_resource_with_lock for i in range(COUNT): shared_resource_lock.acquire() shared_resource_with_lock -= 1 shared_resource_lock.release() # 没有锁的情况 def increment_without_lock(): global shared_resource_with_no_lock for i in range(COUNT): shared_resource_with_no_lock += 1 def decrement_without_lock(): global shared_resource_with_no_lock for i in range(COUNT): shared_resource_with_no_lock -= 1 if __name__ == \u0026#34;__main__\u0026#34;: t1 = threading.Thread(target=increment_with_lock) t2 = threading.Thread(target=decrement_with_lock) t3 = threading.Thread(target=increment_without_lock) t4 = threading.Thread(target=decrement_without_lock) t1.start() t2.start() t3.start() t4.start() t1.join() t2.join() t3.join() t4.join() print (\u0026#34;the value of shared variable with lock management is %s\u0026#34; % shared_resource_with_lock) print (\u0026#34;the value of shared variable with race condition is %s\u0026#34; % shared_resource_with_no_lock)   上述代码的几次输出结果：\n1 2 3 4 5 6 7 8 9  #1 the value of shared variable with lock management is 0 the value of shared variable with race condition is 32622 #2 \u0026amp; 3 \u0026amp; 4 the value of shared variable with lock management is 0 the value of shared variable with race condition is 0 #5 the value of shared variable with lock management is 0 the value of shared variable with race condition is -8627   　产生这种输出的原因是，在没有锁的情况下，线程的切换是由Python控制的，不一定执行到什么时候就切换线程了，导致线程切换前后共享变量的值不一致。\n　举例来说，可能执行到时刻A时，t4运行到shared_resource_with_no_lock -= 1时切换到了t3，运行shared_resource_with_no_lock += 1，执行了很多次shared_resource_with_no_lock += 1，运行到时刻B切换回t4,但此时shared_resource_with_no_lock -= 1中的shared_resource_with_no_lock已经不是时刻A的shared_resource_with_no_lock，导致被减数增大了，结果就会与预期不符，产生第一次输出的结果。\n　但t4加上锁之后，即使切换到t3，由于锁已经被获得，t3会被阻塞，又会切回到t4，从而保证了在线程切换时shared_resource_with_lock的值不变。\n3.3 信号量对象 　用于保护数量有限的资源，在资源数量固定的任何情况下，都应该使用有界信号量。在生成任何工作线程前，应该在主线程中初始化信号量。还没用过，暂时不介绍了，用法和锁相似，只不过信号量计数。\n3.4 队列 　注意进程、线程、协程都有队列，且定义在不同的文件里，这里指线程队列。\n　线程用的队列就是queue模块的Queue对象。\n 文档：https://docs.python.org/zh-cn/3/library/queue.html\nqueue.Queue（先入先出，还有LifoQueue和PriorityQueue）\n qsize()：返回队列的大致大小。 empty()：队列为空返回True。 full()：队列满返回True。 put(item, block=True, timeout=None)：将item放入队列。  如果可选参数 block 是 true 并且 timeout 是 None (默认)，则在必要时阻塞至有空闲插槽可用。如果 timeout 是个正数，将最多阻塞 timeout 秒，如果在这段时间没有可用的空闲插槽，将引发 Full 异常。 如果可选参数 block 是 false，如果空闲插槽立即可用，则把 item 放入队列，否则引发 Full 异常 ( 在这种情况下，timeout 将被忽略)。   put_nowait(item)：相当于put(item, False)。 get(block=True,timeout=None)：从队列中移除并返回一个项目。  如果可选参数 block 是 true 并且 timeout 是 None (默认值)，则在必要时阻塞至项目可获得。如果 timeout 是个正数，将最多阻塞 timeout 秒，如果在这段时间内项目不能得到，将引发 Empty 异常。 如果可选参数 block 是 false , 如果一个项目立即可得到，则返回一个项目，否则引发 Empty 异常 (这种情况下，timeout 将被忽略)。   get_nowait()：相当于get(False)。 task_done()：表示前面排队的任务已经被完成。被队列的消费者线程使用。每个 get() 被用于获取一个任务， 后续调用 task_done() 告诉队列，该任务的处理已经完成。如果 join() 当前正在阻塞，在所有条目都被处理后，将解除阻塞(意味着每个 put() 进队列的条目的 task_done() 都被收到)。如果被调用的次数多于放入队列中的项目数量，将引发 ValueError 异常 。 join()：阻塞至队列中所有的元素都被接收和处理完毕。   创建一个队列：\n1 2  import queue q = queue.Queue()   线程间使用队列通信：\n 来源：https://www.cnblogs.com/Keep-Ambition/p/7597664.html\n 　修改了来源中consumer中count+1的位置，因为会出现队列为空的情况，会导致执行了else语句，没有消费到东西，却对count+1了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # thread_queue.py import time,random import queue,threading q = queue.Queue() def producer(name): count = 0 while count \u0026lt; 20: time.sleep(random.randrange(3)) q.put(count) # 生产item到队列 print(\u0026#39;Producer %shas produced %sitem..\u0026#39; % (name, count)) count += 1 def consumer(name): count = 0 while count \u0026lt; 20: time.sleep(random.randrange(4)) if not q.empty(): # 如果还有item data = q.get() # 就继续获取item #\u0026#39;\\033[显示方式;前景色;背景色m\u0026lt;输出内容\u0026gt;\\033[0m\u0026#39;输出有文字颜色和背景色的内容，其中\u0026#39;\\033[0m\u0026#39;表示该颜色结束 print(\u0026#39;\\033[32;1mConsumer %shas eat %sitem...\\033[0m\u0026#39; % (name, data)) count += 1 q.task_done() else: print(\u0026#34;waiting...\u0026#34;) p1 = threading.Thread(target=producer, args=(\u0026#39;A\u0026#39;,)) c1 = threading.Thread(target=consumer, args=(\u0026#39;B\u0026#39;,)) p1.start() c1.start() q.join()   　输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  Producer A has produced 0 item.. Producer A has produced 1 item.. Producer A has produced 2 item.. Consumer B has eat 0 item... Producer A has produced 3 item.. Producer A has produced 4 item.. Consumer B has eat 1 item... Producer A has produced 5 item.. Consumer B has eat 2 item... Producer A has produced 6 item.. Producer A has produced 7 item.. Producer A has produced 8 item.. Producer A has produced 9 item.. Producer A has produced 10 item.. Producer A has produced 11 item.. Producer A has produced 12 item.. Consumer B has eat 3 item... Producer A has produced 13 item.. Consumer B has eat 4 item... Producer A has produced 14 item.. Producer A has produced 15 item.. Producer A has produced 16 item.. Consumer B has eat 5 item... Consumer B has eat 6 item... Producer A has produced 17 item.. Producer A has produced 18 item.. Producer A has produced 19 item.. Consumer B has eat 7 item... Consumer B has eat 8 item... Consumer B has eat 9 item... Consumer B has eat 10 item... Consumer B has eat 11 item... Consumer B has eat 12 item... Consumer B has eat 13 item... Consumer B has eat 14 item... Consumer B has eat 15 item... Consumer B has eat 16 item... Consumer B has eat 17 item... Consumer B has eat 18 item... Consumer B has eat 19 item...   4.参考代码 　地址：https://github.com/yyyIce/py_exercise/tree/main/threading\n　有帮助的话可以点个Star⭐哦~\n参考资料 1.管道和队列的选择：https://qastack.cn/programming/8463008/multiprocessing-pipe-vs-queue\n2.可调用对象：https://zhuanlan.zhihu.com/p/101792911\n3.线程调度机制：https://my.oschina.net/u/240562/blog/137040\n4.Python输出有颜色和背景色的文字：https://o-u-u.com/?p=2940\n","description":"Python多线程编程的理解与应用","id":24,"section":"posts","tags":["python"],"title":"Python：多线程编程","uri":"https://yyyIce.github.io/zh/posts/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"},{"content":"\r 本文是多进程和协程的综合使用示例，至少2个核才能充分达到速度要求，实际工程中这么用好不好暂时不清楚。 这里仅给出简要代码，可以自行添加日志、配置文件等功能。  \r \r整体设计 进程1：从kafka的CONSUME_TOPIC中消费消息放入进程队列msg_q，然后修改消息内容生产到PRODUCE_TOPIC中。通过一个协程打印消费生产速度。\n进程2：从进程队列msg_q中取出消息，根据消息内容异步发送HTTP请求，对于失败的请求，将消息重新放回队列。通过一个协程打印获取响应的速度。\n【图】\n代码编写 需要注意的问题 队列为空时进程将阻塞，需要进行检查和处理\n同步问题，修改偏移量需要在同一次控制权中修改\n参考代码 　地址：\n","description":"综合进程和协程进行使用","id":25,"section":"posts","tags":["python"],"title":"Python：多进程+协程编程示例","uri":"https://yyyIce.github.io/zh/posts/python%E5%A4%9A%E8%BF%9B%E7%A8%8B+%E5%8D%8F%E7%A8%8B%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B/"},{"content":"\r Python文档：https://docs.python.org/zh-cn/3/library/multiprocessing.html 可以先参看他人博客，大概了解以后看官方文档进行使用，不建议直接看文档进行学习。  \r \r1 multiprocessing  multiprocessing可以并发操作，通过子进程有效地绕过了全局解释器锁，允许程序员充分利用给定机器上的多个处理器，在Unix和Windows上均可运行。\n源代码：https://github.com/python/cpython/tree/3.9/Lib/multiprocessing/\n其中Process类在process.py文件中。\n 　Process和threading.Thread的API相同。\n class multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, ***, daemon=None)\n  group：不用管，保留 target：run()方法调用的可调用对象 name：线程名称，默认格式为Thread-N args：调用目标函数的参数元组 kwargs：用于调用目标函数的关键字参数字典 daemon：设置线程是否是守护模式，不建议使用setDaemon()  　创建一个进程：\n1 2 3  from multiprocessing import Process if __name__ == \u0026#39;__main__\u0026#39;: p = Process(target=\u0026lt;可调用对象\u0026gt;,args=(\u0026lt;可调用对象的参数\u0026gt;),...)   　启动一个进程：\n1 2 3 4  from multiprocessing import Process if __name__ == \u0026#39;__main__\u0026#39;: p = Process(target=\u0026lt;可调用对象\u0026gt;,args=(\u0026lt;可调用对象的参数\u0026gt;),...) p.start()   　开始进程活动：\n\u0026lt;Process\u0026gt;.run()，.run()方法代码如下，和线程相似，把活动写成函数在初始化时赋值给target即可。\n1 2 3 4 5 6 7  #Lib/multiprocessing/process.py def run(self): \u0026#39;\u0026#39;\u0026#39; Method to be run in sub-process; can be overridden in sub-class \u0026#39;\u0026#39;\u0026#39; if self._target: self._target(*self._args, **self._kwargs)   　启动进程的方法\n spawn：只继承运行run()方法所必要的资源，比fork和forkserver慢。Windows和Unix上都可用。 fork：父进程使用os.fork()，只能在Unix上用。 forkserver：需要新进程时，父进程连接服务器，请求它分出一个新进程。可在Unix上使用。  　使用方法：在if __name__ == '__main__':中调用set_start_method()，这个方法不应该被多次调用。\n1 2 3 4 5 6 7 8 9 10 11 12  import multiprocessing as mp def foo(q): q.put(\u0026#39;hello\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: mp.set_start_method(\u0026#39;fork\u0026#39;) q = mp.Queue() p = mp.Process(target=foo, args=(q,)) p.start() print(q.get()) p.join()   　进程调度\n　进程优先在不同的核（CPU）上运行，比如有CPU1，CPU2，CPU3和进程1-6，则进程可能是1、4在CPU1上运行，2、5在CPU2上运行，3、6在CPU3上运行（不一定是按这种顺序分布，但会在不同的核上，也可能1和6在同一个核上）。multiprocessing.cpu_count()查看当前核数。\n1 2  import multiprocessing print(multiprocessing.cpu_count())   　可以使用psutil查看进程信息，这里利用pstutil的cpu_num()方法查看当前进程运行在哪个CPU上，编号从0开始，使用方法参考下面的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import psutil import os import multiprocessing from multiprocessing import Process def producer(name): //... pass if __name__ == \u0026#39;__main__\u0026#39;: p1 = Process(target=producer, args=(\u0026#39;A\u0026#39;,)) p1.start() parent = psutil.Process(os.getpid()) print(\u0026#34;main: %d\u0026#34; % parent.cpu_num())   2 进程间通信 　multiprocessing支持进程间两种通信通道：队列（multiprocessing.Queue和multiprocessing.Pipe）和管道。\n　并发编程尽量避免使用共享状态，使用队列和管道进行通信，本次学习进程的目的本身就是为了并发编程，所以这里不介绍共享状态。\n2.1 队列和管道的选择   Pipe只能有两个端点，Queue可以有多个生产者和消费者\n  如果需要两个以上的交流点，使用Queue；需要缓冲数据时，队列是比管道更好的选择。\n  如果想要绝对的性能，选用Pipe()，因为Queue()建立在Pipe()之上。\n  2.2 进程队列 　multiprocessing.Queue和queue.Queue近似，但进程Queue没有join()和task_done()方法，但它有join_thread()方法。\n 文档：https://docs.python.org/zh-cn/3/library/multiprocessing.html?highlight=queue#pipes-and-queues\nqueue.Queue（先入先出，还有LifoQueue和PriorityQueue）\n qsize()：返回队列的大致大小。 empty()：队列为空返回True。 full()：队列满返回True。 put(obj[, block[, timeout]])：将obj放入队列。  如果可选参数 block 是 true 并且 timeout 是 None (默认)，则在必要时阻塞至有空闲插槽可用。如果 timeout 是个正数，将最多阻塞 timeout 秒，如果在这段时间没有可用的空闲插槽，将引发 queue.Full 异常。 如果可选参数 block 是 false，如果空闲插槽立即可用，则把 item 放入队列，否则引发 Full 异常 ( 在这种情况下，timeout 将被忽略)。   put_nowait(item)：相当于put(item, False)。 get([block[, timeout]])：从队列中移除并返回一个项目。  如果可选参数 block 是 true 并且 timeout 是 None (默认值)，则在必要时阻塞至项目可获得。如果 timeout 是个正数，将最多阻塞 timeout 秒，如果在这段时间内项目不能得到，将引发 Empty 异常。 如果可选参数 block 是 false , 如果一个项目立即可得到，则返回一个项目，否则引发 Empty 异常 (这种情况下，timeout 将被忽略)。   get_nowait()：相当于get(False)。 close()：指示当前进程将不会再往队列中放入对象。 join_thread()：等待后台线程，仅在调用了close()方法后可用。阻塞当前进程，直到后台线程退出，确保所有缓冲区的数据都被写入管道中。 cancel_join_thread()：防止join_thread()方法阻塞当前进程。如果该进程不是队列的创建者，不需要等待队列的后台线程退出，此时调用这个方法可以让join_thread()方法什么都不做直接跳过。   　队列的简单使用：\n1 2 3 4 5 6 7 8 9 10 11 12  # multiprocess_queue.py from multiprocessing import Process, Queue def f(q): q.put([42, None, \u0026#39;hello\u0026#39;]) if __name__ == \u0026#39;__main__\u0026#39;: q = Queue() p = Process(target=f, args=(q,)) p.start() print(q.get()) # prints \u0026#34;[42, None, \u0026#39;hello\u0026#39;]\u0026#34; p.join()   　生产者消费者模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # multiprocess_queue_pc.py import time,random from multiprocessing import Process, Queue def producer(name): count = 0 while count \u0026lt; 20: time.sleep(random.randrange(3)) q.put(count) # 生产item到队列 print(\u0026#39;Producer %shas produced %sitem..\u0026#39; % (name, count)) count += 1 def consumer(name): count = 0 while count \u0026lt; 20: time.sleep(random.randrange(4)) if not q.empty(): # 如果还有item data = q.get() # 就继续获取item #\u0026#39;\\033[显示方式;前景色;背景色m\u0026lt;输出内容\u0026gt;\\033[0m\u0026#39;输出有文字颜色和背景色的内容，其中\u0026#39;\\033[0m\u0026#39;表示该颜色结束 print(\u0026#39;\\033[32;1mConsumer %shas eat %sitem...\\033[0m\u0026#39; % (name, data)) count += 1 else: print(\u0026#34;waiting...\u0026#34;) q = Queue() p1 = Process(target=producer, args=(\u0026#39;A\u0026#39;,)) c1 = Process(target=consumer, args=(\u0026#39;B\u0026#39;,)) p1.start() c1.start()   JoinableQueue 　Queue队列没有join()和task_done()方法，在JoinableQueue中有这两个方法，JoinableQueue是Queue的子类。\n　经使用发现，Queue队列会出现队列中还有未处理元素却退出的情况，这时需要使用JoinableQueue()。\n  task_done：对于每次调用get()获取的任务，执行完成后调用task_done()告诉队列该任务已经处理完成。\n  join()：阻塞至队列中所有的元素都被接收和处理完毕。\n  3 进程池 　暂时用不到，不整理了，参考：https://blog.csdn.net/SeeTheWorld518/article/details/49639651\n4 参考代码 　地址：https://github.com/yyyIce/py_exercise/tree/main/multiprocessing\n　有帮助的话可以点个Star⭐哦~\n参考资料 　管道和队列的选择：https://qastack.cn/programming/8463008/multiprocessing-pipe-vs-queue\n","description":"对Python多进程的理解与应用","id":26,"section":"posts","tags":["python"],"title":"Python：多进程编程","uri":"https://yyyIce.github.io/zh/posts/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%BC%96%E7%A8%8B/"},{"content":"\r 原文：https://realpython.com/async-io-python/ 这个看原英文挺容易看懂的，在看的同时顺便翻译出来供英语属实不行的人使用。  \r \r　异步IO（Async IO）是并发编程设计，它在Python中的到支持，在Python3.4-3.7以及以后的版本中快速发展。你可能会对“并发，并行，线程，多进程”感到迷惑，需要掌握太多知识，异步IO属于哪个？\n　这个教程可以帮助你解答这个问题，并帮助你掌握Python中异步IO的用法。\n　你将学到：\n 异步IO：与语言无关的范式（模型），具有跨多种编程语言的实现 async/await：两种新的Python关键字，用于定义协程（coroutines） asyncio：提供运行和管理协程的基础和API的Python包  　协程**（特殊的生成器函数**）是异步IO的核心，我们稍后会深入探讨它们。\n　注意：在这篇文章中，我使用术语async IO表示异步IO的语言无关设计，而asyncio指的是Python包。\n　在开始之前，你应该确认你已经安装了asyncio和这篇教程中的其它库。\n安装环境 　你需要Python3.7或更高版本、aiohttp包和aiofiles包才能完整地完成教程。\n1 2 3  $python3.7 -m venv ./py37async $source ./py37async/bin/activate # Windows: .\\py37async\\Scripts\\activate.bat $pip install --upgrade pip aiohttp aiofiles # Optional: aiodns   　安装Python3.7和虚拟环境，访问：https://realpython.com/installing-python/或https://realpython.com/python-virtual-environments-a-primer/\n　下面就让我们开始吧。\n10000英尺的异步IO视图 　异步IO没有它的兄弟多进程和线程知名。本节将为您提供更全面的了解异步IO是什么以及它如何适应周围的环境。\n异步IO适用于什么场景？ 　并发和并行是不容易涉足的扩展主题。但这篇文章专注于异步IO及其在Python中的实现，值得花一点时间将异步IO与相似的概念进行比较，以了解异步IO如何适应更大、有时令人困惑的难题。\n　并行（Parallelism）由同时执行的多个操作组成。多进程（Multiprocessing）是一种实现并行的方法，并且它需要把任务分散到CPU的中央处理单元上（CPU或核）。多进程适合CPU密集任务：紧绑定循环和数学计算通常属于此类。\n　并发（Concurrency）是比并行边界更松弛的术语。它表明多个任务可以以重叠的方式运行。（有一种说法是并发不一定是并行）\n　线程（Threading）是并发执行的模型，多个线程轮流执行任务。一个进程可以包含多个线程。Python由于它的全局锁（GIL）具有复杂的线程关系，但是那超过了这篇文章的范围。\n　关于线程需要了解的是它更适合IO密集型任务。CPU密集型任务的特征是计算机内核从头到尾不断地努力工作，而IO密集型工作主要由大量等待输入/输出来完成。\n　回顾上面描述的，并发包含多进程（CPU密集型任务）和多线程（IO密集型任务）。多进程是并行的一种形式，并行是并发的特殊类型（子集）。Python标准库通过它的multiprocessing，threading和concurrent.futures包为他们提供了长久的支持。\n　现在是时候带来一个新成员了。在过去的几年中，单独的设计已更全面地内置到CPython中：异步IO，通过标准库的asyncio包和新的async和await语言关键字启动。进一步说，异步IO不是新提出的概念，它已经存在并构建到了其它语言和运行时环境中，比如Go，C#，或Scala。\n　asyncio包被Python文档记为用于编写并发代码的库。然而，异步IO不是多线程也不是多进程。它不属于上述的任何一个。\n　事实上，异步IO是单线程，单进程设计：它使用多任务协作模式，你会在文章结尾时完全了解这个术语。换句话来说异步IO尽管使用单进程中的单线程，仍然给人一种并发的感觉。协程（Coroutines，异步IO的核心功能）能够并发调度，但它们内部不是并发。\n　重申一次，异步IO是并发编程的一种风格，但它不是并行。与多进程相比，它与线程处理的关系更紧密，但两者截然不同，并且是并发技巧中的独立成员。\n还剩下一个术语。异步是什么意思？这不是一个严格的定义，但在我们的语境中，我认为它有两种属性：\n 异步例程能够在等待他们最终结果的时候“暂停”，并且同时让其他例程运行。 通过以上机制，异步代码有助于并发执行。换句话说，异步代码给出了并发的视觉效果。  　下面是一张把他们放在一起的图。白色的术语代表概念，绿色的术语代表他们的实现方式：\n图1 并行\u0026amp;并发:\r\r 　我将在这里停止并发编程模型之间的比较，这篇教程专注于它的子成分——异步IO的子组件，如何使用异步IO，和围绕它兴起的API。要全面了解线程，多处理和异步IO，请在此处暂停并查看Jim Anderson的Python并发概述。Jim比我有趣得多，也比我参加了更多的会议。\n异步IO解释 　异步IO似乎是违反直觉且矛盾的。如何使用单线程和单核实现并发代码？我从来都不擅长编造例子，所以我从Miguel Grinberg 2017年PyCon的演讲中取一个例子，他将所有的事情都解释得很漂亮。\n 国际象棋大师朱迪特·波尔加（Judit Polgár）举办国际象棋展览，她在展览中与多个业余玩家对战。她有两种进行展览得方式：同步和异步。\n假设条件：\n 24位对手 Judit每次在5秒内落子 对手每次在55秒内落子 每局游戏平局30对落子（一方移动棋子30次）  同步版本：Judit一次只玩一局，绝不同时进行两局，直至游戏完成。每局游戏花费(55 + 5) * 30 = 1800 秒，即30分钟。整个展览需要花费24 * 30 = 720 分钟，即12小时。\n异步版本：每张桌子代表一局游戏，Judit在每张桌子上落子一次，然后从一张桌子移动到另一张。她离开桌子并让对手在等待的时间进行下一次落子。在全部24局游戏中落子1次花费Judit 24 * 5 = 120 秒，或2分钟。总共的展览时间现在被缩减到12 * 30 = 3600秒，即一小时。\n来源：https://youtu.be/iG6fr81xHKA?t=4m29s\n 　上述例子中Judit只有一位，她只有两只手，并且同一时刻只能在一局游戏上落子一次。但是异步对局将展览时间从12小时缩短至了1小时。所以，协作式多任务是一种优雅的方式，即程序的事件循环（稍后将详细描述）和多任务进行通信，让每个任务可以在最优时间轮流运行。\n　异步IO需要较长的等待时间，在该时间内函数将被阻塞，并允许其他功能在停机期间运行。（有效阻塞的函数从开始到返回为止一直禁止其他人运行）\n异步IO并不简单 　我听说，“当你有选择的时候使用异步IO；当你必须使用的时候选择线程。”事实就是构建耐用的多线程代码是困难且易出错的。异步IO避免了一些在线程设计中会遇到的潜在的速度颠簸。\n　但这并不是说Python中的异步IO很简单。请注意：当你试图进入接口层以下时，异步编程也会很困难！Python的异步模型构建在回调（callbacks），事件（events），传输（transports），协议（protocols）和期货（futures）的基础上（仅仅是术语就令人困惑），实际上由于它持续变化的API让它更不简单。\n　幸运的是，asyncio已经到了一个成熟的地步，它的大多数功能都不再是临时的，而它的文档进行了大幅度的修改，一些关于它的优质资源也开始出现。\nasyncio包和async/await 　现在你已经对异步IO的背景有了一定的了解，让我们探索一下Python的实现。Python的asyncio包（在Python 3.4中介绍）并且它有两个关键词，async和await，具有不同的功能，但它们一起帮你声明、构建、执行并管理异步代码。\nasync/await语法和原生协程 　告诫：请您注意在互联网上阅读的内容。Python的异步IO API从Python 3.4到 Python3.7迅速变化。一些老的模式不再使用，并且在新的文档中旧的一些将被禁止。据我所知，这篇教程很快也成为过时大军的一员。\n　异步IO的核心是协程。协程是Python生成器函数的特殊版本。让我们从基础定义开始，然后在它的基础上构建一个程序：协程是一个函数，它能够在到达return之前暂停它的执行，并且它可以在一段时间内将控制权间接传递给其它协程。\n　稍后，你会更深入地了解到传统生成器是怎么重新定义为协程的。但现在，最简单的了解协程工作方式的方法就是创建一些协程。\n　让我们用沉浸式方法并编写一些异步IO代码。这个程序相当于异步IO的Hello World，但距离说明它的核心功能还有很长的路要走：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  #!/usr/bin/env python3 # countasync.py import asyncio async def count(): print(\u0026#34;One\u0026#34;) await asyncio.sleep(1) print(\u0026#34;Two\u0026#34;) async def main(): await asyncio.gather(count(), count(), count()) if __name__ == \u0026#34;__main__\u0026#34;: import time s = time.perf_counter() asyncio.run(main()) elapsed = time.perf_counter() - s print(f\u0026#34;{__file__} executed in {elapsed:0.2f} seconds.\u0026#34;)   　当你执行这个文件，注意和你用def和time.sleep()定义的函数有哪些不同：\n1 2 3 4 5 6 7 8  $ python3.7 countasync.py One One One Two Two Two countasync.py executed in 1.01 seconds.   　输出的顺序就是异步IO的核心。与count()的每个调用进行交流的是一个事件循环或协调器。当每个任务到达await asyncio.sleep(1)，函数会大喊通知事件循环并归还控制权给它，并说“我将睡眠1秒，请让其它有意义的事情在这个时间段执行。”\n　将它和同步版本比较：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  #!/usr/bin/env python3 # countsync.py import time def count(): print(\u0026#34;One\u0026#34;) time.sleep(1) print(\u0026#34;Two\u0026#34;) def main(): for _ in range(3): count() if __name__ == \u0026#34;__main__\u0026#34;: s = time.perf_counter() main() elapsed = time.perf_counter() - s print(f\u0026#34;{__file__} executed in {elapsed:0.2f} seconds.\u0026#34;)   　当执行时，在执行顺序和时间上有轻微但关键的不同：\n1 2 3 4 5 6 7 8  $ python3.7 countsync.py One Two One Two One Two countsync.py executed in 3.01 seconds.　  　虽然使用time.sleep()和asyncio.sleep()看似平庸，但它们可以用作涉及等待时间的所有耗时的过程的替身。（你可以进行等待的最平凡的事情就是调用sleep()，因为它什么都不做。）也就是说，time.sleep()可以代表任何耗时的阻塞函数调用，而asyncio.sleep()可以用来代表非阻塞调用（但是仍需要花费一些时间去完成）。\n　你在下一节将看到awaiting一些事情的好处，包括asyncio.sleep()，即周围的函数可以暂时地将控制权交给其他函数（这些函数已经就绪，能立即开始做某些事情）。相对地，time.sleep()或任何其它阻塞调用在Python中是和异步代码不兼容的，因为它会在睡眠期间停止执行过程中的任何事情。\n异步IO的规则 　在这一段中，会依次介绍async，await，和协程函数的正式定义。这一节有一点枯燥，但对于掌握async/await是有帮助的，所以如果需要的话请回顾这一节：\n async def的语法介绍了原生协程（native coroutine）或者叫异步生成器（asynchronous generator）。表达式async with和async for也是合法的，稍后你将看到它们。 关键字await将函数控制权交还给事件循环（它暂停当前协程的执行状态）。如果Python在g()范围内遇到了一个await f()表达式，await就会这样告诉事件循环，“停止g()的执行，直到我等到了f()的执行结果，即f()返回。在等待期间，请让其他的事情运行。”  　第二点用代码大概像下面这样表示：\n1 2 3 4  async def g(): # Pause here and come back to g() when f() is ready r = await f() return r   　关于何时、如何、以及不能使用async/await有一套严格的规则，无论你在学习语法或已经有使用async/await的经验，这些都非常有用。\n 你用async def定义的函数就是一个协程。它可能会使用await，return，或是yield，这些都是可选的。声明async def noop():pass也是有效的：  使用await和/或return创建一个协程函数。为调用协程函数，你必须使用await来获得需要等待的结果。 比较少见的是在async def函数体中使用yield（只有最近才在Python中合法）。这创建了一个异步生成器，你会使用async for迭代它。暂时忘记异步生成器，并专注于获取协程函数的语法，使用await和/或return。   就像在def函数外使用yield是一个SyntaxError一样，在async def协程外使用await也是一个SyntaxError。你只能在协程内部使用await。  　下面是一些总结上面几条规则的简洁示例。\n1 2 3 4 5 6 7 8 9 10 11 12 13  async def f(x): y = await z(x) # OK - `await` and `return` allowed in coroutines return y async def g(x): yield x # OK - this is an async generator async def m(x): yield from gen(x) # No - SyntaxError def m(x): y = await z(x) # Still no - SyntaxError (no `async def` here) return y   　最后，当你使用await f()时，它需要f()是一个可等待的对象。听起来好像并不是十分有用。你仅需要知道一个可等待对象是（1）另一个协程，或（2）一个定义了.__await__()方法的对象，该方法返回一个迭代器。如果你编写了一个程序，对于大多数目的来说，你应该只需要考虑case#1。\n　这带给我们另一个你需要理解的技术上的区别：以前标识一个协程的方法是使用装饰器，即在标准def函数定义前用@asyncio.coroutine装饰。这个定义结果是基于生成器的协程（generator-based coroutine）。在Python3.5中提出async/await以后，这种用法已经过时了。\n　这两种协程基本上是等效的（都是可等待的），但是第一个是基于生成器的，第二个是原生协程（native coroutine）。\n1 2 3 4 5 6 7 8 9 10  import asyncio @asyncio.coroutine def py34_coro(): \u0026#34;\u0026#34;\u0026#34;Generator-based coroutine, older syntax\u0026#34;\u0026#34;\u0026#34; yield from stuff() async def py35_coro(): \u0026#34;\u0026#34;\u0026#34;Native coroutine, modern syntax\u0026#34;\u0026#34;\u0026#34; await stuff()   　如果你自己正在写任何代码，最好使用原生协程，因为显式比隐式更好。基于生成器的协程将在Python3.10中被移除。\n　在本篇教程的后半部分，我们会使用基于生成器的协程，仅仅是用于解释。引入async/await的原因是为了使协程称为Python的独立功能，从而很容易地将它和普通的生成器函数区分开，从而减少了歧义。\n　不要在基于生成器的协程上越陷越深，他已经被async/await淘汰了。它们有它们自己的一组小规则（例如，await不能被使用在基于协程的生成器中），如果您坚持使用async/await语法，很大程度上是和它们无关的。\n　事不宜迟，让我们举更多的例子。\n　这有一个异步IO是怎样缩减等待时间的例子：给定一个协程makerandom()，它持续生产范围为（0，10）的随机数，直到其中一个超过阈值为止，你要让此协程的多个调用不需要彼此等待依次完成。你可以大部分遵循上述两种脚本模式，并进行一些更改：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #!/usr/bin/env python3 # rand.py import asyncio import random # ANSI colors c = ( \u0026#34;\\033[0m\u0026#34;, # End of color \u0026#34;\\033[36m\u0026#34;, # Cyan \u0026#34;\\033[91m\u0026#34;, # Red \u0026#34;\\033[35m\u0026#34;, # Magenta ) async def makerandom(idx: int, threshold: int = 6) -\u0026gt; int: print(c[idx + 1] + f\u0026#34;Initiated makerandom({idx}).\u0026#34;) i = random.randint(0, 10) while i \u0026lt;= threshold: print(c[idx + 1] + f\u0026#34;makerandom({idx}) == {i} too low; retrying.\u0026#34;) await asyncio.sleep(idx + 1) i = random.randint(0, 10) print(c[idx + 1] + f\u0026#34;---\u0026gt; Finished: makerandom({idx}) == {i}\u0026#34; + c[0]) return i async def main(): #*args,将参数打包为一个元组 res = await asyncio.gather(*(makerandom(i, 10 - i - 1) for i in range(3))) return res if __name__ == \u0026#34;__main__\u0026#34;: random.seed(444) r1, r2, r3 = asyncio.run(main()) print() print(f\u0026#34;r1: {r1}, r2: {r2}, r3: {r3}\u0026#34;)   　彩色输出的内容比我说的要多得多，使你对如何执行此脚本有一定的了解。\n图2 rand.py运行结果:\r\r 　程序使用一个主协程，makerandom()，并以三种不同的输入同时运行它。大多数程序会包含小型模块化协程和一个包装函数，该包装函数能够把每个小协程链在一起。main()通过把中心协程映射到一些迭代器或池中来收集任务（futures）。\n　在这个微型示例中，池是range(3)。稍后我们会展示一个更完整的示例，它需要并发处理请求、解析一组URL，并且main()为每个URL封装了整个例程。\n　尽管“生产随机数（主要受限于CPU）”使用asyncio不是一个最佳选择，该示例中存在asyncio.sleep()的目的是模仿一个IO受限进程，其中涉及不确定的等待时间。例如，asyncio.sleep()调用可能代表消息应用中发送和接收两个客户端之间的非随机整数。\n异步IO设计模式 　异步IO有它自己的一组脚本设计，将在这一节为你介绍。\n链式协程 　协程的关键功能是它们能够被链接在一起。（记住，协程对象是可等待的，所以另一个协程可以await它。）这让你能够将程序分割成更小的、可管理的、易于回收的协程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  #!/usr/bin/env python3 # chained.py import asyncio import random import time \u0026#39;\u0026#39;\u0026#39; n:int，n是传入参数，int是注释，注释参数n应该是一个整型，但传入的n不是整型也不会报错， -\u0026gt;str，是返回值的注释，表示返回值是字符串 \u0026#39;\u0026#39;\u0026#39; async def part1(n: int) -\u0026gt; str: i = random.randint(0, 10) #以f开头表示在字符串内支持大括号内的python表达式 print(f\u0026#34;part1({n}) sleeping for {i} seconds.\u0026#34;) await asyncio.sleep(i) result = f\u0026#34;result{n}-1\u0026#34; print(f\u0026#34;Returning part1({n}) == {result}.\u0026#34;) return result async def part2(n: int, arg: str) -\u0026gt; str: i = random.randint(0, 10) print(f\u0026#34;part2{n, arg} sleeping for {i} seconds.\u0026#34;) await asyncio.sleep(i) result = f\u0026#34;result{n}-2 derived from {arg}\u0026#34; print(f\u0026#34;Returning part2{n, arg} == {result}.\u0026#34;) return result async def chain(n: int) -\u0026gt; None: start = time.perf_counter() p1 = await part1(n) p2 = await part2(n, p1) end = time.perf_counter() - start print(f\u0026#34;--\u0026gt;Chained result{n} =\u0026gt; {p2} (took {end:0.2f} seconds).\u0026#34;) async def main(*args): await asyncio.gather(*(chain(n) for n in args)) if __name__ == \u0026#34;__main__\u0026#34;: import sys random.seed(444) args = [1, 2, 3] if len(sys.argv) == 1 else map(int, sys.argv[1:]) start = time.perf_counter() asyncio.run(main(*args)) end = time.perf_counter() - start print(f\u0026#34;Program finished in {end:0.2f} seconds.\u0026#34;)   　注意输出，part1()睡眠可变时间，并且part2()开始处理可用的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ python3.7 chained.py 9 6 3 part1(9) sleeping for 4 seconds. part1(6) sleeping for 4 seconds. part1(3) sleeping for 0 seconds. Returning part1(3) == result3-1. part2(3, \u0026#39;result3-1\u0026#39;) sleeping for 4 seconds. Returning part1(9) == result9-1. part2(9, \u0026#39;result9-1\u0026#39;) sleeping for 7 seconds. Returning part1(6) == result6-1. part2(6, \u0026#39;result6-1\u0026#39;) sleeping for 4 seconds. Returning part2(3, \u0026#39;result3-1\u0026#39;) == result3-2 derived from result3-1. --\u0026gt;Chained result3 =\u0026gt; result3-2 derived from result3-1 (took 4.00 seconds). Returning part2(6, \u0026#39;result6-1\u0026#39;) == result6-2 derived from result6-1. --\u0026gt;Chained result6 =\u0026gt; result6-2 derived from result6-1 (took 8.01 seconds). Returning part2(9, \u0026#39;result9-1\u0026#39;) == result9-2 derived from result9-1. --\u0026gt;Chained result9 =\u0026gt; result9-2 derived from result9-1 (took 11.01 seconds). Program finished in 11.01 seconds.   　在这个设置中，main()的运行时间和它收集在一起并调度的运行时间最长的任务相同。\n使用队列 　asyncio包提供队列类，该类的涉及和queue模块中的类很相似。在我们目前的例子里，我们实际上并不需要队列结构。在chained.py中，每个任务（future）都有一组协程组成，每个协程都在彼此等待，并且每条链传递一个单独的输入。\n　有一个能够和异步IO共同工作的可选结构：有一定数量的生产者（它们彼此之间没有联系），向一个队列里添加事项。每个生产者在交错、随机、未通知的时间里可能向队列里添加多个事项。一组消费者在它们出现时贪婪地从队列中拉取事项，无需等待任何信号。\n　在这个设计中，没有任何单个消费者和生产者之间的链。消费者不知道生产者的数量，甚至也不会提前知道添加到队列中的事项的累积数量。\n　每个生产者或消费者花费可变的时间分别从队列中放入和提取事项。队列作为吞吐量，可以让生产者和消费者之间通信而无需他们彼此之间直接交流。\n\r\r尽管由于queue.Queue()的线程安全性，队列通常在线程程序中使用，但当涉及异步IO时，你不需要担心线程安全。（除了你试图结合这两者时，但在此篇教程中并未实现。）\n\r \r　一种队列的使用情形（如此处的情况）是队列作为生产者和消费者之间的传输者（transmitter），而不是将他们彼此相联系或直接链接起来。\n　这个程序的同步版本看起来相当惨淡：一组阻塞生产者串行将事项添加到队列，一次运行一个生产者。只有所有的生产者都完成生产，一次才能有一个消费者逐项处理队列。这个设计中有过多的延迟。事项可能闲置地放置在队列中，耳不是立即拿起并处理。\n　一个异步版本，asyncq.py，写在下面。这项工作的挑战部分是需要给消费者一个生产者已经完成生产的信号。否则await q.get()会无限期的挂起，因为队列已经被充分处理，但是消费者完全不知道生产已经完成。\n　（特别感谢来自StackOverflow用户在理清main()上的帮助：关键是await q.join()，直到队列中的所有事项都被接收和处理之前都会阻塞，然后取消消费者任务，否则它会挂起并无限等待额外的队列事项出现。）\n　下面是完整的脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  #!/usr/bin/env python3 # asyncq.py import asyncio import itertools as it import os import random import time async def makeitem(size: int = 5) -\u0026gt; str: return os.urandom(size).hex() async def randsleep(a: int = 1, b: int = 5, caller=None) -\u0026gt; None: i = random.randint(0, 10) if caller: print(f\u0026#34;{caller} sleeping for {i} seconds.\u0026#34;) await asyncio.sleep(i) async def produce(name: int, q: asyncio.Queue) -\u0026gt; None: n = random.randint(0, 10) for _ in it.repeat(None, n): # Synchronous loop for each single producer await randsleep(caller=f\u0026#34;Producer {name}\u0026#34;) i = await makeitem() t = time.perf_counter() await q.put((i, t)) print(f\u0026#34;Producer {name} added \u0026lt;{i}\u0026gt; to queue.\u0026#34;) async def consume(name: int, q: asyncio.Queue) -\u0026gt; None: while True: await randsleep(caller=f\u0026#34;Consumer {name}\u0026#34;) i, t = await q.get() now = time.perf_counter() print(f\u0026#34;Consumer {name} got element \u0026lt;{i}\u0026gt;\u0026#34; f\u0026#34; in {now-t:0.5f} seconds.\u0026#34;) q.task_done() async def main(nprod: int, ncon: int): q = asyncio.Queue() producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)] consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)] await asyncio.gather(*producers) await q.join() # Implicitly awaits consumers, too for c in consumers: c.cancel() if __name__ == \u0026#34;__main__\u0026#34;: import argparse random.seed(444) parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--nprod\u0026#34;, type=int, default=5) parser.add_argument(\u0026#34;-c\u0026#34;, \u0026#34;--ncon\u0026#34;, type=int, default=10) ns = parser.parse_args() start = time.perf_counter() asyncio.run(main(**ns.__dict__)) elapsed = time.perf_counter() - start print(f\u0026#34;Program completed in {elapsed:0.5f} seconds.\u0026#34;)   　前几个协程是辅助函数，返回一个随机字符串，一个分秒性能计数器和一个随机整数。生产者向队列中从1放到5个事项。每个事项是(i,t)元组，i是随机字符串，t是生产者试图将元组放入队列的时间。\n　当消费者拉取一个事项时，它仅使用事项放入队列时的时间戳计算事项放入队列中存在的时间。\n　记住asyncio.sleep()用来模仿其它更复杂的协程，如果这是常规的阻塞函数，会消耗时间并阻塞其他所有的执行。\n　这是用两个生产者和五个消费者的测试结果：（我实际运行的时候发现没有Producer 1，太奇怪了，有生产者1，生产者2，生产者3，就是没有1）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  $ python3.7 asyncq.py -p 2 -c 5 Producer 0 sleeping for 3 seconds. Producer 1 sleeping for 3 seconds. Consumer 0 sleeping for 4 seconds. Consumer 1 sleeping for 3 seconds. Consumer 2 sleeping for 3 seconds. Consumer 3 sleeping for 5 seconds. Consumer 4 sleeping for 4 seconds. Producer 0 added \u0026lt;377b1e8f82\u0026gt; to queue. Producer 0 sleeping for 5 seconds. Producer 1 added \u0026lt;413b8802f8\u0026gt; to queue. Consumer 1 got element \u0026lt;377b1e8f82\u0026gt; in 0.00013 seconds. Consumer 1 sleeping for 3 seconds. Consumer 2 got element \u0026lt;413b8802f8\u0026gt; in 0.00009 seconds. Consumer 2 sleeping for 4 seconds. Producer 0 added \u0026lt;06c055b3ab\u0026gt; to queue. Producer 0 sleeping for 1 seconds. Consumer 0 got element \u0026lt;06c055b3ab\u0026gt; in 0.00021 seconds. Consumer 0 sleeping for 4 seconds. Producer 0 added \u0026lt;17a8613276\u0026gt; to queue. Consumer 4 got element \u0026lt;17a8613276\u0026gt; in 0.00022 seconds. Consumer 4 sleeping for 5 seconds. Program completed in 9.00954 seconds.   　在这种情况下，事项在几秒钟就被处理完毕。延迟可能有两点原因：\n 标准的、很大程度上不可避免的开销 当队列中出现一项时，所有消费者都在睡眠的情况  　关于第二个理由，幸运的是，扩展到成百上千个消费者是完全正常的。在执行python3.7 asyncq.py -p 5 -c 100时应该不会出现问题。这里需要注意的是，理论上，你在不同的系统上会有不同的用户控制消费者和生产者的管理，队列作为中央吞吐量。\n　到目前为止，你已经陷入困境，并看到了三个相关的asyncio调用async和await协程的示例。如果你没有完全跟上或者仅仅想加深一下Python中现代协程机制的了解，你可以从下一章重新开始。\n异步IO在生成器中的根 　先前你已经看到了旧风格的基于生成器的协程，已经被更显式的原生协程淘汰。该示例值得稍作调整重新展示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import asyncio @asyncio.coroutine def py34_coro(): \u0026#34;\u0026#34;\u0026#34;Generator-based coroutine\u0026#34;\u0026#34;\u0026#34; # No need to build these yourself, but be aware of what they are s = yield from stuff() return s async def py35_coro(): \u0026#34;\u0026#34;\u0026#34;Native coroutine, modern syntax\u0026#34;\u0026#34;\u0026#34; s = await stuff() return s async def stuff(): return 0x10, 0x20, 0x30   　作为实验，没有await，或没有任何对asyncio.run()的调用，或其他asyncio有关的函数的情况下，单独调用py34_coro()或py35_coro()会发生什么？独立调用一个协程会返回一个协程对象：\n1 2  \u0026gt;\u0026gt;\u0026gt; py35_coro() \u0026lt;coroutine object py35_coro at 0x10126dcc8\u0026gt;   　表面上这不是十分有趣，调用一个协程的结果是一个可等待的协程对象（coroutine object）。\n　提问时间：Python的其他功能是什么样子？（单独调用Python的哪个功能实际上并没有做什么？）\n　这个问题的其中一个答案是生成器（generator），因为协程是引擎强化版的生成器。在这方面它的行为是相似的：\n1 2 3 4 5 6 7 8  \u0026gt;\u0026gt;\u0026gt; def gen(): ... yield 0x10, 0x20, 0x30 ... \u0026gt;\u0026gt;\u0026gt; g = gen() \u0026gt;\u0026gt;\u0026gt; g # Nothing much happens - need to iterate with `.__next__()` \u0026lt;generator object gen at 0x1012705e8\u0026gt; \u0026gt;\u0026gt;\u0026gt; next(g) (16, 32, 48)   　生成器函数，正如它发生的那样，是异步IO的基础（不管你是否使用async def声明协程还是使用旧版本的@asyncio.coroutine装饰器）。技术上，await和yield from比yield更相似。（但是请记住，yield from x()仅是在替代for i in x():yield i 的语法糖。）\n　和异步IO有关的生成器的一个关键特点是它能够有效地按意愿停止和重启。例如，你可以break正在迭代的生成器对象，然后稍后恢复对剩余值得迭代。当生成器函数到达yield，它会生成该值，随后处于空闲状态，直至被告知要生成后续值。\n　可以通过一个示例具体说明：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \u0026gt;\u0026gt;\u0026gt; from itertools import cycle \u0026gt;\u0026gt;\u0026gt; def endless(): ... \u0026#34;\u0026#34;\u0026#34;Yields 9, 8, 7, 6, 9, 8, 7, 6, ... forever\u0026#34;\u0026#34;\u0026#34; ... yield from cycle((9, 8, 7, 6)) \u0026gt;\u0026gt;\u0026gt; e = endless() \u0026gt;\u0026gt;\u0026gt; total = 0 \u0026gt;\u0026gt;\u0026gt; for i in e: ... if total \u0026lt; 30: ... print(i, end=\u0026#34; \u0026#34;) ... total += i ... else: ... print() ... # Pause execution. We can resume later. ... break 9 8 7 6 9 8 7 6 9 8 7 6 9 8 \u0026gt;\u0026gt;\u0026gt; # Resume \u0026gt;\u0026gt;\u0026gt; next(e), next(e), next(e) (6, 9, 8)   　await关键字表现相似，标记一个断点，协程会暂停然后让其它协程工作。“暂停”，在这种情况下是说协程暂时交出了控制权，但没有完全退出或完成。请记住yield，和它的扩展yield from和await，相当于在生成器执行时标记断点。\n　这是函数和生成器之间的基本不同。函数是全做或不做。一旦它启动，它在遇到return之前不会停止，然后将值返回给调用者（调用它的函数）。生成器，在每次遇到yield的时候暂停，不再继续运行。它不仅可以将值压入调用栈，还能通过调用next()恢复它时保留它的局部变量。\n　还有第二个鲜为人知但同样重要的生成器特点。你可以通过.send()方法发送一个值到生成器。这允许生成器（和协程）调用（await）彼此而无需阻塞。我不会更深的介绍这一点，因为它主要和协程本身的实现有关，但是你自己不需要直接使用它。\n　如果你想要了解更多，可以从PEP 342开始，是协程的正式介绍。Brett Cannon的How the Heck Does Async-Await in Python和PYMOTW writeup on asyncio也是不错的读物。最后，还有David Beazley的Curious Course on Coroutines and Concurrency，深入介绍了协程运行。\n　让我们试着把上述文章总结为几句话：这些协程实际上是通过一种特别非常规的机制运行的。他们的结果是异常对象的属性，该异常对象在调用其.send()方法时被抛出。所有这些还有更多细节，但是它可能对你实际使用语言没有帮助，让我们继续吧。\n　将所有事情总结一下，以下是协程作为生成器的一些关键点：\n 协程是改装的生成器（repurposed generators），具有生成器方法的优点。 旧的基于生成器的协程使用yield from等待协程结果。现代Python语法的原生协程用await替换掉yield from来等待协程结果。await和yield from相似，通常可以看作等价。 await的使用是标记断点的信号。它让协程暂时停止执行，并且允许程序稍后回来继续执行。  其他功能：async for和异步生成器、推导式（Comprehension） 　除了普通的async/await，Python还允许使用async for对**异步迭代器（asynchronous iterator）**进行迭代。异步迭代器的目的是能够在迭代时在每个阶段调用异步代码。\n　这个概念的自然扩展是异步生成器（asynchronous generator）。回顾在原生协程中你可以使用await，return或yield。从Python3.6（PEP 525）起，可以在协程内使用yield，他引入了异步生成器，目的是允许在同一个协程函数体中使用await和yield。\n1 2 3 4 5 6 7  \u0026gt;\u0026gt;\u0026gt; async def mygen(u: int = 10): ... \u0026#34;\u0026#34;\u0026#34;Yield powers of 2.\u0026#34;\u0026#34;\u0026#34; ... i = 0 ... while i \u0026lt; u: ... yield 2 ** i ... i += 1 ... await asyncio.sleep(0.1)   　最后但同样重要的是，Python用async for启用异步推导式（asynchronous comprehension）。像它的同步兄弟，这主要是语法糖：\n1 2 3 4 5 6 7 8 9 10 11 12  \u0026gt;\u0026gt;\u0026gt; async def main(): ... # This does *not* introduce concurrent execution ... # It is meant to show syntax only ... g = [i async for i in mygen()] ... f = [j async for j in mygen() if not (j // 3 % 5)] ... return g, f ... \u0026gt;\u0026gt;\u0026gt; g, f = asyncio.run(main()) \u0026gt;\u0026gt;\u0026gt; g [1, 2, 4, 8, 16, 32, 64, 128, 256, 512] \u0026gt;\u0026gt;\u0026gt; f [1, 2, 16, 32, 256, 512]   　这里有一个关键的区别：异步生成器和异步理解都不会使迭代并发。它们只提供和它们同步写法相对应的感觉，但是具有使相关循环放弃对事件循环的控制权以便使其他协程运行的能力。\n　换句话说，异步迭代器和异步生成器并非旨在序列或迭代器上并发映射某些函数。它们仅仅是让封闭的协程允许其他任务轮流运行。仅在使用普通的for和with会“破坏”写成中的await性质的情况下，才需要async for和async with。异步和并发的区别是要把握的关键。\n\r Comprehension：这里取Python文档中简体中文版的翻译，即翻译为”推导式“，其它地方也译为列表生成式等。  \r \r事件循环（Event loop）和asyncio.run() 　你可以认为事件循环就像while True循环一样，它监视协程，反馈空闲状态信息，寻找在此时可以执行的地十五。当协程正在等待的东西可用时，它能够唤醒该空闲协程。\n　因此，事件循环的整个管理已经隐式地通过一个函数调用处理：\n1  asyncio.run(main()) # Python 3.7+   　asyncio.run()，在Python3.7引入，负责获取事件循环，在标记完成前一直运行任务，然后关闭事件循环。\n　使用get_event_loop()可以更广泛地管理asyncio事件循环，典型的模式看起来像这样：\n1 2 3 4 5  loop = asyncio.get_event_loop() try: loop.run_until_complete(main()) finally: loop.close()   　你可能在一些旧的示例中看到loop.get_event_loop()，但是除非你特别需要微调对事件循环管理的控制，否则asyncio.run()对大多数程序来说应该足够了。\n　如果你需要在Python程序内部和事件循环交互，loop是一个老式的Python对象，它通过loop.is_running()和loop.is_closed()支持自省。\n　如果你需要更精细的控制，比如把循环作为参数进行调度回调，你可以操纵它们。\n　更关键的是要对事件循环的机制有一定了解。关于事件循环这里有几点值得强调。\n　**#1：**协同程序只有在与事件循环相关联的情况下，才能自行完成很多工作。\n　这点你在之前对生成器的解释中看到过，但值得重述一遍。如果你有一个await其它协程的主协程，仅仅独立地调用它是无效的：\n1 2 3 4 5 6 7 8 9 10  \u0026gt;\u0026gt;\u0026gt; import asyncio \u0026gt;\u0026gt;\u0026gt; async def main(): ... print(\u0026#34;Hello ...\u0026#34;) ... await asyncio.sleep(1) ... print(\u0026#34;World!\u0026#34;) \u0026gt;\u0026gt;\u0026gt; routine = main() \u0026gt;\u0026gt;\u0026gt; routine \u0026lt;coroutine object main at 0x1027a6150\u0026gt;   　记住使用asyncio.run()通过调度main()协程在事件循环上执行来强制执行。\n1 2 3  \u0026gt;\u0026gt;\u0026gt; asyncio.run(routine) Hello ... World!   　（其它协程可以通过await来执行。通常只将main()包装在asyncio.run()中，然后从那里调用带有await的链式协程。\n　**#2：**异步IO循环默认运行在单核单个线程上。通常，在一个CPU核上运行一个单线程事件循环绰绰有余。它可能通过多个核运行事件循环。可以参考talk by John Reese了解更多，但你的笔记本电脑可能会自燃。\n　**#3：**事件循环是可插入的。也就是说，如果你真的想的话，你可以自己实现你自己的事件循环，并让它以相同方式运行任务。这在uvloop包中被充分地介绍，是CPython中的事件循环实现。\n　“可插入的事件循环”的意思是：你可以使用任何事件循环的工作实现，核协程他们本身的结构无关。asyncio包本身涉及两种不同的事件循环实现，默认情况下基于selectors模块。（第二种实现仅为Windows构建）\n完整的程序：异步请求 　到目前为止你已经看了大部分了，是时候进行有趣而无痛的工作了。在本节中，你会构建一个网页抓取网址收集器，areq.py，使用aiohttp，快速的异步HTTP客户端/服务器框架。（我们仅需要客户端部分）这这样的工具可用于映射站点集群之间的连接，链接形成有向图。\n\r\r你可能会好奇为什么Python的requests包和异步IO不兼容。requests在urllib3的上层构建，实际上使用的是Python的http包和socket模块。\n\r \r　默认情况下，套接字操作是阻塞的。这意味着Python不会await requests.get(url)，因为.get()不是可等待的。相对地，几乎所有的事情在aiohttp中都是可等待的协程，如session.request()和response.text()。它虽然是一个很棒的包，但是在异步代码中使用requests会对自己造成损害。\n　高级程序结构像这样：\n 从本地文件urls.txt读取URL序列。 发送对URL的GET请求并解析结果内容。如果失败，则在此处停止输入URL。 在URL响应的HTML中寻找href标签。 将结果写入foundurls.txt。 尽可能异步和同时执行上述所有操作。（使用aiohttp发送请求，和aiofiles追加文件。这两个是非常适合异步IO模型的IO主要示例）。  　这是urls.txt的内容，它并不大，但包含了大部分流量大的网站\n1 2 3 4 5 6 7 8 9  $ cat urls.txt https://regex101.com/ https://docs.python.org/3/this-url-will-404.html https://www.nytimes.com/guides/ https://www.mediamatters.org/ https://1.1.1.1/ https://www.politico.com/tipsheets/morning-money https://www.bloomberg.com/markets/economics https://www.ietf.org/rfc/rfc2616.txt   　列表中的第二个URL会返回一个404响应，你需要优雅地处理。如果你运行这个程序的扩展版本，你需要处理更棘手的问题，比如连不上服务器和无休止的重定向。\n　请求本身应该使用单个的会话发出，以充分利用会话内部的连接池。\n　让我们看一眼整个程序。我们一步一步地浏览：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  #!/usr/bin/env python3 # areq.py \u0026#34;\u0026#34;\u0026#34;Asynchronously get links embedded in multiple pages\u0026#39; HMTL.\u0026#34;\u0026#34;\u0026#34; import asyncio import logging import re import sys from typing import IO import urllib.error import urllib.parse import aiofiles import aiohttp from aiohttp import ClientSession logging.basicConfig( format=\u0026#34;%(asctime)s%(levelname)s:%(name)s: %(message)s\u0026#34;, level=logging.DEBUG, datefmt=\u0026#34;%H:%M:%S\u0026#34;, stream=sys.stderr, ) logger = logging.getLogger(\u0026#34;areq\u0026#34;) logging.getLogger(\u0026#34;chardet.charsetprober\u0026#34;).disabled = True HREF_RE = re.compile(r\u0026#39;href=\u0026#34;(.*?)\u0026#34;\u0026#39;) async def fetch_html(url: str, session: ClientSession, **kwargs) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;GET request wrapper to fetch page HTML. kwargs are passed to `session.request()`. \u0026#34;\u0026#34;\u0026#34; resp = await session.request(method=\u0026#34;GET\u0026#34;, url=url, **kwargs) resp.raise_for_status() logger.info(\u0026#34;Got response [%s] for URL: %s\u0026#34;, resp.status, url) html = await resp.text() return html async def parse(url: str, session: ClientSession, **kwargs) -\u0026gt; set: \u0026#34;\u0026#34;\u0026#34;Find HREFs in the HTML of `url`.\u0026#34;\u0026#34;\u0026#34; found = set() try: html = await fetch_html(url=url, session=session, **kwargs) except ( aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, ) as e: logger.error( \u0026#34;aiohttp exception for %s[%s]: %s\u0026#34;, url, getattr(e, \u0026#34;status\u0026#34;, None), getattr(e, \u0026#34;message\u0026#34;, None), ) return found except Exception as e: logger.exception( \u0026#34;Non-aiohttp exception occured: %s\u0026#34;, getattr(e, \u0026#34;__dict__\u0026#34;, {}) ) return found else: for link in HREF_RE.findall(html): try: abslink = urllib.parse.urljoin(url, link) except (urllib.error.URLError, ValueError): logger.exception(\u0026#34;Error parsing URL: %s\u0026#34;, link) pass else: found.add(abslink) logger.info(\u0026#34;Found %dlinks for %s\u0026#34;, len(found), url) return found async def write_one(file: IO, url: str, **kwargs) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Write the found HREFs from `url` to `file`.\u0026#34;\u0026#34;\u0026#34; res = await parse(url=url, **kwargs) if not res: return None async with aiofiles.open(file, \u0026#34;a\u0026#34;) as f: for p in res: await f.write(f\u0026#34;{url}\\t{p}\\n\u0026#34;) logger.info(\u0026#34;Wrote results for source URL: %s\u0026#34;, url) async def bulk_crawl_and_write(file: IO, urls: set, **kwargs) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Crawl \u0026amp; write concurrently to `file` for multiple `urls`.\u0026#34;\u0026#34;\u0026#34; async with ClientSession() as session: tasks = [] for url in urls: tasks.append( write_one(file=file, url=url, session=session, **kwargs) ) await asyncio.gather(*tasks) if __name__ == \u0026#34;__main__\u0026#34;: import pathlib import sys assert sys.version_info \u0026gt;= (3, 7), \u0026#34;Script requires Python 3.7+.\u0026#34; here = pathlib.Path(__file__).parent with open(here.joinpath(\u0026#34;urls.txt\u0026#34;)) as infile: urls = set(map(str.strip, infile)) outpath = here.joinpath(\u0026#34;foundurls.txt\u0026#34;) with open(outpath, \u0026#34;w\u0026#34;) as outfile: outfile.write(\u0026#34;source_url\\tparsed_url\\n\u0026#34;) asyncio.run(bulk_crawl_and_write(file=outpath, urls=urls))   　这个脚本比我们的程序长多了，所以我们分解它。\n　常量HREF_RE是一个常规表达式，用来提取我们最终查找的HTML中的href标签：\n1 2  \u0026gt;\u0026gt;\u0026gt; HREF_RE.search(\u0026#39;Go to \u0026lt;a href=\u0026#34;https://realpython.com/\u0026#34;\u0026gt;Real Python\u0026lt;/a\u0026gt;\u0026#39;) \u0026lt;re.Match object; span=(15, 45), match=\u0026#39;href=\u0026#34;https://realpython.com/\u0026#34;\u0026#39;\u0026gt;   　协程fetch_html()是一个GET请求的装饰器，用来创建请求并对HTML页面解码。它创建请求，await响应，并用正确的方式处理非200的状态码。\n1 2  resp = await session.request(method=\u0026#34;GET\u0026#34;, url=url, **kwargs) resp.raise_for_status()   　如果状态是OK，fetch_html()返回HTML页面（一个字符串）。注意，这个函数中没有异常处理。逻辑是将该异常传播给调用方，并在那里进行处理。\n1  html = await resp.text()   　我们await session.request()和resp.text()是因为他们是可等待的协程。请求/响应循环是应用延迟高且耗时的部分，但是使用异步IO，fetch_html()让事件循环工作在其它就绪可用的工作上，比如解析和写入已经获取的URL。\n　协程链的下一步是parse()，在fetch_html()中等待给定的URL，然后提取那个HTML页面的所有href标签，保证每个都有效并且将它格式化为绝对路径。\n　诚然，parse()的第二部分是阻塞的，但是它由快速的正则表达式匹配组成，并确保发现的连接设置为绝对路径。\n　在这种具体情形下，同步代码需要是快速且不显眼的。但是请记住，给定协程中的任何行都会阻塞其它协程，除非该行使用yield，await或return。如果解析，你可能需要考虑使用loop.run_in_executor()在它自己的程序中运行这部分。\n　接下来，协程write()接收一个文件对象和一个URL，等待parse()返回一组解析的URL，通过aiofiles（异步文件IO包）根据源URL把每一个异步地写入文件。\n　最后，bulk_crawl_and_write()作为进入脚本协程链的主要入口点。它使用一个会话，并为每个从url.txt中读取的URL创建一个任务。\n　这里有额外的几点需要注意：\n 默认的ClientSession最大具有100个开放连接的适配器。需要更改此设置，请传递一个asyncio.connector.TCPConnector实例给ClientSession。你也可以基于每个主机指定限制。 你可以为整个会话和单个请求指定最大超时。 这个脚本也使用了await with，用作异步上下文管理器（asynchronous context manager）。我没有为此开设一节讲述这个概念，因为从同步上下文管理器到异步上下文管理器的转变比较直接。后者定义了.__aenter__()和.__aexit__()而不是.__exit__()和.__enter__()。和你想的一样，async with也只能用在声明为async def的协程函数内部。  　如果你想了解更多，Github上本教程的配套文件也附带了注释和文档。\n　这是全部执行过程，areq.py在1秒钟获取、解析并保存了9个URL的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  $ python3.7 areq.py 21:33:22 DEBUG:asyncio: Using selector: KqueueSelector 21:33:22 INFO:areq: Got response [200] for URL: https://www.mediamatters.org/ 21:33:22 INFO:areq: Found 115 links for https://www.mediamatters.org/ 21:33:22 INFO:areq: Got response [200] for URL: https://www.nytimes.com/guides/ 21:33:22 INFO:areq: Got response [200] for URL: https://www.politico.com/tipsheets/morning-money 21:33:22 INFO:areq: Got response [200] for URL: https://www.ietf.org/rfc/rfc2616.txt 21:33:22 ERROR:areq: aiohttp exception for https://docs.python.org/3/this-url-will-404.html [404]: Not Found 21:33:22 INFO:areq: Found 120 links for https://www.nytimes.com/guides/ 21:33:22 INFO:areq: Found 143 links for https://www.politico.com/tipsheets/morning-money 21:33:22 INFO:areq: Wrote results for source URL: https://www.mediamatters.org/ 21:33:22 INFO:areq: Found 0 links for https://www.ietf.org/rfc/rfc2616.txt 21:33:22 INFO:areq: Got response [200] for URL: https://1.1.1.1/ 21:33:22 INFO:areq: Wrote results for source URL: https://www.nytimes.com/guides/ 21:33:22 INFO:areq: Wrote results for source URL: https://www.politico.com/tipsheets/morning-money 21:33:22 INFO:areq: Got response [200] for URL: https://www.bloomberg.com/markets/economics 21:33:22 INFO:areq: Found 3 links for https://www.bloomberg.com/markets/economics 21:33:22 INFO:areq: Wrote results for source URL: https://www.bloomberg.com/markets/economics 21:33:23 INFO:areq: Found 36 links for https://1.1.1.1/ 21:33:23 INFO:areq: Got response [200] for URL: https://regex101.com/ 21:33:23 INFO:areq: Found 23 links for https://regex101.com/ 21:33:23 INFO:areq: Wrote results for source URL: https://regex101.com/ 21:33:23 INFO:areq: Wrote results for source URL: https://1.1.1.1/   　这样不严谨，对于健全的检查，你可以检查输出行数，我是626，但这可能会有所波动：\n1 2 3 4 5 6 7  $ wc -l foundurls.txt 626 foundurls.txt $ head -n 3 foundurls.txt source_url parsed_url https://www.bloomberg.com/markets/economics https://www.bloomberg.com/feedback https://www.bloomberg.com/markets/economics https://www.bloomberg.com/notices/tos   　下一步：如果你想要提高底注，请使用这个网络爬虫递归。你可以使用aio-redis追踪哪个URL已经被抓取了，从而避免请求他们两次，并用Python的networkx库和链接相连。\n　记住要友好。发送1000个并发请求给一个小型、毫无防御的网站是非常糟糕的。有一些方法可以限制你一次发出的并发请求，例如使用asyncio的sempahore对象或使用和它相似的模板。如果你不注意此警告，可能会受到大量的TimeoutError异常，最终只会损害您自己的程序。\n异步IO的使用场景 　现在你已经了解了健康的代码，让我们退一步思考一下异步IO什么时候是一个理想的选择，以及你怎样做出比较得出结论或是否选择其他的并发模型。\n异步IO什么时候是一个正确的选择？为什么？ 　本教程并没有对异步IO、线程、多进程进行扩展论述。但是，了解异步IO何时是三者间的最佳选择很有用。\n　异步和多进程之间的斗争根本不存在。实际上，它们可以被正确地使用。如果你有多个公平统一的CPU受限任务，（一个很好的例子是库内网格查找（grid search），比如scikit-learn或keras）多进程是不错的选择。\n　如果所有的函数都是阻塞调用，而仅仅是将async放到每个函数前面是糟糕的想法。（实际上还可能会减速你的代码）但是像先前提到的那样，异步IO和多进程在很多地方可以和谐共存。\n　异步IO和线程之间的竞争更直接，我在引言中提到了“线程是困难的”。完整的说法是，即使某些情形下线程看起来很容易实现，由于竞争条件和内存使用等原因，它仍然可能导致那些臭名昭著、无法跟踪的错误。\n　和异步IO相比，线程扩展的趋势也不太好。因为线程是具有有限可用性的系统资源。创建几千个线程在大多数机器上都会失败，因此我不建议你首先进行尝试。创建数千异步IO任务完全可行。\n　在你有多个IO受限任务时异步IO闪闪发光，否则这些任务的时常会被阻塞的IO等待时间占据，例如：\n 网络IO，无论你的程序是服务端还是客户端。 无服务器设计，例如像聊天室多用户网络或P2P网络。 模仿“即写即忘”风格的读/写操作，而不担心读写时锁定的东西时。  　不使用异步IO的最大原因是await仅支持一组由特定方法定义的特定对象。如果你想要对DBMS进行异步读操作，你需要找到的不仅是DBMS的Python装饰器，还需要支持async/await的语法。包含同步调用的协程会阻止其它协程和任务的运行。\n　支持async/await的清单，可以在教程的尾部查看。\n应该选择异步IO的哪个包？ 　这篇教程专注于异步IO，async/await语法，使用asyncio进行事件循环管理和指定任务。asyncio并不是唯一的异步IO库。Nathaniel J. Smith发现了很多：\n 在未来的几年，asyncio可能会沦落为精明的开发人员避免使用的标准库之一，如urllib2。\n\u0026hellip;\n实际上，我要说的是asyncio是其自身成功的受害者：设计时，它使用了可能的最佳方法；但是从那时起，异步的启发下的工作（例如添加异步/等待）已经改变了现状，以便我们可以做得更好，现在异步已被其早期的承诺所束缚。（Source）\n 　尽管如此，可以替代asyncio的著名替代品是curio和trio，它们使用不同的API和方法。从个人来讲，我认为如果你要构建大小适中，简单明了的程序，仅使用asyncio就足够且能够理解了，并且让你避免添加Python标准库以外的大型依赖。\n　但是无论如何，检查curio和trio，你会发现它们以相同的方式完成了对用户而言更直观的操作。这里介绍的与包无关的概念也应该渗透到其它异步IO包中。\n其他零碎的内容 　在接下来的几节，你会了解到asyncio和async/await的其它各部分，这些部分目前不太匹配本教程，但对于构建和理解完整的程序很重要。\n其它顶级asyncio函数 　除了asyncio.run()以外，你会看到几个其它包级别的函数，比如asyncio.create_task()和asyncio.gather()。\n　你可以使用create_task()调度协程对象的执行，然后使用asyncio.run()：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \u0026gt;\u0026gt;\u0026gt; import asyncio \u0026gt;\u0026gt;\u0026gt; async def coro(seq) -\u0026gt; list: ... \u0026#34;\u0026#34;\u0026#34;\u0026#39;IO\u0026#39; wait time is proportional to the max element.\u0026#34;\u0026#34;\u0026#34; ... await asyncio.sleep(max(seq)) ... return list(reversed(seq)) ... \u0026gt;\u0026gt;\u0026gt; async def main(): ... # This is a bit redundant in the case of one task ... # We could use `await coro([3, 2, 1])` on its own ... t = asyncio.create_task(coro([3, 2, 1])) # Python 3.7+ ... await t ... print(f\u0026#39;t: type {type(t)}\u0026#39;) ... print(f\u0026#39;t done: {t.done()}\u0026#39;) ... \u0026gt;\u0026gt;\u0026gt; t = asyncio.run(main()) t: type \u0026lt;class \u0026#39;_asyncio.Task\u0026#39;\u0026gt; t done: True   　这里有一个细节：如果你不在main()中使用await()，它会在main()发送完成信号之前完成。因为asyncio.run(main())调用loop.run_until_complete(main())，事件循环仅关心（没有await t）main()是否完成，并不关心在main()内部创建的任务是否完成。没有await t，循环其他的任务可能在它们完成之前取消。如果你需要获取当前待处理的任务列表，可以使用asyncio.Task.all_tasks()。\n\r\rasyncio.create_task()是在Python3.7中提出的。在Python3.6或更低版本，使用asyncio.ensure_future()替换create_task()。\n\r \r　另外，还有asyncio.gather()。尽管他没做什么特别的事，gather()意思是把协程（future）的集合整齐地放入一个单个的future中。它返回一个future对象作为结果，如果你await asyncio.gather()并指定多个任务或协程，你需要等待它们全部完成。（这个有点类似于先前例子中queue.join()的并行版）gather()的结果是输入的结果列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \u0026gt;\u0026gt;\u0026gt; import time \u0026gt;\u0026gt;\u0026gt; async def main(): ... t = asyncio.create_task(coro([3, 2, 1])) ... t2 = asyncio.create_task(coro([10, 5, 0])) # Python 3.7+ ... print(\u0026#39;Start:\u0026#39;, time.strftime(\u0026#39;%X\u0026#39;)) ... a = await asyncio.gather(t, t2) ... print(\u0026#39;End:\u0026#39;, time.strftime(\u0026#39;%X\u0026#39;)) # Should be 10 seconds ... print(f\u0026#39;Both tasks done: {all((t.done(), t2.done()))}\u0026#39;) ... return a ... \u0026gt;\u0026gt;\u0026gt; a = asyncio.run(main()) Start: 16:20:11 End: 16:20:21 Both tasks done: True \u0026gt;\u0026gt;\u0026gt; a [[1, 2, 3], [0, 5, 10]]   　你可能注意到了gather()等待Future或你传递的协程的全部结果。或者，你可以遍历asyncio.as_completed()来按顺序获取完成的任务。函数返回一个在完成任务时产生任务的迭代器。在下面的例子中，在coro([10,5,0])完成之前可以使用coro([3,2,1])的结果，在gather()中则不行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \u0026gt;\u0026gt;\u0026gt; async def main(): ... t = asyncio.create_task(coro([3, 2, 1])) ... t2 = asyncio.create_task(coro([10, 5, 0])) ... print(\u0026#39;Start:\u0026#39;, time.strftime(\u0026#39;%X\u0026#39;)) ... for res in asyncio.as_completed((t, t2)): ... compl = await res ... print(f\u0026#39;res: {compl} completed at {time.strftime(\u0026#34;%X\u0026#34;)}\u0026#39;) ... print(\u0026#39;End:\u0026#39;, time.strftime(\u0026#39;%X\u0026#39;)) ... print(f\u0026#39;Both tasks done: {all((t.done(), t2.done()))}\u0026#39;) ... \u0026gt;\u0026gt;\u0026gt; a = asyncio.run(main()) Start: 09:49:07 res: [1, 2, 3] completed at 09:49:10 res: [0, 5, 10] completed at 09:49:17 End: 09:49:17 Both tasks done: True   　最后，你可能会看到asyncio.ensure_future()。你应该很少需要它，因为它是较低级的管道API，并且大多数情况都能被create_task()替代，稍后会介绍它。\nawait优先 　尽管它们有点相似，但await关键字比yield有更高的优先级。这意味着，由于界限紧密，因此在许多情况下，你需要在yield from语句中使用括号，而在类似的await语句中则不需要。详情参考PEP 492关于await表达式的示例。\n结论 　现在你已经可以使用async/await和由此构建的库了。这是本篇教程内容的回顾：\n 异步IO是一种语言无关模型和一种通过让协程之间间接交流实现有效并发的方式。 Python的新关键字async和await，用来标记和定义协程。 asyncio，Python提供运行和管理协程的包。  资源 Python版本细节 　Python中的异步IO发展迅速，很难追踪何时发生了什么。以下是与asyncio相关的Python版本变更列表：\n 3.3：生成器中允许使用yield from 3.4：在Python标准库中以临时API状态引入asyncio 3.5：async和await称为Python语法的一部分，用来标识和等待协程。它们仍不是保留字。（你仍然可以定义名为await和async的函数或变量） 3.6：引入异步生成器和异步理解（生产列表的工具）。asyncio的API被声明为稳定的，而不是临时的。 3.7：async和await称为保留字。（他们不能被用作标识符）它们替代了asyncio.coroutine()装饰器。``asyncio包引入了asyncio.run()`，其中包括许多其它功能。  　如果你想要稳一点（并且能够使用asyncio.run()），请使用Pyhton3.7或更高版本获取完整的功能。\n文章 　这是额外的资源清单：\n Real Python: Speed up your Python Program with Concurrency Real Python: What is the Python Global Interpreter Lock? CPython: The asyncio package source Python docs: Data model \u0026gt; Coroutines TalkPython: Async Techniques and Examples in Python Brett Cannon: How the Heck Does Async-Await Work in Python 3.5? PYMOTW: asyncio A. Jesse Jiryu Davis and Guido van Rossum: A Web Crawler With asyncio Coroutines Andy Pearce: The State of Python Coroutines: yield from Nathaniel J. Smith: Some Thoughts on Asynchronous API Design in a Post-async/await World Armin Ronacher: I don’t understand Python’s Asyncio Andy Balaam: series on asyncio (4 posts) Stack Overflow: Python asyncio.semaphore in async-await function Yeray Diaz:  AsyncIO for the Working Python Developer Asyncio Coroutine Patterns: Beyond await    A few Python What’s New sections explain the motivation behind language changes in more detail:\n What’s New in Python 3.3 (yield from and PEP 380) What’s New in Python 3.6 (PEP 525 \u0026amp; 530)  From David Beazley:\n Generator: Tricks for Systems Programmers A Curious Course on Coroutines and Concurrency Generators: The Final Frontier  YouTube talks:\n John Reese - Thinking Outside the GIL with AsyncIO and Multiprocessing - PyCon 2018 Keynote David Beazley - Topics of Interest (Python Asyncio) David Beazley - Python Concurrency From the Ground Up: LIVE! - PyCon 2015 Raymond Hettinger, Keynote on Concurrency, PyBay 2017 Thinking about Concurrency, Raymond Hettinger, Python core developer Miguel Grinberg Asynchronous Python for the Complete Beginner PyCon 2017 Yury Selivanov asyncawait and asyncio in Python 3 6 and beyond PyCon 2017 Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream What Is Async, How Does It Work, and When Should I Use It? (PyCon APAC 2014)  相关的PEP    PEP Date Created     PEP 342 – Coroutines via Enhanced Generators 2005-05   PEP 380 – Syntax for Delegating to a Subgenerator 2009-02   PEP 3153 – Asynchronous IO support 2011-05   PEP 3156 – Asynchronous IO Support Rebooted: the “asyncio” Module 2012-12   PEP 492 – Coroutines with async and await syntax 2015-04   PEP 525 – Asynchronous Generators 2016-07   PEP 530 – Asynchronous Comprehensions 2016-09    支持async/await的库 　From aio-libs:\n aiohttp: Asynchronous HTTP client/server framework aioredis: Async IO Redis support aiopg: Async IO PostgreSQL support aiomcache: Async IO memcached client aiokafka: Async IO Kafka client aiozmq: Async IO ZeroMQ support aiojobs: Jobs scheduler for managing background tasks async_lru: Simple LRU cache for async IO  From magicstack:\n uvloop: Ultra fast async IO event loop asyncpg: (Also very fast) async IO PostgreSQL support  From other hosts:\n trio: Friendlier asyncio intended to showcase a radically simpler design aiofiles: Async file IO asks: Async requests-like http library asyncio-redis: Async IO Redis support aioprocessing: Integrates multiprocessing module with asyncio umongo: Async IO MongoDB client unsync: Unsynchronize asyncio aiostream: Like itertools, but async  ","description":"一篇讲解Python异步IO博客的译文，内容很清晰","id":27,"section":"posts","tags":["python"],"title":"Python：完整地了解异步IO（译）","uri":"https://yyyIce.github.io/zh/posts/pythonasync-io-in-python-a-complete-walkthrough%E8%AF%91/"},{"content":"　主要是为了回调函数的编写和阅读，顺带了一点指针的基础内容。\n0.理解前提 　指针的定义和运算不再赘述，需要理解指针和地址的关系。\n　在编写代码的过程中，认为变量名表示的是数据本身，函数名、字符串名、数组名表示代码块或数据块的首地址。\n　变量都对应一块内存空间，整型变量5，对应的内存空间中存储的内容就是5，赋值时应赋给这块内容一个整型值；指针对应的这块内存空间存储的内容是它指向的数据的地址，所以在赋值时应将一个地址赋值给指针，也就是常说的指针就是地址。\n printf函数族中对于%p一般以十六进制整数方式输出指针的值）\n 　例如：指针p存储的内容是整型变量c的地址，对p存储的内容进行取值运算（*），即对地址取值，p存储c的地址，c中存储的是5，所以*p=5。\n1 2 3 4 5 6 7 8 9 10 11 12  int main() { int c = 5; int * p = \u0026amp;c; printf(\u0026#34;c:%d\t\u0026amp;c:%p\\n\u0026#34;, c, \u0026amp;c); printf(\u0026#34;\u0026amp;p:%p\tp:%p *p:%d\\n\u0026#34;, \u0026amp;p, p, *p); return 0; } 预计输出： c:5 \u0026amp;c:000000000062FE1C \u0026amp;p:000000000062FE10 p:000000000062FE1C *p:5   \r　下图左侧一列为地址，右侧一列为地址单元中存储的值。\n图1 \u0026amp;p，p，*p，c，\u0026amp;c的存储位置:\r\r 1.void * 　void *，void *表示这是一个指针，但是没有指定指针的类型（整型、字符型等等），即该指针可以被指向任意类型的变量。\n　(void *) \u0026amp;a， 把变量a的地址转换为任意类型的指针变量。\n　void *类型的变量给别的变量赋值应对其进行强制类型转换，例：\n1 2  void * p1 = NULL; int * p2 = (int *)p1;   2.二维数组和指针 　先明确指针的加（减）运算，是加上（减去）该指针指向的数据类型大小的整数倍。例如下面的代码中，指针p是int类型，将他指向a，则p + 1是将p，即a[0]的地址值加上一个整型数据的大小，即指向了a[0]的下一个元素，a[2]。\n1 2 3 4 5 6  int a[4] = {1,2,3,4}; int * p = a; for(int i = 0; i \u0026lt; 4; i++) { printf(\u0026#34;%d \u0026#34;,*(p+i)); }   　二维数组在内存中的分布是线性的（a[行][列]），整个数组占用一块连续的内存：\nint a[3][4] = {{0,1,2,3}, {4,5,6,7}, {8,9,10,11}};\r\r图2 a[3][4]内存分布图:\r\r 　即只是为了人类理解、编程方便，才将数组在逻辑上分成二维数组，在内存中仍是一维数组。\n　所以处理二维数组时可以把二维数组分解成多个一维数组来处理。a[3][4]可以分为a[0]，a[1]，a[2]，每个一维数组中有4个元素。\n　下面的代码声明了一个数组指针变量p，并将它指向了二维数组a，此时p等价于a[0]的首地址，p+1则是a[1]的首地址，因为此时p的指向类型是int [4]，所以+1指针会向后移动4个int大小，即指针值加上4个int大小，其值就是a[1]的地址。可以从输出看出，p+1的地址值比p多了0x10，即16，为4个int的大小。\n1 2 3 4 5 6 7 8  int b[3][4] = {{0,1,2,3}, {4,5,6,7}, {8,9,10,11}}; int (*p)[4] = b; printf(\u0026#34;sizeof(int): %d\\n\u0026#34;,sizeof(int)); printf(\u0026#34;p:%p , p+1:\u0026#34;,p , p+1); //输出 \u0026gt;\u0026gt;\u0026gt;sizeof(int): 4 \u0026gt;\u0026gt;\u0026gt;p:000000000062FDC0 , p+1:000000000062FDD0   　用二维数组指针访问二维数组元素：\n　根据上面所说，p+1是a[1]的首地址，所以p+1对应的内存单元中存储的是\u0026amp;a[1][0]，即a[1]的首地址，即*(p+1)，要访问a[1][1]，需要把a[1][0]的地址加上一个int大小，然后对这个地址做取值运算（*运算），\u0026amp;a[1][1]即*(p+1) + 1（也可以写成a[1]+1，因为a[1]代表首地址），是a[1][1]的地址，对它做取值运算，a[1][1] = *(*(p+1) + 1)=*(a[1]+1)，访问其他行列的元素同理。\n1 2 3 4 5 6 7 8  int b[3][4] = {{0,1,2,3}, {4,5,6,7}, {8,9,10,11}}; int (*p)[4] = b; printf(\u0026#34;\u0026amp;b[1][0]:%p\\tb[1]:%p\\t*(p+1):%p\\n\u0026#34;,\u0026amp;b[1][0],b[1],*(p+1)); printf(\u0026#34;b[1][0]:%d\\t*(b[1]+1):%d\\t*(*(p+1)+1):%d\\n\u0026#34;,b[1][0],*(b[1]+1),*(*(p+1)+1) ); //输出 \u0026gt;\u0026gt;\u0026gt;\u0026amp;b[1][0]:000000000062FDD0 b[1]:000000000062FDD0 *(p+1):000000000062FDD0 \u0026gt;\u0026gt;\u0026gt;b[1][0]:4 *(b[1]+1):5 *(*(p+1)+1):5   　指针数组和数组指针声明时的区别：括号位置\n1 2  int (*p1)[4]; //括号一定在*p1外，(*p) int *(p2[5]); //这是一个指针数组，该数组有5个元素，每个元素都是int类型的指针   3.函数指针 　函数指针即指向函数的指针，这种指针定义可以用一个标识符表示参数和返回值类型相同的任意函数，方便修改函数内容而引用函数的部分无需变动。\n　函数指针的定义形式：\n1  returnType (*pointerName)(param list); //括号不能省略，省略括号会变成返回值是指针的函数   　例：\n1 2 3 4 5 6 7 8 9 10 11  void haaa(int a, char c) { printf(\u0026#34;a:%d, c:%c\\n\u0026#34;,a,c); } int main() { int (*fun_pointer)(int, char) = NULL; //如果未指定初值记得指向NULL，养成良好习惯  fun_pointer = haaa; //函数名的值是函数的首地址，所以可以直接把haaa赋值给fun_pointer  fun_pointer(1,\u0026#39;x\u0026#39;); return 0; }   　其中对于函数指针的赋值，由于\u0026amp;haaa和haaa等价，所以(* fun_pointer)就是fun_pointer，所以使用函数指针的方法有很多种，即调用时加不加*都一样。\n1 2 3 4 5  printf(\u0026#34;\u0026amp;haaa:%p, haaa:%p\\n\u0026#34;, \u0026amp;haaa, haaa); \u0026gt;\u0026gt;\u0026gt;\u0026amp;haaa:0000000000401530, haaa:0000000000401530 printf(\u0026#34;fun_pointer:%p, *fun_pointer:%p\\n\u0026#34;, fun_pointer, *fun_pointer); \u0026gt;\u0026gt;\u0026gt;fun_pointer:0000000000401530, *fun_pointer:0000000000401530   1 2 3 4 5 6 7 8 9  //给指针函数赋值 fun_pointer = haaa; //或 fun_pointer = \u0026amp;haaa; //调用指针函数 fun_pointer(1,\u0026#39;x\u0026#39;); //或 (* fun_pointer)(1,\u0026#39;x\u0026#39;);   4.回调函数 　回调函数就是某函数的参数含有该函数的指针，则这个指针指向的函数被称为回调函数。\n　回调：没有直接调用这个函数，而是作为参数被别的函数调用。\n　在下面的代码中，test通过参数传递的方式调用了传入的函数，可以用一个函数灵活地调用不同的函数，当函数指针指向max时，把该指针作为参数传入test，会在test内执行max；指向min时同理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  #include \u0026lt;stdio.h\u0026gt;int test(int a, int b, int (* callback)(int,int)) { return callback(a,b); } int max(int a, int b) { return a \u0026gt; b ? a : b; } int min(int a, int b) { return a \u0026gt; b ? b : a; } int main() { int ret = 0; ret = test(4, 8, max); printf(\u0026#34;result:%d\\n\u0026#34;, ret); ret = test(4, 8, min); printf(\u0026#34;result:%d\\n\u0026#34;, ret); return 0; } \u0026gt;\u0026gt;\u0026gt;result:8 \u0026gt;\u0026gt;\u0026gt;result:4   　注：\n　在C中还可以向下面这样写，即不写指针的参数个数和类型，此时文件后缀必须为.c，.cpp会报错。可能是不规范但能执行的写法，不建议使用，但应该了解以便于程序的阅读。\n1 2 3 4  int test(int a, int b, int (* callback)()) { return callback(a,b); }   　回调函数的作用：可以作为接口，由应用实现test函数，第三方开发者实现callback函数，应用就可以帮助第三方开发者执行callback函数，从而达到开放性和可扩展性。这样应用只需要向第三方开发者提供头文件和动态链接库而无需提供源代码。第三方开发者通过头文件中的回调函数接口，编写需要应用执行的函数即可。\n5.分析练习 　试分析下列代码片段的逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; typedef unsigned long (*bf_hash_handler_type)(void *element); struct bloom_filter { bf_hash_handler_type *handlers; unsigned char *bitmap; unsigned long k, m; }; typedef struct bloom_filter *bf_core_type; bf_core_type bf_create(bf_hash_handler_type handlers[], unsigned long k, unsigned long m) { bf_core_type bf; bf=(bf_core_type)malloc(sizeof(struct bloom_filter)); bf-\u0026gt;k=k; bf-\u0026gt;m=m; bf-\u0026gt;handlers=handlers; bf-\u0026gt;bitmap=(unsigned char *)malloc((m+7)\u0026gt;\u0026gt;3); return bf; } static inline unsigned char bitget(unsigned char *bitmap, unsigned long index) { return 1\u0026amp;(bitmap[index\u0026gt;\u0026gt;3]\u0026gt;\u0026gt;(index\u0026amp;7)); } int bf_has(bf_core_type bf, void *element) { int i; for(i=0; i\u0026lt;bf-\u0026gt;k; i++) { if(!bitget(bf-\u0026gt;bitmap, bf-\u0026gt;handlers[i](element)%bf-\u0026gt;m)) { return 0; } } return 1; } unsigned long simple_hash(void *element) { return *(unsigned long*)element; } //...  int main() { bf_core_type bf; bf_hash_handler_type handlers[]= {simple_hash}; unsigned long handlers_len=sizeof(handlers)/sizeof(bf_hash_handler_type); bf=bf_create(handlers, handlers_len, 100); //....  return 0; }   　　解析：\n　直接看main函数，一共四行。\n 第一行声明了一个bf_core_type类型的变量，bf_core_type是struct bloom_filter类型的指针的别名。 第二行初始化了一个bf_hash_handler_type类型的数组，该数组有一个元素simple_hash，bf_hash_handler_type是一个参数为void *类型、返回值为unsigned long类型的函数指针，并且该指针的别名为bf_hash_handler_type，定义函数指针时可直接：  \r1 2 3  bf_hash_handler_type fun_name; //等价于  unsigned long fun_name(void *);    第三行计算的是handlers中有几个元素，此行的计算结果应该为1，handlers的类型是指针数组，只有一个元素，指针类型是unsigned long，所以handlers的大小应该和bf_hash_handler_type的大小相同，也和unsigned long *的大小相同。  \r1 2 3 4 5 6 7 8 9  printf(\u0026#34;sizeof unsigned long:%d\\n\u0026#34;,sizeof(unsigned long)); printf(\u0026#34;sizeof unsigned long *:%d\\n\u0026#34;,sizeof(unsigned long *)); printf(\u0026#34;sizeof handlers:%d\\n\u0026#34;,sizeof(handlers)); printf(\u0026#34;sizeof bf_hash_handler_type:%d\\n\u0026#34;,sizeof(bf_hash_handler_type)); //输出  \u0026gt;\u0026gt;\u0026gt;sizeof unsigned long:4 \u0026gt;\u0026gt;\u0026gt;sizeof unsigned long *:8 \u0026gt;\u0026gt;\u0026gt;sizeof handlers:8 \u0026gt;\u0026gt;\u0026gt;sizeof bf_hash_handler_type:8   \r 第四行调用了bf_create函数，传入了函数指针数组，和两个值。 在bf_create函数中，为main中的bf申请了内存空间，并对bf进行初始化，没有调用simple_hash。 在bf_hash函数中，传入了含有回调函数的结构体bf，在bf-\u0026gt;handlers[i](element)%bf-\u0026gt;m这句所在的for循环中调用了handlers中的所有指针指向的函数。  6.参考代码 　地址：https://github.com/yyyIce/c_exercise/tree/main/pointer_test\n　有帮助的话可以点个Star⭐哦~\n参考资料 　1.二维数组指针：http://c.biancheng.net/view/2022.html\n　2.回调函数：https://cloud.tencent.com/developer/article/1457059\n","description":"C语言指针的用法和示例","id":28,"section":"posts","tags":["c"],"title":"C语言：指针","uri":"https://yyyIce.github.io/zh/posts/c%E8%AF%AD%E8%A8%80%E6%8C%87%E9%92%88/"},{"content":"　注意题目：C语言，不是C+。\n　本文适合对C语言有一定理解，只存在部分概念模糊的人群阅读，不适合初学者，参考资料列在文章最后。\n　本文的目标是彻底理解结构体，在需要使用结构体时，光速且正确地写完相关代码，做明明白白的工具人。\n0.变量的定义、声明、初始化 　变量的声明，是向程序表明变量的类型和名字。如结构体类型声明、函数声明等，在仅声明时，不分配内存空间。\n　定义也是声明，但在定义时，为变量分配内存空间。\n1 2  int a; //是定义，也是声明，未初始化 extern int a; //是声明，不是定义，未初始化   　声明主要是为了(提前)使用声明的内容，声明的作用举例：\n 不声明结构体类型，则无法在后来需要时才定义结构体类型的变量。 如果有一个函数fun_1，在文件的代码顺序中它定义在调用它的函数fun_2后面，此时如果不更改定义顺序，则必须在文件开始处声明fun_1。  　变量初始化就是赋予变量一个初始值：\n1 2 3 4  int a; //定义变量 a = 5; //初始化 //或 int b = 10; //在变量定义的同时初始化   1.结构体类型的定义和声明 (1)struct \u0026lt;结构体名\u0026gt;{\n​ 类型 \u0026lt;成员名\u0026gt; ;\n​ 类型 \u0026lt;成员名\u0026gt; ;\n​ \u0026hellip;\n}；\n1 2 3 4 5 6 7  //例如： struct student_t { char name[128]; int sex; int age; };   (2)可在声明结构体的同时定义变量，如下文代码，在声明类型struct student_t的同时，还定义了2个变量， student1 和 student2 ，它们的类型是struct student_t。\n1 2 3 4 5 6  struct student_t { char name[128]; int sex; int age; }student1,student2;   (3)也可以不写结构体名，直接定义结构体变量：\n1 2 3 4 5 6  struct { char name[128]; int sex; int age; }student;   2.结构体的初始化和内存空间分配 (1)定义时赋值 1  struct student_t s1 = { \u0026#34;Sarah\u0026#34;, 0, 18};   (2)定义后逐个赋值 理论上应该如下所示：\n1 2 3 4  struct student_t s2; s2.name = \u0026#34;Sam\u0026#34;; //此句报错 s2.sex = 1; s2.age = 19;   解释：不能在定义以外的地方把字符串赋值给一个字符数组。\n1 2 3  char a[10] = \u0026#34;hello\u0026#34;; //这样可以，这种情况是c语言初始化所支持的，此时\u0026#34;hello\u0026#34;没有存储在常量区 char b[10]; b = \u0026#34;hello\u0026#34;; //这样不行，不能把字符串赋值给一个数组，此时\u0026#34;hello\u0026#34;存储在常量区   　双引号\u0026quot;\u0026quot;在常量区申请内存空间并存放字符串hello，并在字符串尾加入\\0，最后返回申请的内存空间的地址，b是数组的首地址，但是它是一个常量，编译器不允许更改存储在常量区的内容，即它不是一个左值。左值就是指在程序中占用内存空间、可以被修改的量,比如各种变量。\n　更改：将name的类型改为字符指针char *，此时再进行赋值则不会出现问题。s2.name是一个字符指针，指针的本质就是地址，即指针这块内存空间存储着其指向内容的内存地址，假设Sam这个字符串的地址为addr，则s2.name申请的空间中存储着addr，在寻址时取出addr，即可找到存储的内容Sam。\n1 2 3 4 5 6 7 8 9 10  struct student_t { char * name; int sex; int age; }; struct student_t s2; s2.name = \u0026#34;Sam\u0026#34;; //不报错 s2.sex = 1; s2.age = 19;   　这时候可能有人要提问了，既然s2.name中存储的是addr，那么为什么我输出s2.name，会输出Sam而不是addr呢？这是printf的实现机制决定的，如果我们选用%p格式，则会输出存储数值的地址，如果用%d，%c，则会输出地址中存储的数值，如果使用%s，则会输出首地址及其余地址存储的字符串内容。测试代码及结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12  struct student_t s4; char a[10] = \u0026#34;Sally\u0026#34;; s4.name = a; //a是数组首地址 s4.sex = 1; s4.age = 19; printf(\u0026#34;%p\\n\u0026#34;, a); //%p可以输出变量存储的地址，000000000062FD50 printf(\u0026#34;%s\\n\u0026#34;, a); //%s寻址，输出地址内存储的内容，Sally printf(\u0026#34;%p\\n\u0026#34;, s4.name); //输出s4.name存储(指向)的地址，000000000062FD50，可以发现与a的地址相同 printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;s4.name); //输出s4.name本身的地址，000000000062FD60 printf(\u0026#34;%c\\n\u0026#34;, *s4.name); //注意使用%c，\u0026#39;*\u0026#39;为取地址中存储的数值，s4.name中存储的内容是第一个字母\u0026#39;S\u0026#39; printf(\u0026#34;%s\\n\u0026#34;, s4.name); //%s，输出s4.name这一串地址存储的内容，Sally printf(\u0026#34;%p\\n\u0026#34;, \u0026amp;*s4.name); //输出s4.name存储内容\u0026#39;S\u0026#39;的地址， 000000000062FD50   　我们可以看出实质上a和s4.name的地址是相同的。在实际程序调试过程中可以使用gdb等调试工具查看内存地址，不建议使用输出的方式。\n(3)定义时乱序赋值\n1 2 3 4 5 6  struct student_t s5 = { .name = \u0026#34;Tom\u0026#34;, .sex = 1, .age = 10 }; printf(\u0026#34;%s, %d, %d\\n\u0026#34;, s5.name, s5.sex, s5.age);   (4)内存空间占用\n　C语言不是内存安全的，需要编程人员自己管理内存，所以应对内存空间的分配有清晰的认识。现在感觉没什么用，等用到就懵了。基本类型的内存空间占用（单位：字节）如下：\n1 2 3 4 5 6 7  printf(\u0026#34;void:%d\\n\u0026#34;,sizeof(void)); //1 printf(\u0026#34;char:%d\\n\u0026#34;,sizeof(char)); //1 printf(\u0026#34;short:%d\\n\u0026#34;,sizeof(short)); //2 printf(\u0026#34;int:%d\\n\u0026#34;,sizeof(int)); //4 printf(\u0026#34;long:%d\\n\u0026#34;,sizeof(long)); //4(win64),8(ubuntu18) printf(\u0026#34;float:%d\\n\u0026#34;,sizeof(float)); //4 printf(\u0026#34;double:%d\\n\u0026#34;,sizeof(double)); //8   　指针类型的空间占用，和指针的基本类型无关，和计算机一次能够处理的操作数大小有关，俗称字长（关于字和字长可参考《汇编语言》-王爽），在64位操作系统中，能够处理的操作数大小为64bit，所以指针类型的大小为8字节，如char *，int *， struct example1_t *，其大小都是8字节。32位操作系统指针的大小就为4字节。\n1 2 3 4 5 6  char * c; int * d; struct example1_t * e; printf(\u0026#34;char * c:%d\\n\u0026#34;,sizeof(c)); printf(\u0026#34;int * d:%d\\n\u0026#34;,sizeof(d)); printf(\u0026#34;example1 * e:%d\\n\u0026#34;,sizeof(e));   　结构体的内存空间占用不能单纯的将结构体中的所有类型大小相加，以下面两个结构体为例，这两个结构体内的变量和变量类型完全相同， 但占用的内存空间大小不同。\n1 2 3 4 5 6 7 8 9 10 11 12  struct example1_t { char a; int b; short c; }; struct example2_t { char a; short c; int b; }; printf(\u0026#34;example1: %d\\n\u0026#34;,sizeof(struct example1_t)); //12B printf(\u0026#34;example2: %d\\n\u0026#34;,sizeof(struct example2_t)); //8B   　这是因为结构体在存储时，以结构体中占用空间最大的基本变量类型为一个单元，按结构体中的变量定义顺序开辟一个又一个单元的空间。\n　在example1_t中，占用空间最大的变量类型为int，所以以4字节为一个单元开辟空间。第一个单元存放char类型的a后，还剩3个字节，不够继续存放b，所以开辟第二个大小为4字节的单元，存放b，b的类型为int，第二个单元被放满，所以开辟第三个大小为4字节的单元，存放c。一共开辟了三个4字节的单元，所以example1_t的占用的空间大小为3 * 4 = 12 字节。\n　在example2_t中，仍是以int的大小作为单元大小，与example1_t不同的是，第二个定义的是c，c是2字节的short类型，可以和a放在同一内存单元中，此时第一个单元剩1字节，不够存放b，于是开辟第二个4字节内存单元。一共开辟了2个4字节的单元，所以example2_t占用的空间大小为2 * 4 = 8 字节。\n　在结构体中有数组时，数组就相当于若干个该类型的元素，如下面的example3_t，其中的char b[4]就相当于4个char b，则第一个4字节内存单元存放a，b[0]，b[1]，b[2]；第二个单元存放b[3]和c；最后一个单元存放d，所以example3_t的大小为3 * 4 = 12 字节。\n1 2 3 4 5 6  struct example3_t { char a; char b[4]; short c; int d; };   　example1_t、example2_t、example3_t的空间占用图示：\n图1 example1_t、example2_t、example3_t的空间占用:\r\r 　所以结构体占用的空间大小应是结构体中占用空间最大的基本变量类型的整数倍，并且我们可以准确地计算出来。基本变量类型包括整型、浮点型、字符型、指针、无值型(void)。\n　那如果结构体中含有结构体时，占用空间是什么样的呢？当结构体中含有结构体时，以其中的所有基本变量类型中占用空间最大的为单元大小。以下列两个结构体为例：\n1 2 3 4 5 6 7 8 9  struct example4_t { char a; int b; double c; }; struct example5_t { char a; struct example4_t c; };   　结构体example_4的大小为16字节，结构体example5_t的大小为24字节。\n3.typedef与结构体结合使用 　我们经常在声明结构体的时候看到与typedef连用，于是就又懵了，那么这里的typedef又是什么意思呢？\n　typedef是一个关键字，常用作给类型起一个别名，使用形式为\n1  typedef \u0026lt;旧变量名\u0026gt; \u0026lt;新变量名\u0026gt;;   　下列代码给int类型起了别名ios_int，在定义变量b时，就可以使用ios_int来定义b，其含义与int b相同。\n1 2 3  typedef int ios_int; int a; ios_int b;   　如何使用typedef给结构体类型起别名呢？\n1 2 3 4 5 6  typedef struct student_t { char name[128]; int sex; int age; }student;   　此时相当于typedef struct student_t{} student，即student是struct stutent_t的别名，此时student不再是定义的一个变量，而是别名，要注意与前边struct student_t {...} student;进行区分。在变量定义时：\n1 2 3  student a; //等价于 struct student_t a;   4.结构体指针 　结构体指针需要申请内存空间后才可以对元素赋值（不然指针只有8字节，存储地址，赋予的这些值没有位置存放），或者直接将结构体指针直接指向已有结构体。\n1 2 3 4 5 6 7 8 9  struct student_t { char name[128]; int sex; int age; }student; student b = {\u0026#34;Lee\u0026#34;, 1, 19}; student * a; a = \u0026amp;b;   　使用malloc函数申请内存空间，该函数包含在头文件\u0026lt;stdlib.h\u0026gt;中。注意申请的空间大小不可以带星号，'*\u0026lsquo;表示指针，指针大小不等于结构体大小，我们申请空间是用来来存放结构体中的数据的。\n1 2 3 4  void* malloc (size_t size); //(申请空间的变量类型)malloc(申请的空间大小) //例： student * a = (student *)malloc(sizeof(student)); a-\u0026gt;age = 19;   5.分析练习 　如果对以上5点都熟悉了以后可以来看这样一个结构体(代码来自于不知道哪位)进行练习：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  typedef struct _p_data_analysis { char p_type[8]; //包类型  int time[6]; //时间戳  int len; //长度  struct _ethernet_h *eth_hdr; //以太网帧头  struct _arp_h *arp_hdr; //arp包头  struct _ip_h *ip_hdr; //ip包头  struct _icmp_h *icmp_hdr; //icmp包头  struct _udp_h *udp_hdr; //udp包头  struct _tcp_h *tcp_hdr; //tcp包头  }p_data_analysis;   　请完成以下练习：\n　（1）定义一个该类型的变量\n（2）计算该类型占用的空间大小(win64系统)\n（3）定义一个该类型的指针变量，其申请内存空间，并用memset清0\n（4）根据（3）中定义的指针变量，为.eth_hdr申请内存空间\n参考答案：\n　（1）typedef给struct _p_data_analysis定义别名p_data_analysis，所以可以用两种方式定义：\n1 2 3  struct _p_data_analysis a; //或 p_data_analysis a;   　（2）win64系统的指针大小为8字节，该结构体以8字节为一个单元进行存储，p_type占用1个单元，time占用3个单元，len占用1个单元，6个指针占用6个单元，一共需要1 + 3 + 1 + 6 = 11个单元，每个单元8字节，最终占用的空间大小为11 * 8 = 88 字节。\n　（3）\n1 2 3 4 5 6 7 8  p_data_analysis * data = (p_data_analysis *)malloc(sizeof(p_data_analysis)); //或 struct _p_data_analysis * data = (struct _p_data_analysis *)malloc(sizeof(struct _p_data_analysis)); memset(data, 0, sizeof(p_data_analysis)); //或 memset(data, 0, sizeof(struct _p_data_analysis));   　（4）\n1  data-\u0026gt;eth_hdr = (struct _ethernet_h *)malloc(sizeof(struct _ethernet_h));   参考代码 　地址：https://github.com/yyyIce/c_exercise/tree/main/struct_test\n　有帮助的话可以点个Star⭐哦~\n参考资料 　结构体内存空间的占用：https://blog.csdn.net/fb2058/article/details/15502071\n","description":"C语言结构体的用法和示例","id":29,"section":"posts","tags":["c"],"title":"C语言：结构体的使用","uri":"https://yyyIce.github.io/zh/posts/c%E8%AF%AD%E8%A8%80%E7%BB%93%E6%9E%84%E4%BD%93/"},{"content":"1.快速了解 typedef用来为数据类型起别名，这样在阅读时能够更清晰地理解程序，同时也可以让变量名变短。\n例如下面的代码中，struct student很长(但是可以表明这是一个结构体类型的变量)。\n1 2 3 4 5  struct student{ char * name; int sex; } struct student a;   我们可以用typedef来将struct student缩短\n1  typedef struct student stu;   而我们更常见的形式是下面这样，在声明的同时定义别名：\n1 2 3 4 5 6 7  typedef struct student{ char * name; int sex; }stu; struct student a; //等价于 stu a;   2.typedef用法 关键字typedef可以为类型起一个新的别名，用法一般为：\n1  typedef \u0026lt;old_type_name\u0026gt; \u0026lt;new_type_name\u0026gt;;   1.易读别名：给基础类型起别名\n1 2 3 4  typedef int ICE_INT; int a; //等价于 ICE_INT a;   2.不太易读的别名：给数组起别名，为结构体起别名(可见引言)\n1 2 3 4  typedef char ICE_ARRAY15[15]; //ICE_ARRAY15是char [15]的别名 char a[15]; //等价于 ICE_ARRAY15 a;   3.看不太懂的别名\n(1)为指针起别名(注意，*必须和new_type_name用括号括起来)\n1 2 3 4  typedef char (* ICE_PTR_ARR)[10]; //ICE_PTR_ARR是char * [10]的别名 char (* p)[10]; //等价于 ICE_PTR_ARR p; //引用时也是p[0],p[1],...   (2)为函数指针定义别名\n1 2 3 4  typedef int (* ICE_FUNC)(int, int); //别名是ICE_FUNC，类型是参数为(int,int)返回值位int的函数指针 int (* p_func)(int, int); //等价于 ICE_FUNC p_func;   （3）为结构体指针起别名\n1 2 3 4 5  //其中ice_t是struct ice_struct *类型，即一个struct ice_struct类型的指针 typedef struct ice_struct * ice_t; //写成下面这种形式容易看不懂，容易将ice_t和*读到一起，和struct ice_struct分开 typedef struct ice_struct *ice_t;   3.参考代码 链接：https://github.com/yyyIce/c_exercise/blob/main/typedef_test.c\n有帮助的话可以点个Star⭐哦~\n参考资料 [1]http://c.biancheng.net/view/2040.html\n","description":"C语言Typedef的使用方法","id":30,"section":"posts","tags":["c"],"title":"C语言：Typedef","uri":"https://yyyIce.github.io/zh/posts/c%E8%AF%AD%E8%A8%80typedef/"},{"content":"　Wireshark是一款强大的数据包分析软件。\n　官网地址：https://www.wireshark.org/\n1.安装 　（1）进入官网\n图1 官方界面:\r\r 　（2）点击“Download”进入下载界面，选择合适的版本，一路下一步即可。\n　Download地址：https://www.wireshark.org/#download\n图2 选择合适的wireshark版本:\r\r 2.简单使用 　详细的使用可以从图1的“Learn”点进去，包括使用文档和开发文档\n　Learn地址：https://www.wireshark.org/#learnWS\n2.1 抓包 　（1）打开wireshark，双击选择网卡，后面的线有波动即为该网卡有数据包\n图3 选择网络适配器（网卡）:\r\r 　（2）开始捕包的界面如下，点击“文件”下面的红色方块即可停止。\n图4 开始捕包（网卡）:\r\r 2.2 过滤 　在应用显示过滤器中输入过滤规则可以过滤出符合条件的数据包，一般用于筛选特定协议、端口的数据包，提取它们的特征。过滤规则输入协议简称后会有提示补全。\n　常用过滤规则：\n1 2 3  ip.addr=192.168.1.1 tcp.port==80 udp.port==80   　过滤规则：\n图5 过滤规则为http:\r\r 　常用追踪TCP流分析会话，调试纠错，可以看到完整的TCP握手分手过程\n图6 选中某数据包右键追踪TCP流:\r\r 图7 追踪TCP流的结果:\r\r 　可以看到客户端先发送SYN，seq=0，然后服务端发送SYN，ACK，seq=0，ack=1，客户端发送ACK，seq=1，ack=1，握手结束，建立连接。\n图8 追踪TCP流的数据包结果:\r\r 2.3 导出分组 　（1）文件-\u0026gt;导出特定分组\n图9 导出特定分组:\r\r 　（2）根据“PacketRange内的选项导出数据包，比如可以先将想要过滤的数据包在应用显示过滤器中过滤出来，然后选择\u0026quot;displayed\u0026quot;，导出过滤后的分组\n图10 在导出界面进行选择:\r\r 　（3）导出csv文件\n图11 导出csv文件:\r\r 2.4 IO Graph  Wireshark之利用IO Graph分析数据并将数据复制到excel中生成图：https://blog.csdn.net/Mary19920410/article/details/72828033\n 　Wireshark IO Graph可以展示符合过滤条件的数据包时间和数量的关系曲线图。曲线图的X轴为tick interval（滴答时间间隔），Y轴为每tick的报文数。默认情况下，X轴的tick为1s，可以自己调节tick的大小。调节tick对于查看流量中的波峰／波谷很有帮助。要进一步查看数据包信息，可以点击图形中的任一点，即可在wireshark的数据包界面自动选中该数据包。\n　查看IO图\n　（1）菜单栏-\u0026gt;统计-\u0026gt;IO图表\n图12 查看IO图:\r\r 　（2）我们打开IO图后，点击图中一个峰值，可以发现在后面的wireshark数据包界面，已经自动选中了序号为1281的数据包。过滤器、时间间隔等信息也可以在图中查看到。点击图13最下方的save as，可以导出IO图。\n图13 IO图:\r\r 　导出IO图信息到Excel\n　默认导出的IO图过于粗糙，我们可以仅导出它的图数据，然后在Excel中重新绘制。\n（1）点击图13中的复制\n（2）打开Excel，粘贴-\u0026gt;使用文本导入向导\n图14-1 粘贴到Excel:\r\r （3）第1步，点击下一步\n图14-2 粘贴到Excel:\r\r （4）第2步，分隔符号选择逗号，可以在“数据预览”中看到数据已经分成两列，点击下一步\n图14-2 粘贴到Excel:\r\r （5）选择每列的数据格式，如不需要更改直接点击完成即可。\n（6）之后就可以使用这两列数据绘制需要的曲线图。\n","description":"wireshark的抓包、过滤、导出分组和可视化的基本操作方法","id":31,"section":"posts","tags":["basic"],"title":"工具使用：wireshark","uri":"https://yyyIce.github.io/zh/posts/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8wireshark/"},{"content":"　论文名称：SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switching ASICs\n　会议信息：SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA\n　成果：在switch.p4的基础上编写400行P4代码，在一台可编程ASIC交换机上实现百万连接的负载均衡，可代替多达数百个软件负载均衡器。\n　贡献：将ConnTable从负载均衡软件上转移到交换机中；更好地保证每个连接一致性(PCC)\n四层负载均衡面临的挑战：低延迟的完整双向通信 ；频繁的DIP池变动过程中，确保每个连接的一致性\n　前人工作不足：软件负载均衡器(ConnTable和VIPTable都在软件上，能保证PCC，延迟高，性能隔离差)、Duet(VIPTable在交换机上，不存储连接状态，能低延迟但是不能完全保证PCC）\n　本文工作：ConnTable和VIPTable都在交换机上，存储连接状态，低延迟，且比Duet保持PCC的效果好\n注：涉及到的基础概念可在文末查询。\n一、背景 1.四层负载均衡 　四层负载均衡维护两张表，ConnTable和VIPTable\n图1 ConnTable和VIPTable:\r\r 　ConnTable：记录每个连接映射到的DIP，即连接状态\n　VIPTable：记录VIP映射到的DIP池\n　当一个新连接(TCP SYN数据包)到达时，负载均衡软件根据VIP，从该VIP对应的DIP池中运行hash运算选择一个DIP，然后将该数据包转发到所选DIP对应的服务器上去。当该连接的后续数据包到达时，负载均衡软件直接将这些数据包转发到之前所选DIP对应的服务器。所以需要两个存储结构，分别用来记录连接和DIP的关系、VIP和DIP池的关系。一般用ConnTable记录连接和DIP的映射关系，用VIPTable记录VIP和DIP池的映射关系。当新连接到达时，将不会命中ConnTable，此时由交换机在VIPTable中根据新连接的VIP选择一个DIP，选择DIP之后，由交换机的CPU将这个连接和为它选择的DIP记录在ConnTable中，这样该连接的后续数据包到达时，直接根据ConnTable进行转发即可。\n　每个连接的一致性(PCC)：同一连接的每个数据包都映射到相同的DIP。\n　破坏每个连接的一致性：当DIP池由于添加或删除服务器而发生变化时，同一连接的数据包可能会哈希到不同的DIP，从而破坏每个连接的一致性（per-connection consistency, PCC）。\n　破坏PCC的情况分析：\n　1）如果在ConnTable中记录连接状态，则向DIP池中添加DIP不会影响已经写入ConnTable的连接的PCC，因为已经写入ConnTable的连接的数据包不经过VIPTable，直接转发到记录的DIP去；而DIP池中删除DIP一定会将这个DIP的连接全部断开后再删除，也不会影响已经写入ConnTable的连接的PCC。所以在记录连接状态的情况下，只有在新连接到达，且还没有将新连接状态写入ConnTable时，VIPTable更新，才有可能破坏PCC。\n　举例说明，假设有一个新连接A，A的第一个数据包到达，未命中ConnTable，经过hash计算，选择了VIP对应的DIP池中索引为20的DIP，记作a_dip，此时向VIPTable的此DIP池中增加3个DIP，这3个DIP位于a_dip前，则a_dip的索引变为23，在A的第二个数据包到达时，连接A-a_dip还没来得及写入ConnTable，第二个数据包也去匹配VIPTable，但由于向VIPTable中插入了3个DIP，这次经过hash计算后索引仍为20，但索引20存储的DIP已经不是a_dip了，第一个数据包和第二个数据包映射到了不同的DIP，PCC被破坏。\n　所以在记录连接状态的情况下，只有“新连接到达时”到“完成向ConnTable写入新连接状态”的这一时间段内更新VIPTable会影响PCC，其他时间更新VIPTable不会影响PCC。换句话说，更新VIPTable时，要考虑是否有新连接到达，要对新连接进行处理以保护PCC。\n　2）如果在ConnTable中不记录连接状态，则每次都运行hash运算来选择同一DIP，则只要VIPTable发生变化，就可能破坏PCC。\n　VIPTable必须是原子更新，且要与向ConnTable中插入\u0026quot;连接-DIP\u0026quot;的动作同步。\n2.前人的解决方案 2.1 负载均衡软件(Software Load Balancer, SLB) 　在软件中实现ConnTable和VIPTable，需要大量服务器，高成本、高延迟、高抖动，性能隔离差，但保证了PCC。在软件实现中，负载均衡器锁定VIPTable，并将新的传入连接保存在缓冲区中，防止它们匹配VIPTable。然后，SLB将新的DIP池映射更新到VIPTable，然后释放缓冲区中的连接。这样，SLB可以确保在VIPTable更新之前和之后的PCC，但是代价是CPU慢路径包处理和缓冲延迟。\n2.2 Duet 　在交换机上实现VIPTable，但仅用交换机处理具有较高流量的VIP，使用SLB(包含VIPTable和ConnTable)处理其他VIP，但无法保证频繁的DIP池更新情况下的PCC。\n　由于Duet不记录连接状态(即ConnTable中不存储DIP)，每次都依靠Hash算法来选择对应的DIP，导致DIP池中的DIP频繁添加或删除时，同一连接的不同数据包可能会映射到不同的DIP上。为应对此种情况，Duet定期将VIP迁移到交换机，或等所有旧连接都完成再更新VIPTable。\n二、本文的工作 　在ASIC交换机上实现ConnTable和VIPTable，始终在交换机上处理所有连接。\n　存在的问题，ConnTable太大，无法存储在SRAM中；VIPTable更新造成的破坏PCC的问题。\n1.控制ConnTable的规模 　ASIC交换机的SRAM大小为50-100MB，而正常的ConnTable有几百MB，所以对ConnTable的存储内容进行压缩。\n　P4中的Match-Action表的数据包括匹配字段(match field，就是key)、动作数据(action data)，ConnTable的匹配字段即能够标识一个连接的五元组，动作数据即为该连接的VIP所选择的DIP。所以可以从两个方向缩减ConnTable的规模。一是存储的匹配字段的大小，另一个就是缩短动作数据。\n　在Silkroad中，ConnTable的匹配字段不存储连接的五元组，而存储五元组的16位的hash摘要，从而减少匹配字段的大小；动作数据不存储DIP，而存储大小为6位的DIP池版本号。\n1.1 ConnTable用 hash摘要标识连接而引入的问题 　问题描述：如果两个连接的hash计算结果相同，即hash摘要相同，发生冲突，这时使用hash摘要去匹配ConnTable会导致误报。例如，假设连接A的hash摘要为digest，连接B计算后的hash摘要也是digest，连接A已经存在于ConnTable中，此时连接B作为新连接到达，则连接B将被错误地当作连接A，数据包将不经过VIPTable，直接转发到连接A的DIP对应的服务器上，而连接A的DIP，和连接B的VIP，不一定具有映射关系，导致误报。\n　问题解决：每当连接与ConnTable中的条目匹配，就将连接的TCP SYN数据包重定向到交换机的CPU，交换机软件对ConnTable中的每个条目具有完整的5元组信息，因此可以识别新连接的错误命中。\n　ConnTable这样的大型表，在ASIC交换机上会以多个物理级表的形式实例化，即物理上将ConnTable拆分成了几个子表，假设为ConnTable0,ConnTable1,ConnTable2。利用Match-Action的多阶段体系结构，我们可以在不同阶段使用不同的Hash函数进行映射，即在ConnTable0上使用Hash0，在ConnTable1上使用Hash1\u0026hellip;。当新连接B的Hash摘要与ConnTable0中的连接A的Hash摘要相同时，CPU检测到冲突，则将连接A迁移到另一个阶段，即计算其在ConnTable2中的Hash摘要并它迁移到ConnTable2中，这时连接B存储在ConnTable0中，连接A存储在ConnTable2中，由于ConnTable0和ConnTable2的Hash计算方式不同，连接A的Hash摘要和连接B的Hash摘要不同，冲突解决。这实际上利用了Cuckoo Hash的思想，即将冲突的条目放到别的槽里。\n1.2 ConnTable中用DIP池的版本来代替DIP引入了新表 　如下图所示，由于在ConnTable中使用了“Conn-Version”的映射，必须增加一张表维护Version和DIP池的映射关系，才能让连接中的数据包转发到对应的DIP上去。由于不同VIP对应DIP池的版本号可能相同，所以Version不是唯一标识DIP池的字段，VIP+Version才是唯一标识DIP池的字段。所以引入一张新表DIPPoolTable，记录VIP Version和DIP池的映射关系。\n图2 DIPPoolTable:\r\r 　DIP池更新时，先创建一个原DIP池的副本，然后在这个副本上进行更改，这个更改后的副本就成为了新DIP池，再将新版本号分配给新DIP池，再对VIPTable进行编程，将连接映射到最新的DIP池版本。当使用DIP池的连接超时并从ConnTable中删除时，DIP池将销毁，销毁时池的版本号会释放回环形缓冲区。交换机软件跟踪连接到池的映射，并管理DIP池的创建、删除、存储版本号的环形缓冲区。\n　可以通过版本重用减少活动版本的数量，从而减少版本位的大小。在添加的新DIP和删除的DIP数量相同时，可以修改现有池，将现有池重新用作最新池，而不是重新创建DIP池并获得新版本号。比如图2，之前20.0.0.1：80的V1中有两个DIP，现在进行更新，删去红色划线的DIP，增加绿色的DIP，更新后DIP的数量不变。如果重新建立DIP池，则获得版本号V3，如果重用原有DIP池，则版本号为V1。智能地重用版本号降低了版本号增加的速度。\n　根据大型Web服务提供商的数据，6bit的版本号足够处理DIP池更新的各种情况。当映射到每个DIP池版本的连接数量很大且寿命很短时，采用Version的ConnTable比记录DIP的ConnTable节省了很多开销。当活跃连接的数量很少且寿命很长时，将退回到“Conn-DIP”的映射，而不使用Version。\n2.保证每个连接的一致性 　SilkRoad在ConnTable中存储了连接状态。在本文第一章的第一小节“四层负载均衡”中我们提到过破坏PCC的条件，在ConnTable存储连接状态的情况下，VIPTable更新时仅需要考虑处理到达交换机但未写入ConnTable中的新连接。\n　在软件负载均衡器(SLB)中，在VIPTable更新时，会锁定VIPTable，并设立一个缓冲区存储新连接，当VIPTable更新完毕，释放缓冲区中的新连接。SilkRoad采用的方法与SLB中的方法一致，也是将新连接存储起来，存储到一个新的表TransitTable中，但在细节上有所不同。\n　天真的设计：将每个新连接都存储在TransitTable中，并记录其选择的DIP池版本。\n　缺点：TransitTable特别大。\n　前边在第一章也分析过了，ConnTable存储链接状态时，不需要存储所有新连接，仅考虑在DIP池更新时到达但未写入ConnTable的连接，即在TransitTable中存储映射到旧DIP池的挂起连接(挂起连接：在时刻t到达，但尚未插入ConnTable中的连接)。文章把处理挂起连接分为3步。\n　第一步：当收到VIPTable的更新请求时，会有一些连接A、B、C还没有被CPU插入到ConnTable，这时开始记录新连接D、E、F到TransitTable，因为VIPTable这时还没有开始更新，所以新连接D、E、F映射到了旧版本的DIP池，即TransitTable存储了VIPTable更新时，映射到旧DIP池但没有插入到ConnTable的新连接（第二步中可知，连接A、B、C在VIPTable更新时已经插入到了ConnTable中）。\n　第二步：当收到VIPTable更新请求之前的连接A、B、C都插入到ConnTable时，停止记录新连接到TransitTable。然后更新VIPTable。更新完毕后，所有未命中ConnTable的数据包都会先经过TransitTable，如果命中TransitTable，说明该连接应该映射到旧版本的DIP池，如果未命中TransitTable，则将匹配VIPTable，将映射到新版本的DIP池。\n　第三步：当TransitTable中的所有连接都插入ConnTable后，清除TransitTable，完成该过程。\n　所以由以上三步，我们只需要记录挂起连接和旧版本到TransitTable中。进一步思考，挂起连接要么映射到旧版本，要么映射到新版本，而根据DIPPoolTable的新版本创建规则，创建新版本时先创建旧版本的副本，然后修改副本为新版本。所以DIPPoolTable中存储了所有的版本，用新版本号-1即可找到旧版本，所以TransitTable中可以只存储挂起连接而不必存储旧版本号。\n　TransitTable中只存储连接，则可以转化为连接是否存储在TransitTable中的问题，即元素是否存在于集合中的问题。所以TransitTable可以用Bloom Filter来实现，Bloom Filter可以节省大量内存，但Bloom Filter可能存在误报，导致连接映射到旧版本。Bloom Filter可以用事务寄存器来实现。所以实际上，在SilkRoad中，TransitTable不是Match-Action表，而是用事务寄存器实现的Bloom Filter。Bloom Filter的大小设置影响误报率，Silkroad将其大小设置为256字节。\n3.两个VIPTable更新的例子 　1）对于更新时已经存在于ConnTable中的连接，当DIP池更新前后，不会影响PCC。如下图所示，虽然更新了v1所对应的DIP池，但由于DIPPoolTable的更新机制，conn_a仍然能映射到原来v1的DIP池中去。\n图3 更新时已存在于ConnTable的连接的PCC:\r\r 　2）对于更新时到达的新连接，VIPTable在更新时，复制原来的VIPTable得到一个副本，然后在副本上修改为新的VIPTable（Lock Free RCU的思想），修改完毕后，再清空旧的VIPTable和BloomFilter。所以BloomFilter只需要存储新连接摘要，而不需要存储旧的版本号。\n图4 更新时到达的新连接的PCC:\r\r 4.为什么使用Bloomfilter存储新连接？  Silkroad使用布隆过滤器实现缓存新连接而没有使用数组，可能不是因为布隆过滤器的高性能，可能就是恰好想到了布隆过滤器，实验以后发现能用，就用了。 如果Transit基本保持在8个连接，连接摘要16位，用表存储摘要，一共占用16 * 8 =16字节，而布隆过滤器占用256字节，为什么不用Table？可能是CPU将表项插入到表中速度较慢，而寄存器读写不需要经过CPU。 为什么不使用其他方法，如顺序查找？一个周期内对一个寄存器只能操作一次。  5.整体架构 图5 SilkRoad总体架构:\r\r 　系统整体架构如上图所示,除了前文中提到的ConnTable，VIPTable，TransitTable，DIPPoolTable以外，增加了一个LearnTable，用来触发CPU学习新连接。\n三、实现 　基线switch.p4之上构建了SilkRoad的P4原型，并在可编程交换机ASIC [1]上进行了编译。基线switch.p4以大约5000行P4代码实现了典型云数据中心（L2 / L3 / ACL / QoS / \u0026hellip;）所需的各种联网功能。基线switch.p4的简化版本在[16]中开源。我们添加了约400行P4代码，这些代码实现了SilkRoad所需的所有表和元数据（图10）。 [32]中展示了我们原型的更多细节。\n　我们将所有表实现为exact-match表，但TransitTable作为事务内存上的Bloom过滤器除外。 ASIC通常支持字打包，这允许一次有效地与SRAM块中的多个字匹配[19]。我们精心设计单词打包，以最大程度地提高存储效率，同时最大程度减少误报[27]。\n　我们还在交换软件中实现了一个控制平面，该平面处理来自学习过滤器的新连接事件以及来自ConnTable的连接到期事件。该软件运行杜鹃哈希算法以插入或删除ConnTable中的连接条目。此外，控制平面对DIP池更新执行3步PCC更新。事件和更新处理程序用大约1000行C代码编写，而条目插入/删除是开关驱动程序软件的一部分。\n四、涉及到的基础知识 1.负载均衡(Load Balance) 　现代负载均衡技术通常操作于网络的第四层或第七层。本文提到的是4层(L4)负载均衡技术(具体是什么Google一下吧)。\n　四层的负载均衡就是基于IP+端口的负载均衡：在三层负载均衡的基础上，通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。\n2.VIP(Virtual IP) 　虚拟IP: 就是一个未分配给真实主机的IP，也就是说对外提供数据库服务器的主机除了有一个真实IP外还有一个虚IP，使用这两个IP中的 任意一个都可以连接到这台主机，所有项目中数据库链接一项配置的都是这个虚IP，当服务器发生故障无法对外提供服务时，动态将这个虚IP切换到备用主机。\n　例：主机A，查询主机A IP得到10.1.2.3，主机A还有一个虚拟IP，10.1.2.4，当用户访问10.1.2.4时，会访问到主机A，当用户访问到10.1.2.3时，也会访问到主机A，但当主机A宕机时，可以使得10.1.2.4映射到备用主机B，使得访问10.1.2.4时访问到主机B，从而达到高可用的效果。\n3.DIP(Direct IP) 　直接IP，即真实的IP地址。\n4.连接池 　连接池的作用就是为了提高性能，将已经创建好的连接保存在池中，当有请求来时，直接使用已经创建好的连接对Server端进行访问。这样省略了创建连接和销毁连接的过程（TCP连接建立时的三次握手和销毁时的四次握手），从而在性能上得到了提高。\n5.性能隔离(Performance Isolation) 　一台服务器实例为多个VIP提供服务时各个VIP获得的服务互不影响，则称为性能隔离。\n6.DIP池更新 　向DIP池中添加或删除DIP。大部分DIP池更新来自于服务升级，即服务升级会导致DIP池中DIP的添加或删除。\n7.布隆过滤器(Bloom Filter) 　用来判断一个元素是否存在于集合中。判定不存在的元素一定不存在，判定存在的元素可能不存在。\n","description":"在switch.p4的基础上编写400行P4代码，在一台可编程ASIC交换机上实现百万连接的负载均衡，可代替多达数百个软件负载均衡器，主要解决每个连接一致性问题","id":32,"section":"posts","tags":["P4"],"title":"SilkRoad ：使用ASIC交换机作为高速低成本的有状态四层负载均衡器","uri":"https://yyyIce.github.io/zh/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBsilkroad/"},{"content":"Sample images from Pixabay\n","description":"随便练点什么","id":33,"section":"gallery","tags":null,"title":"练习","uri":"https://yyyIce.github.io/zh/gallery/%E7%BB%83%E4%B9%A0/"},{"content":"咸鱼的博客。\n这一页没构思好怎么写。\n以后再说吧👍\n","description":"About myself","id":34,"section":"","tags":null,"title":"关于我","uri":"https://yyyIce.github.io/zh/about/"}]